[{"body":"","excerpt":"","ref":"/v3.3/docs/quickstart/","title":"快速开始"},{"body":"功能介绍 云平台支持 Baremetal(物理机) 管理，提供的功能如下:\n  自动化上架: 物理机上架加电启动后，自动注册到云管平台，自动分配BMC IP地址，初始化IPMI账号密码，自动上报物理机硬件配置（CPU、内存、序列号、网卡、磁盘等）\n  自动化装机: 根据配置要求自动配置 RAID，自动分区格式化磁盘，自动部署操作系统镜像，自动初始化操作系统账号密码，自动分配IP地址，可以植入配置文件\n  生命周期管理: 支持物理机自动化开机，关机，重装系统，远程带外管理，卸载操作系统等操作\n  与虚拟机共享镜像: 使用虚拟机镜像部署物理机，便于虚拟机和物理机统一操作系统运行环境\n  API 支持: 以上操作均支持API操作，便于与其他系统的自动化流程集成\n  服务器型号支持: 支持Dell、HP、华为、浪潮、联想、超微等主流x86服务器厂商和机型\n  RAID 控制器支持: LSI MegaRaid, HP Smart Array, LSI MPT2SAS, LSI MPT3SAS, Mrarvell RAID等\n  转换为宿主机: 直接将物理机转换为运行虚拟机的宿主机\n  托管已有服务器： 托管已有并装好系统的物理机\n  服务架构 物理机管理服务架构如下:\n  Baremetal \u003c-\u003e DHCP Relay： 处理 PXE 网络启动\n  DHCP Relay \u003c-\u003e Baremetal Agent:\n 转发 PXE Boot 请求，获取网络启动相关的信息 通过 DHCP 和 TFTP 服务下发 PXE 配置  云平台定制的网络启动小系统(yunionos) kernel 和 initramfs: 运行 SSH 服务，制作 RAID，收集硬件信息等      Baremetal Agent \u003c-\u003e Region Server:\n 通过 Region Server 注册物理机记录 获取网络 IP 地址    Baremetal Agent \u003c-\u003e Baremetal:\n Baremetal 通知 Agent SSH 相关的登录信息 Agent 通过 SSH 配置 Baremetal 的 IPMI Agent 通过 IPMI 控制 Baremetal 开关机等操作 Agent 通过 SSH 执行做 RAID，装机，销毁等操作    Glance Server -\u003e Baremetal: Baremetal 从 Glance server 下载装机镜像\n  在交换机上开启 DHCP Relay 功能(或者使用 DHCP Relay软件)，relay 指向 Baremetal Agent\n 物理机上架通电后，设置 PXE 网络启动，DHCP Relay 会将 PXE Boot 请求转发到 Baremetal Agent，Baremetal Agent 收到 PXE Boot 请求，向 Region Server 注册物理机记录    技术细节 注册物理机 注册物理机有自动注册和手动注册两种方式，如果 Baremetal Agent 开启了自动注册功能，就会自动在云平台创建 baremetal 记录；如果为手动注册方式，就需要先调用物理机创建接口把对应的 PXE 网卡对应的 MAC 地址注册到平台。\n注册的流程如下:\n 物理机 PXE 启动时会发送 DHCP PXE boot 的请求，通过 DHCP Relay 请求会到 Baremetal Agent; Baremetal Agent 从 DHCP 请求中取出网卡 MAC 地址，拿 MAC 地址向 Region Server 过滤物理机记录; Region Server 告诉 Baremetal Agent 改 MAC 地址没有物理机，Baremetal Agent 就会新建记录，并从 Region Server 获取分配对应网段的 IP 地址, 通过内置 DHCP 服务回包给物理机; 物理机 PXE DHCP 请求获得分配的 IP 地址后，会通过 TFTP 从 Baremetal Agent 下载启动引导文件(kernel 和 initramfs)，然后使用 ramdisk 机制进入我们定制的 initramfs 小系统; initramfs 小系统启动后，会启动 sshd 服务，然后修改 root 用户密码，将这些登录信息通知回 Baremetal Agent; Baremetal Agent 收到通知后，记录 ssh 登录的信息，开始进行准备工作; 准备工作包括配置 IPMI，收集硬件信息等，当这些操作完成后，将所有信息上报给 Region Server 完成注册  yunionos 网络启动小系统 yunionos(https://github.com/yunionio/yunionos) 是我们使用 Buildroot 工具定制的用于 PXE 启动和管理物理机的小型 Linux 系统，作用如下:\n 运行 sshd 服务，提供 Baremetal Agent 远程执行命令 包含 LSI MegaRaid, HP Smart Array, LSI MPT2SAS, LSI MPT3SAS, Mrarvell RAID等驱动和工具，用于制作 RAID 包含 ipmitool 和相关 driver，用于配置和调用 IPMI BMC 管理物理机 包含 qemu-img, sgdisk, parted 等磁盘分区工具，用于创建操作系统  SSH 管理 当物理机通过 PXE 进入 yunionos 小系统后会启动 sshd 服务，并将 ssh login 信息通知给 Baremetal Agent，Baremetal Agent 会更新 ssh 相关的登录信息\nRAID 配置 RAID 配置由 Baremetal Agent 根据用户的配置，生成 raid 配置命令，通过 ssh 远程控制 yunionos 在物理机上制作 RAID\n安装操作系统 RAID 做完后，Baremetal Agent 会通过 ssh 远程控制 yunionos 安装操作系统和分区，流程如下:\n 调用 /lib/mos/rootcreate.sh 将系统创建到磁盘:   通过 wget 从 Glance Server 下载用户指定的 image 镜像 通过 qemu-img convert 命名将 image 写入到磁盘  创建好系统后，会根据用户的配置将系统盘 resize 分区 创建其它分区并格式化 Baremetal Agent 进行一些网络，磁盘配置的设置：比如 bonding，ip 设置, /etc/fstab, 改变 hostname 等  开关机 注册好的的物理机会配置好 IPMI, IPMI 相关的信息会记录在数据库，Baremetal Agent 通过 ipmitool 控制开关机\n重装操作系统 类似于安装操作系统，流程上会让安装了操作系统的物理机重新进入 yunionos 小系统，然后重新安装操作系统\n远程访问 Baremetal Agent 通过 ipmitool sol 接口提供串口控制界面\n删除操作系统 对正在运行操作系统的物理机重启进入 PXE 网络启动，进入 yunionos 小系统，调用 /lib/mos/partdestory.sh 销毁磁盘分区和相应的 raid 命令销毁 raid 配置\n","excerpt":"功能介绍 云平台支持 Baremetal(物理机) 管理，提供的功能如下:\n  自动化上架: 物理机上架加电启动后，自动注册到云管平台，自动 …","ref":"/v3.3/docs/howto/baremetal/intro/","title":"介绍"},{"body":"OneCloud 原生提供基于 kvm 的私有云虚拟机管理功能，运行 kvm 虚拟机的机器叫做宿主机，这种宿主机也叫作 “计算节点”，上面会运行管理虚拟机、网络和存储的一系列服务，如何部署并上线宿主机请参考: 安装部署/计算节点。\n宿主机操作 查询 通过 host-list 查询宿主机列表，host-show 查询宿主机详情。\n# 查询 kvm 这种类型的宿主机 $ climc host-list --hypervisor kvm # 查询被禁用的 kvm 宿主机 $ climc host-list --hypervisor kvm --disabled # 查询启用的 kvm 宿主机 $ climc host-list --hypervisor kvm --enabled 启用 kvm 宿主机上线后，默认是禁用的状态，需要启用才能创建虚拟机。\n# 找到禁用的宿主机 $ climc host-list --disabled # 启用 $ climc host-enable \u003chost_id\u003e 禁用 如果完全不想让宿主机创建虚拟机，可以禁用它。\n$ climc host-disable \u003chost_id\u003e ","excerpt":"OneCloud 原生提供基于 kvm 的私有云虚拟机管理功能，运行 kvm 虚拟机的机器叫做宿主机，这种宿主机也叫作 “计算节点”，上面会 …","ref":"/v3.3/docs/howto/host/kvm/","title":"KVM 宿主机"},{"body":"获取镜像 上传镜像之前需要先获取镜像，途径有多种，比如从发行版官网下载用于云平台的镜像，或者自己制作。\n发行版镜像 根据自己对发行版的需要下载发行版镜像，常用的 Linux 发行版会提供云平台虚拟机使用的镜像，地址如下:\n centos: http://cloud.centos.org/centos/7/images/ ubuntu: https://cloud-images.ubuntu.com/  制作镜像 参考: 制作镜像\n上传 下载或者制作完镜像后，使用 climc image-upload 上传到云平台的 glance 服务，下面以下载 CentOS 提供的 CentOS-7-x86_64-GenericCloud-1711 举例:\n# 下载 CentOS-7-x86_64-GenericCloud-1711.qcow2  $ wget http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-1711.qcow2 # 上传镜像到云平台并名为 CentOS-7-x86_64-GenericCloud-1711.qcow2 $ climc image-upload --format qcow2 --os-type Linux --os-arch x86_64 --standard CentOS-7-x86_64-GenericCloud-1711.qcow2 ./CentOS-7-x86_64-GenericCloud-1711.qcow2 上传时间长短取决于网络环境和镜像大小，上传完成后需要查询镜像的状态，当状态变为 ‘active’ 时，就可以拿来使用了。( 更多的关于镜像的查询参考: 镜像查询 )\n$ climc image-show CentOS-7-x86_64-GenericCloud-1711.qcow2 | grep status | status | active | 上传参数 使用 climc help image-upload 获取各个参数解释。\n","excerpt":"获取镜像 上传镜像之前需要先获取镜像，途径有多种，比如从发行版官网下载用于云平台的镜像，或者自己制作。\n发行版镜像 根据自己对发行版的需要下 …","ref":"/v3.3/docs/howto/image/upload/","title":"上传镜像"},{"body":"云平台的命令行管理工具是 climc, 可以通过该工具向后端各个服务发送API请求, 实现对资源的操控。\n通过快速开始的All in One安装或安装部署章节搭建开源版OneCloud环境时，控制节点都会安装climc，可直接跳过安装climc章节直接查看使用climc章节。\n下面介绍如何在非控制节点上安装climc。\n安装 可以通过 yum 或者源码编译的方式安装climc。\nRPM 安装 添加 yunion 的 yum 源，如果已经添加可以忽略这一步。\n$ cat \u003c\u003cEOF \u003e/etc/yum.repos.d/yunion.repo [yunion] name=Packages for Yunion Multi-Cloud Platform baseurl=https://iso.yunion.cn/yumrepo-3.2 sslverify=0 failovermethod=priority enabled=1 gpgcheck=0 EOF 安装 climc\n$ sudo yum install -y yunion-climc $ ls -alh /opt/yunion/bin/climc -rwxr-xr-x 1 root root 24M Jul 18 19:04 /opt/yunion/bin/climc 源码编译安装 首先需要搭建 go 的开发环境，然后 clone 代码，参考: 开发贡献\n# 编译 climc $ cd $GOPATH/src/yunion.io/x/onecloud $ make cmd/climc # 等待编译完成后，climc 在 _output/bin 目录下 $ ls -alh _output/bin/climc -rwxr-xr-x 1 lzx lzx 25M Jul 15 17:10 _output/bin/climc 可以根据自己的需要，将编译好的 climc 放到对应的目录，或者直接写 alias 对应到 $GOPATH/src/yunion.io/x/onecloud/_output/bin/climc 。\n使用 安装好 climc 后，可以将对应的可执行目录加入到 PATH 环境变量，下面假设 climc 所在的目录是 /opt/yunion/bin 。\n# 根据自己的需要加到 bash 或者 zsh 配置文件里面 $ echo 'export PATH=$PATH:/opt/yunion/bin' \u003e\u003e ~/.bashrc \u0026\u0026 source ~/.bashrc $ climc --help 认证配置 climc 请求云平台后端服务的流程如下:\n 通过配置信息，使用用户名密码从 keystone 获取 token token 中包含了后端服务的 endpoint 地址 climc 将对应资源的 CURD 请求发往所属的后端服务  所以在操作资源前，我们需要通过环境变量告诉 climc 想要操作的云平台和认证信息。\n目前climc支持两种认证方式：\n 通过用户名／密码认证 通过Access Key／Secret认证（从2.11开始支持）  控制节点认证配置 在控制节点上可直接通过以下命令认证配置。\n# 获取环境变量 $ ocadm cluster rcadmin export OS_AUTH_URL=https://192.168.0.246:5000/v3 export OS_USERNAME=sysadmin export OS_PASSWORD=3hV3***84srk export OS_PROJECT_NAME=system export YUNION_INSECURE=true export OS_REGION_NAME=region0 export OS_ENDPOINT_TYPE=publicURL # 认证环境变量 $ source \u003c(ocadm cluster rcadmin) 注意: 如果执行 climc 时出现 Error: Missing OS_AUTH_URL 的错误提示时，请重新执行 source \u003c(ocadm cluster rcadmin) 命令。\n非控制节点认证配置 在非控制节点做认证配置上首先需要在对应的控制节点上获取相关参数，并将认证信息保存到文件中，通过source命令认证配置。\n以下为用户名／密码认证的配置文件模板，通过OS_USERNAME, OS_DOMAIN_NAME, OS_PASSWORD, OS_PROJECT_NAME, OS_PROJECT_DOMAIN等字段指定用户的信息和项目的信息。\n# 在控制节点上获取认证所需要的配置信息。 # ocadm cluster rcadmin export OS_AUTH_URL=https://192.168.0.246:5000/v3 export OS_USERNAME=sysadmin export OS_PASSWORD=3hV3***84srk export OS_PROJECT_NAME=system export YUNION_INSECURE=true export OS_REGION_NAME=region0 export OS_ENDPOINT_TYPE=publicURL # 将认证信息保存到文件中，方便 source 使用 $ cat \u003c\u003cEOF \u003e ~/test_rc_admin # 用户名 export OS_USERNAME=sysadmin # 用户所属域名称(如果为Default域,可以省略) export OS_DOMAIN_NAME=Default # 用户密码 export OS_PASSWORD=*** # 用户所属项目名称 export OS_PROJECT_NAME=system # 项目所属域名称(如果为Default域,可以省略) export OS_PROJECT_DOMAIN=Default # keystone 认证地址 export OS_AUTH_URL=https://192.168.0.246:5000/v3 # 对应的 region export OS_REGION_NAME=Beijing EOF 以下为Access Key/Secret认证的配置文件模板，通过OS_ACCESS_KEY, OS_SECRET_KEY等两个字段指定用户的Access Key/secret。\n# 将认证信息保存到文件中，方便 source 使用 $ cat \u003c\u003cEOF \u003e ~/test_rc_admin # Access Key export OS_ACCESS_KEY=0d184a3c9c484e4c892f4855935e37e7 # Secret export OS_SECRET_KEY=VG***5mUXM= # keystone 认证地址 export OS_AUTH_URL=https://192.168.0.246:5000/v3 # 对应的 region export OS_REGION_NAME=Beijing EOF # 在控制节点上获取用户在一个项目中的Access Key/Secret # 生成 Secret Key $ climc credential-create-aksk +--------+----------------------------------------------+ | Field | Value | +--------+----------------------------------------------+ | expire | 0 | | secret | VGFxZkE3QTd2MmhCbmZkVkJDcFZFaGJYdUQ2c05mUXM= | +--------+----------------------------------------------+ # ID为 Access Key $ climc credential-list +-----------------------+------------+------------+-----------------------+---------+---------+-----------+---------+-----------------------+-------------+--------------+-----------------------+------+-----------------------+----------------------+----------+----------------------+ | blob | can_delete | can_update | created_at | deleted | domain | domain_id | enabled | id | is_emulated | name | project_id | type | update_version | updated_at | user | user_id | +-----------------------+------------+------------+-----------------------+---------+---------+-----------+---------+-----------------------+-------------+--------------+-----------------------+------+-----------------------+----------------------+----------+----------------------+ | {\"expire\":0,\"secret\": | true | true | 2020-06-15T11:43:32.0 | false | Default | default | true | 0d184a3c9c484e4c892f4 | false | --1592221412 | d53ea650bfe144da8ee8f | aksk | 0 | 2020-06-15T11:43:32. | sysadmin | a063d8e2cd584cc48194 | | \"VGFxZkE3QTd2MmhCbmZk | | | 00000Z | | | | | 855935e37e7 | | | 3fba417b904 | | | 000000Z | | 5e7169280435 | | VkJDcFZFaGJYdUQ2c05mU | | | | | | | | | | | | | | | | | | XM=\"} | | | | | | | | | | | | | | | | | +-----------------------+------------+------------+-----------------------+---------+---------+-----------+---------+-----------------------+-------------+--------------+-----------------------+------+-----------------------+----------------------+----------+----------------------+ *** Total: 1 Pages: 1 Limit: 2048 Offset: 0 Page: 1 *** 模板配置完成后，通过以下名称认证环境变量。\n# source 认证环境变量 $ source ~/test_rc_admin # 执行climc。例如，查看虚拟机列表 $ climc server-list 注意: 如果执行 climc 时出现 Error: Missing OS_AUTH_URL 的错误提示时，请 source 或设置认证云平台的环境变量。\n可以通过查看 climc 的版本号来获取构建的信息。\n$ climc --version Yunion API client version: { \"major\": \"0\", \"minor\": \"0\", \"gitVersion\": \"v3.1.9-20200609.1\", \"gitBranch\": \"tags/v3.1.8^0\", \"gitCommit\": \"5591bbec4\", \"gitTreeState\": \"clean\", \"buildDate\": \"2020-06-09T12:00:48Z\", \"goVersion\": \"go1.13.9\", \"compiler\": \"gc\", \"platform\": \"linux/amd64\" } 运行模式 climc 有命令行运行和交互两种运行模式。\n 命令行运行: 执行完对应的资源操作命令就退出，这种模式你知道自己在做什么，并且可以作为 bash function/script 的一部分。  # 删除 server1, server2, server3 for id in server1 server2 server3; do climc server-update --delete enable $id climc server-delete $id done  交互模式: 在 shell 输入 climc，就会进入交互模式，这种模式下有自动补全和参数提示。   子命令语法 云平台有很多资源，对应 climc 的子命令, 比如 climc server-list 中的 server-list 就是子命令，可以查询虚拟机的列表。通用格式如下:\n\u003cResource\u003e-\u003cAction\u003e: Resource 表示资源, Action 表示行为 语法举例:\n server-delete: 删除虚拟机  server 是资源, delete 是行为   host-list: 查询宿主机列表  host 是资源, list 是行为    CRUD 举例:\n C: server-create, disk-create 创建资源 R: server-show, disk-list 查询资源 U: server-update, host-update 更新资源 D: server-delete, image-delete 删除资源  行为举例:\n- 中的 Action 会对应资源的操作，不同的资源会根据可进行的操作进行命名。\n server-migrate: migrate 表示迁移虚拟机 server-change-config: change-config 表示调整虚拟机配置 host-ipmi: ipmi 表示查询宿主机的 IPMI 信息  想要知道资源有哪些操作，可以进入交互模式补全查询。\n使用帮助 help climc 的子命令有很多参数，参数分为必填参数和可选参数，使用 climc help \u003csubcommand\u003e 这种格式，help 子命令会获取  提供的参数和各个参数的解释。\n比如我要查看 image-upload 命令的参数和解释:\n$ climc help image-upload ... Upload a local image Positional arguments: \u003cNAME\u003e Image Name \u003cFILE\u003e The local image filename to Upload Optional arguments: [--private] Make image private [--format {raw,qcow2,iso,vmdk,docker,vhd}] Image format [--protected] ... 高级过滤 filter TODO\nDebug 模式 如果想要知道 climc 操作资源时究竟和服务端发生了哪些请求，可以在子命令前面使用 –debug 参数，使用方式如下:\nclimc --debug \u003cResource\u003e-\u003cAction\u003e 加上 –debug 参数后，终端会有彩色的输出提示，比如 climc --debug server-list 输出如下:\n其中 CURL 部分是可以直接粘贴出来在命令行执行的。\n颜色约定  Request 使用黄色 CURL 使用蓝绿色 根据状态码显示不同颜色，可参考代码: https://github.com/yunionio/onecloud/blob/master/pkg/util/httputils/httputils.go#L234  在bash或zsh下的命令行参数提示补全 climc支持bash或zsh的命令行参数自动提示补全。\n下面以bash为例说明，在使用climc之前，执行如下命令初始化环境。\n# 启用bash命令行参数自动补全 source \u003c(climc --completion bash) 之后在bash中可以在输入climc命令后，通过tab获得命令行参数的提示。\n为了方便使用，推荐将该命令放到$HOME/.bashrc或$HOME/.bash_profile中自动初始化环境。\n","excerpt":"云平台的命令行管理工具是 climc, 可以通过该工具向后端各个服务发送API请求, 实现对资源的操控。\n通过快速开始的All in One …","ref":"/v3.3/docs/howto/climc/","title":"命令行工具"},{"body":"前提 注意 本章内容是通过部署工具快速搭建 OneCloud 服务，如果想了解部署的细节或者部署高可用环境请参考: 安装部署 。  环境准备 OneCloud 相关的组件运行在 kubernetes 之上。\n服务器配置要求  操作系统: Centos 7.6 最低配置要求: CPU 4核, 内存 8G, 存储 100G  以下为待部署机器的环境:\n   IP 登录用户 操作系统     10.168.26.216 root Centos 7.6    提示  10.168.26.216 是本次测试环境 ip，请根据自己的环境做相应修改。\n  OneCloud相关软件依赖  数据库: mariadb Ver 15.1 Distrib 5.5.56-MariaDB docker: ce-19.03.9 kubernetes: v1.15.8  本地环境配置要求 本地环境即用户进行实际操作部署的环境。本次测试的本地环境为MAC操作系统的笔记本，也可在待部署机器上进行操作。\n ssh: 开启 ssh 免密登录 本地环境安装部署 ansbile，Windows操作系统不支持安装 ansible  配置 ssh 免密登录 # 生成本机的 ssh 秘钥 (如果本地已有 ~/.ssh/id_rsa.pub 则跳过此步骤) $ ssh-keygen # 将生成的 ~/.ssh/id_rsa.pub 公钥拷贝到待部署机器 $ ssh-copy-id -i ~/.ssh/id_rsa.pub root@10.168.26.216 # 尝试免密登录待部署机器，应该不需要输入登录密码即可拿到部署机器的 hostname $ ssh root@10.168.26.216 \"hostname\" 开始部署 部署的工具是 https://github.com/yunionio/ocboot , 然后根据需要部署机器的配置， 利用 ansbile 远程登录到待部署的机器安装配置 onecloud 服务，以下操作都在本地环境上进行操作。操作步骤如下:\n下载 ocboot # 本地安装 ansible $ pip install ansible # 下载 ocboot 工具到本地 $ git clone -b release/3.3 https://github.com/yunionio/ocboot \u0026\u0026 cd ./ocboot 编写部署配置 # 编写 config-allinone.yml 文件 $ cat \u003c\u003cEOF \u003e./config-allinone.yml # mariadb_node 表示需要部署 mariadb 服务的节点 mariadb_node: # 待部署节点 ip hostname: 10.168.26.216 # 待部署节点登录用户 user: root # mariadb 的用户 db_user: root # mariadb 用户密码 db_password: your-sql-password # primary_master_node 表示运行 k8s 和 onecloud 服务的节点 primary_master_node: hostname: 10.168.26.216 user: root # 数据库连接地址 db_host: 10.168.26.216 # 数据库用户 db_user: root # 数据库密码 db_password: your-sql-password # k8s 控制节点的 ip controlplane_host: 10.168.26.216 # k8s 控制节点的端口 controlplane_port: \"6443\" # onecloud 登录用户 onecloud_user: admin # onecloud 登录用户密码 onecloud_user_password: admin@123 # 该节点作为 OneCloud 私有云计算节点 as_host: true EOF 开始部署 当填写完 config-allinone.yml 部署配置文件后，便可以执行 ocboot 里面的 ./run.py ./config-allinone.yml 部署集群了。\n# 开始部署 $ ./run.py ./config-allinone.yml .... # 部署完成后会有如下输出，表示运行成功 # 浏览器打开 https://10.168.26.216 # 使用 admin/admin@123 用户密码登录就能访问前端界面 Initialized successfully! Web page: https://10.168.26.216 User: admin Password: admin@123 然后用浏览器访问 https://10.168.26.216 ，用户名输入 admin，密码输入 admin@123 就会进入 OneCloud 的界面。\nFAQ 1. 在 All in One 中找不到虚拟机界面？ All in One 部署的节点会部署 OneCloud host 计算服务，作为宿主机，具有创建和管理私有云虚拟机的能力。没有虚拟机界面应该是 OneCloud 环境中没有启用宿主机。\n请到 管理后台 界面，点击 主机/基础资源/宿主机 查看宿主机列表，启用相应的宿主机，刷新界面就会出现虚拟机界面。\n注意 如果要使用 OneCloud 私有云虚拟机，需要宿主机使用 OneCloud 编译的内核，可使用以下命令查看宿主机是否使用 OneCloud 内核(包含 yn 关键字)。\n# 查看是否使用 yn 内核 $ uname -a | grep yn Linux office-controller 3.10.0-1062.4.3.el7.yn20191203.x86_64 # 如果内核不是带有 yn 关键字的版本，可能是第一次使用 ocboot 安装，重启即可进入 yn 内核 $ reboot   2. 如何导入公有云或者其它私有云平台资源？ 在 多云管理 菜单，选择 云账号 并新建，根据自己的需求填写对应云平台的认证信息，配置完云账号后 OneCloud 服务就会同步相应云平台的资源，同步完成后即可在前端查看。\n3. 其它问题？ 其它问题欢迎在 OneCloud github issues 界面提交: https://github.com/yunionio/onecloud/issues , 我们会尽快回复。\n","excerpt":"前提 注意 本章内容是通过部署工具快速搭建 OneCloud 服务，如果想了解部署的细节或者部署高可用环境请参考: 安装部署 。 …","ref":"/v3.3/docs/quickstart/allinone/","title":"All in One 安装"},{"body":"前提 注意 本章内容是方便快速体验OneCloud, 通过MiniKube快速搭建OneCloud服务，如果想了解部署的细节或者部署高可用环境请参考: 安装部署 。  环境准备 OneCloud 相关的组件运行在MiniKube之上，环境以及相关的软件依赖如下:\n 操作系统: Centos 7.6 最低配置要求: CPU 4核, 内存 8G, 存储 100G 数据库: mariadb Ver 15.1 Distrib 5.5.56-MariaDB  安装MySQL开启远程访问\n# 此密码为上面设置的 MySQL root 密码，为了方便，只读账号也使用此密码 $ MYSQL_PASSWD='your-sql-passwd' $ mysql -uroot -p$MYSQL_PASSWD -e \"GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '$MYSQL_PASSWD' WITH GRANT OPTION;FLUSH PRIVILEGES\" 开始部署 启动minikube 下载minikue/kubectl, 并启动minikube集群, 具体请参考： https://kubernetes.io/docs/tasks/tools/install-minikube/\nminikube config -p onecloud set memory 8192 minikube start --nodes 2 -p onecloud minikube dashboard -p onecloud 部署local-path-storage 参考：https://github.com/rancher/local-path-provisioner, 在minikube部署local-path-storage\nwget https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml -O local-path-storage.yaml kubectl apply -f local-path-storage.yaml 部署onecloud k8s operator onecloud k8s operator地址： https://github.com/yunionio/onecloud-operator\nwget https://raw.githubusercontent.com/yunionio/onecloud-operator/master/manifests/onecloud-operator.yaml -O onecloud-operator.yaml kubectl apply -f onecloud-operator.yaml 部署onecloud 集群 wget https://raw.githubusercontent.com/yunionio/onecloud-operator/master/manifests/example-onecloud-cluster.yaml -O onecloud-cluster.yaml vim onecloud-cluster.yaml  修改onecloud-cluster.yaml mysql相关配额   host: $MYSQL_HOST port: $MYSQL_PORT username: \"$MYSQL_USERNAME\" password: \"$MYSQL_PASSWD\"  其他集群配置请参考： OnecloudClusterSpec:: 启动onecloud集群  kubectl apply -f onecloud-cluster.yaml 打开K8s Dashboard确认相关服务正常启动完成\n创建账号登录WebUI 创建账号\nkubectl exec -n onecloud `kubectl -n onecloud get pods | grep \"example-onecloud-cluster-climc\"| cut -f1 -d\" \"` -c climc -i -t -- /bin/bash -il $ climc user-create demo --password demo123A --system-account --enabled 登陆webUI\nkubectl -n onecloud port-forward `kubectl -n onecloud get pods | grep \"example-onecloud-cluster-web\"| cut -f1 -d\" \"` 9999:443 --address=0.0.0.0 打开浏览器：https://localhost:9999\n待解决的问题 4类Pod启动失败，问题还在分析中，但不影响体验onecloud\n example-onecloud-cluster-notify example-onecloud-cluster-host-deployer example-onecloud-cluster-monitor example-onecloud-cluster-autoupdate  集群清理 kubectl delete -f onecloud-cluster.yaml kubectl delete -f onecloud-operator.yaml kubectl delete -f local-path-storage.yaml minikube -p onecloud stop ","excerpt":"前提 注意 本章内容是方便快速体验OneCloud, 通过MiniKube快速搭建OneCloud服务，如果想了解部署的细节或者部署高可用环 …","ref":"/v3.3/docs/quickstart/minikube/","title":"MiniKube 安装"},{"body":"","excerpt":"","ref":"/v3.3/docs/setup/","title":"安装部署"},{"body":" 说明  服务是以容器的方式运行在 Kubernetes(K8S) 集群里面的，所以开发调试需要部署一个 Kubernetes 集群 后端服务都是用 Golang 编写，所以需要在开发环境安装 Golang 为了把开发的服务发布到 Kubernetes 集群，需要在本地把相关服务构建成 docker 镜像 开发环境最好都是在 Linux 上进行，安装使用 docker 和编译源码都很方便   接下来介绍如何搭建开发环境。\n部署 cloudpods 服务 在开始开发之前，请先参考 All in One 安装 或者 MiniKube 安装 部署 cloudpods 服务。我们的服务全部使用容器的方式运行在 Kubernetes 集群里面，所以需要先搭建好我们的服务，把这个环境作为自己的开发环境。\n这里建议使用一个单独的 CentOS 7 虚拟机，配置(至少 4C8G + 100G 系统盘)，安装部署我们的服务。\n安装 Go Golang 版本要求 1.15 以上\n安装 Golang 环境请参考： Install Golang\n安装配置 Docker 因为要把服务打包成容器镜像，所以需要先安装 docker，这里的 docker 版本需要是 docker-ce 19.03 以上的版本。\n下面是不同操作系统和 Linux 发行版的安装方式，这里还是建议开发环境是 Linux 。\n安装 Docker CentOS 7  # 1. 安装必要的一些系统工具 sudo yum install -y yum-utils # 2. 添加软件源信息 sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 3. 更新并安装 Docker CE sudo yum -y install docker-ce  配置 Docker 后续的代码编译和打包使用了 docker buildx 的功能，需要做在让 docker daemon 开启 experimental 特性。\n# 在 docker daemon 的配置里面打开 experimental 特性 $ cat /etc/docker/daemon.json { \"experimental\": true } # 重启 docker 服务 $ systemctl restart docker # 创建 buildx context $ docker buildx create --use --name build --node build --driver-opt network=host 编译 cloudpods 组件 Fork 仓库 访问 https://github.com/yunionio/cloudpods ，将仓库 fork 到自己的 github 用户下。\nClone 源码 git clone 前确保 GOPATH 等环境变量已经设置好，clone 你自己 fork 的仓库\n$ git clone https://github.com/\u003cyour_name\u003e/cloudpods $GOPATH/src/yunion.io/x/cloudpods $ cd $GOPATH/src/yunion.io/x/cloudpods $ git remote add upstream https://github.com/yunionio/cloudpods 二进制编译 编译是直接调用 go 编译器在本地编译出二进制，对应的 Makefile 规则为 make cmd/% ，% 对应的是 cmd 目录里面的组件名称。\n# cmd 目录下面存放着所有的组件: $ ls cmd ... ansibleserver climc glance keystone qcloudcli ucloudcli awscli cloudir host lbagent region webconsole # 可以编译cmd下指定的组件，比如：编译 climc 和 region 组件 $ make cmd/climc cmd/region # 编译好的二进制会直接在 _output/bin 目录下面，查看编译好的二进制文件 $ ls _output/bin climc region # 编译所有组件 $ make Docker 镜像编译上传 通常我们的开发流程是写完代码，把相应服务打包生成 docker 镜像，然后发布到自己搭建的 Kubernetes 集群里面测试。 下面说明如何生成 docker 镜像。\n生成好的 docker 镜像需要上传的镜像仓库，这里以 Aliyun 的容器镜像服务 为例，比如我在 aliyun 创建了一个公开的命令空间，仓库地址为: registry.cn-beijing.aliyuncs.com/zexi 。\n# 在本地登录镜像仓库，这里以你自己的镜像仓库为准 # 需要先用自己的 aliyun 帐号登录下，后面容器镜像的上传就不需要密码了 $ docker login registry.cn-beijing.aliyuncs.com/zexi ...... Login Succeeded 准备好镜像仓库后，就可以开始打包上传镜像了，这些步骤是通过 Makefile 的 image 规则来执行的。\n这里有以下环境变量用来控制制作镜像的内容：\n REGISTRY: 对应镜像上传的仓库 VERSION: 用于生成镜像的 tag ARCH: 对应 docker 镜像的 arch，可设置成 ‘arm64’ 或者 ‘all’  arm64: 编译打包制作 arm64 的 docker 镜像 all: 编译打包制作 amd64 和 arm64 的镜像   DEBUG: 如果设置为 true 会显示打包步骤  根据 REGISTRY 和 VERSION 这两个环境变量，会生成各个组件的镜像地址，格式是: $(REGISTRY)/$(component):$VERSION\n# 将 region 服务编译并制作成 docker 镜像 # 然后上传到 registry.cn-beijing.aliyuncs.com/zexi/region:dev $ VERSION=dev REGISTRY=registry.cn-beijing.aliyuncs.com/zexi make image region # 编译多个组件，并上传，以下命令会上传以下的组件 # - registry.cn-beijing.aliyuncs.com/zexi/ansibleserver:dev # - registry.cn-beijing.aliyuncs.com/zexi/apigateway:dev # - registry.cn-beijing.aliyuncs.com/zexi/region:dev $ VERSION=dev REGISTRY=registry.cn-beijing.aliyuncs.com/zexi make image \\  ansibleserver apigateway region # 编译 arm64 的镜像，如果指定了 ARCH=arm64 ，则会在对应镜像的末尾加上 '-arm64' 的后缀 # - registry.cn-beijing.aliyuncs.com/yunionio/scheduler:dev-arm64 $ VERSION=dev REGISTRY=registry.cn-beijing.aliyuncs.com/zexi ARCH=arm64 make image scheduler # 编译 amd64 + arm64 的镜像，指定 ARCH=all，这里不会添加后缀，会在镜像名里面包含两个架构的版本 # - registry.cn-beijing.aliyuncs.com/yunionio/cloudid:dev $ VERSION=dev REGISTRY=registry.cn-beijing.aliyuncs.com/zexi ARCH=all make image cloudid # 同时编译多个多架构的组件，并上传 $ VERSION=dev ARCH=all REGISTRY=registry.cn-beijing.aliyuncs.com/zexi make image \\  ansibleserver apigateway baremetal-agent climc cloudevent \\  cloudnet devtool esxi-agent glance host host-deployer keystone \\  logger notify region s3gateway scheduler webconsole yunionconf \\  vpcagent monitor region-dns cloudid 开发调试 开发调试的通常步骤是，写好代码后，使用之前说的 make image 规则打包上传对应服务的 docker 镜像，然后将镜像发布到自己的 Kubernetes 集群进行测试。\n将镜像发布到 Kubernetes 集群 后端的服务都运行在 Kubernetes 的 onecloud namespace 里面，可以通过以下命令查看对应的 Kubernetes 资源。我们的服务使用以下的 Kubernetes 资源进行服务的管理。\n deployment: 管理大部分的服务，这种服务会在 Kubernetes 的任意一个 master 节点启动，比如: region, apigateway 服务等 daemonset: 管理需要在每个 Kubernetes 节点都启动的服务，比如: host 服务(私有云计算节点服务)  说明 另外需要简单了解下 Kubernetes pod 这种资源，pod 是实际运行容器的集合，是 Kubernetes 里面运行容器化服务的最小单元，一个 pod 里面可以运行多个容器，每个容器都有自己对应的镜像。\n其它 Kubernetes 资源介绍可参考官方的 工作负载介绍。\n 下面是使用 kubectl(Kubernetes 命令行工具) 查看各个服务对应资源的命令：\n# 查看 onecloud 命令空间下面的 deployment $ kubectl -n onecloud get deployment # 查看 onecloud 命令空间下面的 daemonset $ kubectl -n onecloud get daemonset # 查看 onecloud 命名空间下面的 pods $ kubectl -n onecloud get pod 对 Kubernetes 资源有了大概了解后，接下来的步骤就是把刚才打包的服务镜像发布到集群里面对应的服务，这里以 region 这个服务为例。\n# 先在本地编译打包 region 镜像 # 会在 aliyun 生成镜像: registry.cn-beijing.aliyuncs.com/zexi/region:dev-test $ VERSION=dev-test REGISTRY=registry.cn-beijing.aliyuncs.com/zexi make image region # 找到 region 服务对应的 kubernetes 资源名称 $ kubectl get deployment -n onecloud | grep region default-region 1/1 1 1 90d # 更新 default-region deployment 资源里面 image 属性 # 然后对应的 default-region 的 pod 就会自动拉取镜像重启 $ kubectl -n onecloud edit deployment default-region 使用 kubectl -n onecloud edit deployment default-region 命令后，进入编辑资源 YAML 属性的步骤，只需要将里面的 image 属性修改成 aliyun 对应镜像地址，截图如下：\n修改完后保存退出，就会拉取刚才指定的镜像创建新的 region pod，删除旧的。可以再次查看 region pod 的当前状态。\n查看服务输出日志 可以使用 kubectl log 命令查看对应 pod 的输出日志，这里以刚才发布的 region pod 为例。\n# 先找到 region 服务对应的 pod $ kubectl get pods -n onecloud | grep region default-region-6bd8c54d68-sq4gq 1/1 Running 0 101m # 查看日志 $ kubectl logs -n onecloud default-region-6bd8c54d68-sq4gq # 把日志重定向到文件 /tmp/region.log $ kubectl logs -n onecloud default-region-6bd8c54d68-sq4gq \u003e /tmp/region.log # 流式查看日志，类似于 'tail -f' $ kubectl logs -n onecloud default-region-6bd8c54d68-sq4gq -f # 查看 5 分钟前的日志 $ kubectl logs -n onecloud default-region-6bd8c54d68-sq4gq -f --since 5m 安装配置 climc climc 是访问后端的命令行工具，可以通过该工具向后端各个服务发送API请求，日常开发中会使用改命令行工具进行功能验证和调试，安装和使用方法参考下面的连接。\nclimc 的本地安装参考: 源码编译安装\nclimc 的本地配置参考: 非控制节点认证配置\nclimc 的使用简介参考: climc 使用\n快速开发调试 我们的服务都已经容器化运行在 Kubernetes 集群中，使用上面说的 “制作docker镜像-\u003e发布到集群” 的开发流程有些长，对于快速开发调试并不方便。\n通过 Telepresence 提供远程 Kubernetes 连接信息上下文，可以在本地开发调试。下面介绍使用 Telepresence 进行本地快速开发。\n安装配置 kubectl 需要在本地安装 kubectl。\n需要在本地配置好集群信息，以通过 kubectl 访问；将 cloudpods 控制节点上的$KUBECONFIG文件拷贝到本地~/.kube/config; 如果本地已经有此文件，参考 配置多集群访问 进行合并。\n安装 telepresence 这里介绍 CentOS 7 的本地环境安装，其他发行版可参考官方文档：Installing Telepresence。\n 不建议K8S集群的部署和开发在同一个环境，使用Telepresence会有端口冲突。\n # 安装依赖 $ yum install -y python3 sshfs conntrack iptables torsocks sshuttle sudo yum-utils # 安装 kubectl 用于连接 K8S 集群 $ cat \u003c\u003cEOF \u003e/etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF $ yum install -y kubectl-1.15.8-0 # 需要自行配置kubctl config # 测试kubctl可以访问之前部署的K8S集群 $ kubctl version # 源码安装 telepresence 到 /usr/local/bin/telepresence $ git clone https://github.com/telepresenceio/telepresence $ cd telepresence $ sudo env PREFIX=/usr/local ./install.sh 使用 利用 telepresence 本地连通远端 K8S 的特性，我们就可以做到在本地编译运行 region，keystone 等服务，同时又能访问远端 K8S 其它服务的环境。\nmacOS 或 linux 中本地编译运行 region 服务\n# 切换到 cloudpods 代码目录 $ cd $GOPATH/src/yunion.io/x/cloudpods # 编译 region 服务 $ make cmd/region # 使用 telepresence 替换 K8S 里面的 default-region deployment # 该命令在 K8S 集群中启动一个 deployment 替换掉原来的 default-regoin # 然后把流量的访问导向本地 # 如果想要使用特定的 shell，比如 zsh，可以在后面加上\"--run /bin/zsh\" $ telepresence --swap-deployment default-region --namespace onecloud 到这里已经进入到 telepresence 隔离的 namespace 里面了， $TELEPRESENCE_ROOT 这个目录 是通过 sshfs 挂载的远端 K8S pod 的文件系统。 接下来我们就可以在这个 namespace 里面运行 region 服务了：\n$ sudo chmod 777 /etc # 将 $TELEPRESENCE_ROOT/etc/yunion 链接到本地的 /etc $ ln -s $TELEPRESENCE_ROOT/etc/yunion /etc # 启动 region 服务 $ ./_output/bin/region --config /etc/yunion/region.conf 这个时候如果我们在外部调用 climc 就会发现相关的请求已经被转发到本地开发机启动 region 服务了。\n$ climc network-list 调试完成后需要进行清理操作\n# 退出 telepresence $ exit # 会看到类似下面的输出 T: Your process exited with return code 127. T: Exit cleanup in progress T: Cleaning up Pod # 删除链接文件 $ rm /etc/yunion Linux系统中本地编译运行 region 服务\n这种方式相比上一种方式，更加干净；但是相对复杂\n# 切换到 cloudpods 代码目录 $ cd $GOPATH/src/yunion.io/x/cloudpods # 编译 region 服务 $ make cmd/region # 使用 telepresence 替换 K8S 里面的 default-region deployment # 该命令在 K8S 集群中启动一个 deployment 替换掉原来的 default-regoin # 然后把流量的访问导向本地 # 如果想要使用特定的 shell，比如 zsh，可以在后面加上\"--run /bin/zsh\" $ telepresence --swap-deployment default-region --namespace onecloud 到这里已经进入到 telepresence 隔离的 namespace 里面了， $TELEPRESENCE_ROOT 这个目录 是通过 sshfs 挂载的远端 K8S pod 的文件系统。 接下来我们就可以在这个 namespace 里面运行 region 服务了：\n# 设置 max_user_namespaces $ cat /proc/sys/user/max_user_namespaces 0 # 如果 max_user_namespaces 为 0，需要设置下 user_namespaces $ echo 640 \u003e /proc/sys/user/max_user_namespaces # 启动一个新的 namespace , 但不共享 mount namespace，这样接下来的 mount bind 操作就不会影响到宿主机 $ unshare --map-root-user --mount # bind K8S /var/run/secrets $ mount --bind $TELEPRESENCE_ROOT/var/run /var/run $ ls /var/run/ secrets # bind cloudpods config $ mkdir /etc/yunion $ mount --bind $TELEPRESENCE_ROOT/etc/yunion /etc/yunion $ ls /etc/yunion/ pki region.conf # 启动 region 服务 $ ./_output/bin/region --config /etc/yunion/region.conf # 这个时候如果我们在外部调用 climc $ climc network-list # 就会发现相关的请求已经被转发到本地开发机启动 region 服务了 更多用法，以及 telepresence 的原理请参考官方文档。\nFAQ 1. 开发环境是 windows 或者 macOS，怎么开发？ 因为我们的服务最后会运行在基于 CentOS 7 搭建的 K8S 集群里面，所以日常的开发和打包中一般都是在 CentOS 7 里面做的。\n如果开发环境是 windows ，可以在 windows 里面写代码，然后创建一个 CentOS 7 的虚拟机，在虚拟机里面把开发环境搭建好，写完代码利用 rsync 等同步工具，把修改的代码增量拷贝到虚拟机中，然后进行打包发布等流程。\n对于 Mac 上的 macOS 也是类似的，可以使用和 windows 开发一样的流程。但 macOS 里开发和 Linux 里面开发差异没有很大，在 macOS 里面安装好对应的命令行工具和 docker 后，就可以直接使用 make image 相关的规则打包生成 docker 镜像了。\n2. 本地调试启动 region 服务，报以下错误 使用climc service-config-edit region2编辑 region 服务的配置，修改参数：\n fetch_etcd_service_info_and_use_etcd_lock: false enable_host_health_check: false   3. 使用 telepresence 时，上次未正常退出，再次使用一直报错 尝试手动清理:\n 使用 kubectl 删除 onecloud namespace 下除 default-region-dns-xxxxx 外，所有以 default-region 开头的Pod； 使用kubectl edit deployment default-region -n onecloud，将 spec 下的 replicas 从0改为1.  # 可以编译cmd下制定的组件，比如：编译 region 和 host 组件 $ make cmd/region cmd/host # 查看编译好的二进制文件 $ ls _output/bin region host 4. 本地直接运行 make cmd/host 会出现 ceph 依赖的报错 host 组件是私有云里面管理虚拟机的一个关键组件，依赖了 cephfs, rbd 和 rados 相关的库，如果是本地直接编译，则需要安装对应的依赖，操作如下：\nCentOS 7 Ubuntu 20.04 On rpm based systems (dnf, yum, etc):\nsudo rpm --import https://download.ceph.com/keys/release.asc sudo yum install -y https://download.ceph.com/rpm-luminous/el7/noarch/ceph-release-1-1.el7.noarch.rpm sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum install -y libcephfs-devel librbd-devel librados-devel  On debian systems (apt):\nwget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add - echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list apt-get update \u0026\u0026 apt-get install -y libcephfs-dev librbd-dev librados-dev  ","excerpt":" 说明  服务是以容器的方式运行在 Kubernetes(K8S) 集群里面的，所以开发调试需要部署一个 Kubernetes 集群 后端服 …","ref":"/v3.3/docs/contribute/dev-env/","title":"搭建开发环境"},{"body":"OneCloud 目前仅支持在 Centos 7 上运行，待部署组件/服务如下:\n   服务组件 用途 安装方式 运行方式     mariadb 关系型数据库 rpm systemd   docker 容器运行时 rpm systemd   kubelet 管理 kubernetes pod rpm systemd   keystone 认证服务 k8s deployment container   region api 控制器 k8s deployment container   scheduler 调度服务 k8s deployment container   glance 镜像存储 k8s deployment container   webconsole 虚拟机访问界面 k8s deployment container   influxdb 监控数据库 k8s deployment container   host 管理虚拟机 k8s daemonset container   sdnagent 管理虚拟机网络 rpm systemd   baremetal-agent 管理物理机 k8s deployment container   climc 命令行工具 rpm shell   ocadm 部署服务管理工具 rpm shell    其中 host 和 baremetal-agent 可以根据需求选择性部署:\n 管理 kvm 虚拟机: 部署 host 服务 管理物理机: 部署 baremetal-agent 服务  ","excerpt":"OneCloud 目前仅支持在 Centos 7 上运行，待部署组件/服务如下: …","ref":"/v3.3/docs/setup/intro/","title":"组件概览"},{"body":"  镜像(image): 是用于新建云服务器(虚拟机)、裸金属(物理机)使用的模板文件，常用类型为 qcow2, vmdk, raw, vhd, iso。\n  镜像服务(glance): 云平台的镜像服务叫做 glance，用于存储转换用户上传或外部导入的镜像，提供下载功能。\n  缓存镜像(cached-image): 创建公/私有云虚拟机时，可以直接使用各个云平台已有的镜像，这些镜像不会存储在 glance，云平台只是保存元信息，创建机器时会直接使用。\n  image 和 cached-image 两种资源的区别如下：\n image: glance 管理的镜像，由用户上传或者外部导入; cached-image:  包括公有云和其他私有云的镜像，不由 glance 管理，一般在创建 OneCloud 之外的公/私有云主机的时候用到; 不提供创建接口，只能查询，刷新和删除;    ","excerpt":"  镜像(image): 是用于新建云服务器(虚拟机)、裸金属(物理机)使用的模板文件，常用类型为 qcow2, vmdk, raw, …","ref":"/v3.3/docs/howto/image/","title":"镜像"},{"body":"在部署生产可用的 kubernetes 集群之前，需要先部署 LoadBalancer 环境，这里使用 keepalived + haproxy 的方式实现负载均衡和高可用。\n环境说明 单独拿两个节点部署 keepalived 和 haproxy 作为后端 kubernetes 控制平面的负载均衡器，拓扑结构如下:\n两个节点上面分别部署 keepalived 和 haproxy 组成负载均衡集群，haproxy 的 backend 为后端的 kubernetes control plane node，vip(虚ip) 在这两个节点之间漂移形成高可用。\n另外 OneCloud 服务使用 Mariadb，如果没有专门的数据库集群，可以单独拿两个节点部署 Mariadb 高可用。参考 部署 DB HA 环境 。\n部署 keepalived 的主要作用是为 haproxy 提供 vip，在2个 haproxy 实例之间提供主备，降低当其中一个haproxy失效的时对服务的影响。\n部署配置 keepalived 设置相关的环境变量，根据不同的环境自行配置。\n# keepalived vip 地址 export K8SHA_VIP=10.168.222.18 # keepalived auth toke export K8SHA_KA_AUTH=412f7dc3bfed32194d1600c483e10ad1d # keepalived network interface export K8SHA_NETIF=eth0 设置 sysctl 选项\n$ cat \u003c\u003cEOF \u003e\u003e/etc/sysctl.conf net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1 EOF $ sysctl -p 安装 keepalived\n$ yum install -y keepalived 添加配置\n$ cat \u003c\u003cEOF \u003e/etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script check_haproxy { script \"pidof haproxy\" interval 3 weight -2 fall 10 rise 2 } vrrp_instance VI_1 { state MASTER interface $K8SHA_NETIF virtual_router_id 51 priority 250 advert_int 1 authentication { auth_type PASS auth_pass $K8SHA_KA_AUTH } virtual_ipaddress { $K8SHA_VIP } track_script { check_haproxy } } EOF 启动 keepalived\n$ systemctl enable --now keepalived $ ip addr show $K8SHA_NETIF 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP qlen 1000 link/ether 00:22:ff:95:87:f7 brd ff:ff:ff:ff:ff:ff inet 10.168.222.189/24 brd 10.168.222.255 scope global eth0 valid_lft forever preferred_lft forever inet 10.168.222.18/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::222:ffff:fe95:87f7/64 scope link valid_lft forever preferred_lft forever 部署配置 haproxy 此处的 haproxy 为 apiserver 提供反向代理，haproxy 将所有请求轮询转发到每个master节点上。\n系统配置\nexport K8S_MASTER0=10.168.222.218 export K8S_MASTER1=10.168.222.197 export K8S_MASTER2=10.168.222.207 安装 haproxy\n$ yum install -y haproxy 配置 haproxy\n$ cat \u003c\u003cEOF \u003e/etc/haproxy/haproxy.cfg #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global # to have these messages end up in /var/log/haproxy.log you will # need to: # # 1) configure syslog to accept network log events. This is done # by adding the '-r' option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 #--------------------------------------------------------------------- # kubernetes apiserver frontend which proxys to the backends #--------------------------------------------------------------------- frontend kubernetes-apiserver mode tcp bind *:6443 option tcplog default_backend kubernetes-apiserver #--------------------------------------------------------------------- # round robin balancing between the various backends #--------------------------------------------------------------------- backend kubernetes-apiserver mode tcp balance roundrobin server master-0 $K8S_MASTER0:6443 check server master-1 $K8S_MASTER1:6443 check server master-2 $K8S_MASTER2:6443 check #--------------------------------------------------------------------- # collection haproxy statistics message #--------------------------------------------------------------------- listen stats bind *:1080 stats auth admin:awesomePassword stats refresh 5s stats realm HAProxy\\ Statistics stats uri /admin?stats EOF 启动并检测服务\n$ systemctl enable haproxy.service --now $ systemctl status haproxy.service $ netstat -tulnp | egrep '6443|1080' tcp 0 0 0.0.0.0:6443 0.0.0.0:* LISTEN 10033/haproxy tcp 0 0 0.0.0.0:1080 0.0.0.0:* LISTEN 10033/haproxy 部署 kubernetes 集群 参考 部署集群 。\n","excerpt":"在部署生产可用的 kubernetes 集群之前，需要先部署 LoadBalancer 环境，这里使用 keepalived + …","ref":"/v3.3/docs/setup/controlplane-ha/","title":"部署 HA 环境"},{"body":"","excerpt":"","ref":"/v3.3/docs/contribute/","title":"开发相关"},{"body":"Fork 仓库 访问 https://github.com/yunionio/cloudpods ，将仓库 fork 到自己的 github 用户下。\nClone 源码 clone 自己 fork 的仓库，并设置 upstream 为源仓库。\n$ git clone https://github.com/\u003cyour_name\u003e/cloudpods $ cd cloudpods $ git remote add upstream https://github.com/yunionio/cloudpods 提交代码流程  从 master checkout 出 feature 或者 bugfix 分支  # checkout 新分支 $ git fetch upstream --tags $ git checkout -b feature/implement-x upstream/master  在新的分支上进行开发 开发完成后，进行提交PR前的准备操作  $ git fetch upstream # 同步远程 upstream master 代码 $ git rebase upstream/master # 有冲突则解决冲突 $ git push origin feature/implement-x # push 分支到自己的 repo  在GitHub的Web界面完成提交PR的流程   提完 PR 后请求相关开发人员 review，并设置Labels来表明提交的代码属于哪一个模块或者哪几个模块   或者通过添加评论的方式来完成上一步；评论 “/cc” 并 @ 相关人员完成设置reviewer，评论/area 并填写label完成设置Labels  ​\t所有Label都可以在issues——Labels下查询到，带area/前缀的Label均可以使用评论\"/area\"的形式添加\n 如果是 bugfix 或者需要合并到之前 release 分支的 feature PR，需要额外使用脚本将此PR cherry-pick 到对应的 release 分支  # 自行下载安装 github 的 cli 工具：https://github.com/github/hub # OSX 使用: brew install hub # Debian: sudo apt install hub # 二进制安装: https://github.com/github/hub/releases # 设置github的用户名 $ export GITHUB_USER=\u003cyour_username\u003e # 使用脚本自动 cherry-pick PR 到 release 分支 # 比如现在有一个提交的PR的编号为18，要把它合并到 release/3.4 $ ./scripts/cherry_pick_pull.sh upstream/release/3.4 18 # cherry pick 可能会出现冲突，冲突时开另外一个 terminal，解决好冲突，再输入 'y' 进行提交 $ git add xxx # 解决完冲突后 $ git am --continue # 回到执行 cherry-pick 脚本的 terminal 输入 'y' 即可 去 upstream 的 PR 页面, 就能看到自动生成的 cherry-pick PR，上面操作的PR的标题前缀就应该为：Automated cherry pick of #18，然后重复 PR review 流程合并到 release\n注意 提交 git 代码后需要书写 commit 内容，规范请参考: Git 提交内容规范。  ","excerpt":"Fork 仓库 访问 https://github.com/yunionio/cloudpods ，将仓库 fork 到自己的 github …","ref":"/v3.3/docs/contribute/contrib/","title":"提交贡献代码"},{"body":"OneCloud 服务使用 Mariadb，这里使用 keepalived 和 Mariadb 的主主复制功能来实现 DB 的高可用。\n部署 keepalived 的主要作用是为 Mariadb 提供 vip，在2个 Mariadb 实例之间切换，不间断的提供服务。\n部署配置 Mariadb 主主复制 安装并启动 Mariadb\n$ yum install -y mariadb-server $ systemctl enable --now mariadb 运行 Mariadb 安全配置向导，设置密码等\n$ mysql_secure_installation ... ... Change the root password? [Y/n] y New password: Re-enter new password: Password updated successfully! Reloading privilege tables.. ... Success! ... ... Remove anonymous users? [Y/n] y ... Success! ... ... Disallow root login remotely? [Y/n] y ... Success! ... ... Remove test database and access to it? [Y/n] y - Dropping test database... ... Success! - Removing privileges on test database... ... Success! ... ... Reload privilege tables now? [Y/n] y ... Success! ... ... 修改 Mariadb 配置文件，准备配置主主复制\n# 主节点 $ cat \u003c\u003cEOF \u003e /etc/my.cnf [mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 # Settings user and group are ignored when systemd is used. # If you need to run mysqld under a different user or group, # customize your systemd unit file for mariadb according to the # instructions in http://fedoraproject.org/wiki/Systemd # skip domain name resolve skip_name_resolve # auto delete binlog older than 30 days expire_logs_days=30 innodb_file_per_table=ON max_connections = 300 server-id = 1 auto_increment_offset = 1 auto_increment_increment = 2 log-bin = mysql-bin binlog-format = row log-slave-updates max_binlog_size = 1G replicate-ignore-db = information_schema replicate-ignore-db = performance_schema max_connections = 1000 max_connect_errors = 0 max_allowed_packet = 1G slave-net-timeout=10 master-retry-count=0 slow_query_log = 1 long_query_time = 2 slow_query_log_file = /var/log/mariadb/slow-query.log [mysql] no-auto-rehash [mysqld_safe] log-error=/var/log/mariadb/mariadb.log pid-file=/var/run/mariadb/mariadb.pid # # include all files from the config directory # !includedir /etc/my.cnf.d EOF # 备节点 $ cat \u003c\u003cEOF \u003e /etc/my.cnf [mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 # Settings user and group are ignored when systemd is used. # If you need to run mysqld under a different user or group, # customize your systemd unit file for mariadb according to the # instructions in http://fedoraproject.org/wiki/Systemd # skip domain name resolve skip_name_resolve # auto delete binlog older than 30 days expire_logs_days=30 innodb_file_per_table=ON max_connections = 300 server-id = 2 auto_increment_offset = 2 auto_increment_increment = 2 log-bin = mysql-bin binlog-format = row log-slave-updates max_binlog_size = 1G replicate-ignore-db = information_schema replicate-ignore-db = performance_schema max_connections = 1000 max_connect_errors = 0 max_allowed_packet = 1G slave-net-timeout=10 master-retry-count=0 slow_query_log = 1 long_query_time = 2 slow_query_log_file = /var/log/mariadb/slow-query.log [mysql] no-auto-rehash [mysqld_safe] log-error=/var/log/mariadb/mariadb.log pid-file=/var/run/mariadb/mariadb.pid # # include all files from the config directory # !includedir /etc/my.cnf.d EOF # 重启服务 $ systemctl restart mariadb 主节点创建只读账号，导出全部数据，导入备节点。记录binlog日志文件名和position。\n# 以下命令在主节点执行 # 此密码为上面设置的 Mariadb root 密码，为了方便，只读账号也使用此密码 $ MYSQL_PASSWD='your-sql-passwd' # 开启 Mariadb 的远程访问 $ mysql -uroot -p$MYSQL_PASSWD -e \"GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '$MYSQL_PASSWD' WITH GRANT OPTION;FLUSH PRIVILEGES\" # 创建只读账号 $ mysql -u root -p$MYSQL_PASSWD -e \"GRANT REPLICATION SLAVE ON *.* TO repl@'%' IDENTIFIED BY '$MYSQL_PASSWD';FLUSH PRIVILEGES\" # \u0008示例是全新安装的 Mariadb ，还没有使用。如果是正在使用的数据库做主主复制，需要锁表后再导出数据 $ mysql -uroot -p$MYSQL_PASSWD -e \"SHOW PROCESSLIST\" +----+------+-----------+------+---------+------+-------+------------------+----------+ | Id | User | Host | db | Command | Time | State | Info | Progress | +----+------+-----------+------+---------+------+-------+------------------+----------+ | 4 | root | localhost | NULL | Query | 0 | NULL | SHOW PROCESSLIST | 0.000 | +----+------+-----------+------+---------+------+-------+------------------+----------+ # 记录binlog日志文件名和position $ mysql -u root -p$MYSQL_PASSWD -e \"SHOW MASTER STATUS\\G\" *************************** 1. row *************************** File: mysql-bin.000001 Position: 2023 Binlog_Do_DB: Binlog_Ignore_DB: # 导出全部数据 $ mysqldump --all-databases -p$MYSQL_PASSWD \u003e alldb.db # 拷贝 alldb.db 到备节点 $ scp alldb.db db2:/root/ # 以下命令在备节点执行 # 此密码为上面设置的 Mariadb root 密码 $ MYSQL_PASSWD='your-sql-passwd' # 导入主节点导出的数据 mysql -u root -p$MYSQL_PASSWD \u003c alldb.db # 重载权限 mysql -u root -p$MYSQL_PASSWD -e \"FLUSH PRIVILEGES\" # 记录binlog日志文件名和position mysql -u root -p$MYSQL_PASSWD -e \"SHOW MASTER STATUS\\G\" *************************** 1. row *************************** File: mysql-bin.000001 Position: 509778 Binlog_Do_DB: Binlog_Ignore_DB: 设置主主复制\n# 以下命令在主节点执行 # 修改MASTER_HOST为备节点IP，修改MASTER_LOG_FILE和MASTER_LOG_POS为上面备节点记录的信息 mysql -u root -p$MYSQL_PASSWD -e \"CHANGE MASTER TO MASTER_HOST='192.168.199.99',MASTER_USER='repl',MASTER_PASSWORD='$MYSQL_PASSWD',MASTER_PORT=3306,MASTER_LOG_FILE='mysql-bin.000001',MASTER_LOG_POS=509778,MASTER_CONNECT_RETRY=2;START SLAVE\" # 以下命令在备节点执行 # 修改MASTER_HOST为主节点IP，修改MASTER_LOG_FILE和MASTER_LOG_POS为上面主节点记录的信息 mysql -u root -p$MYSQL_PASSWD -e \"CHANGE MASTER TO MASTER_HOST='192.168.199.98',MASTER_USER='repl',MASTER_PASSWORD='$MYSQL_PASSWD',MASTER_PORT=3306,MASTER_LOG_FILE='mysql-bin.000001',MASTER_LOG_POS=2023,MASTER_CONNECT_RETRY=2;START SLAVE\" # 主备都执行，验证同步状态，都输出2个 Yes 表示正常 mysql -u root -p$MYSQL_PASSWD -e \"SHOW SLAVE STATUS\\G\" | grep Running Slave_IO_Running: Yes Slave_SQL_Running: Yes 至此，DB 主主复制部署完成，可以测试在任一节点进行数据库操作，另一节点验证。不过对外提供服务还是需要通过 vip，不然发生切换还需要业务端切换 ip，下面配置 keepalived 对外提供服务。\n部署配置 keepalived 设置相关的环境变量，根据不同的环境自行配置。\n# keepalived vip 地址 export DB_VIP=192.168.199.97 # keepalived auth toke export DBHA_KA_AUTH=onecloud # keepalived network interface export DB_NETIF=eth0 设置 sysctl 选项\n$ cat \u003c\u003cEOF \u003e\u003e/etc/sysctl.conf net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1 EOF $ sysctl -p 安装 keepalived nc\n$ yum install -y keepalived nc 添加配置\n# 请确保 virtual_router_id 不会和局域网内的其他 keepalived 集群冲突 $ cat \u003c\u003cEOF \u003e/etc/keepalived/keepalived.conf global_defs { router_id onecloud } vrrp_script chk_mysql { script \"/etc/keepalived/chk_mysql\" interval 1 } vrrp_instance VI_1 { state MASTER interface $DB_NETIF virtual_router_id 99 priority 100 advert_int 1 nopreempt authentication { auth_type PASS auth_pass $DBHA_KA_AUTH } track_script { chk_mysql } virtual_ipaddress { $DB_VIP } } EOF $ cat \u003c\u003cEOF \u003e /etc/keepalived/chk_mysql #!/bin/bash echo | nc 127.0.0.1 3306 \u0026\u003e/dev/null EOF $ chmod +x /etc/keepalived/chk_mysql 启动 keepalived\n$ systemctl enable --now keepalived $ ip addr show $DB_NETIF 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:22:cf:40:1e:29 brd ff:ff:ff:ff:ff:ff inet 192.168.199.99/24 brd 192.168.199.255 scope global dynamic eth0 valid_lft 100651906sec preferred_lft 100651906sec inet 192.168.199.97/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::222:cfff:fe40:1e29/64 scope link valid_lft forever preferred_lft forever 至此，DB 高可用部署完成，任一节点的 Mariadb 或 keepalived 服务异常，或者任一节点宕机，都不影响对外服务。\n","excerpt":"OneCloud 服务使用 Mariadb，这里使用 keepalived 和 Mariadb 的主主复制功能来实现 DB 的高可用。 …","ref":"/v3.3/docs/setup/db-ha/","title":"部署 DB HA 环境"},{"body":"为了方便代码提交记录的查看，以及以后的统计，我们制定了以下使用 Git 书写提交内容的规范，提交代码的时候请大家遵循以下的格式。\nGit 提交记录书写规范 格式如下:\n\u003ctype\u003e(\u003cscope\u003e): \u003csubject\u003e \u003cbody\u003e \u003cfooter\u003e 通过 git commit 命令填写的提交内容应该类似如上的结构，大致分为３个部分（每个部分使用空行分割）：\n 标题行：必填，描述主要修改类型和概要内容 主题内容：选填，描述为什么修改，做了什么样的修改，以及开发的思路，使用方法等等 页脚备注：选填，一些备注  每个部分的占位符说明如下:\n  type(PR 的类型):\n feat: 新功能 fix: 修复 refactor: 代码重构 test: 测试用例相关修改 chore: 其它修改，比如Makefile,Dockerfile等    scope(影响范围，相关组件): 比如 region, scheduler, cloudcommon，如果是多个组件，用英文 ‘,’ 分割，比如: (region,scheduler,monitor)\n  subject(commit 的概述): 建议不超过 50 个字符\n  body(commit 具体修改内容): 可以分为多行，建议每行不超过 72 个字符\n  footer(一些备注，选填): 一些备注，通常是相关参考连接、BREAKING CHANGE 或修复 bug 的连接\n  Commit 举例  fix(region): compute NextSyncTime for snapshotpolicydisk 1. 如果计算出来的 NextSyncTime 和 base 相等，可以将 base 加1一个小时递归处理。 2. 对于 retentionday 有效的快照策略，比如某一个 snaphsotpolicy 是每周一生效，并且打的快照自动保留 3 天，那么，就应该在每周一（打快照） 和每周四（释放快照）进行快照的同步。  feat(scheduler): add cpu filter Added new cpu filter to scheduler: - filter host by cpu model - set host capability by request cpu count  fix(apigateway,monitor,influxdb): disable influxdb service query proxy  feat(climc): support disable wrap line Usage: export OS_TRY_TERM_WIDTH=false climc server-list Closes #6710  辅助工具 也有相应的工具帮忙生成符合要求的 Commit message，使用 commitizen-go 可以协助生成 Commit message，用法如下:\n# osx 安装 commitizen-go $ brew tap lintingzhen/tap $ brew install commitizen-go # 如果是 linux 环境，可以 clone 源码编译，这个工具是 golang 写的，编译也简单 $ git clone https://github.com/lintingzhen/commitizen-go \u0026\u0026 cd commitizen-go $ make \u0026\u0026 ./commitizen-go install # 这一部会生成 git cz 命令的配置 $ sudo commitizen-go install # 测试提交 $ git add . # 使用 git cz 命令，就会以交互式的方式帮忙输入 commit $ git cz 相关配置参考：https://github.com/lintingzhen/commitizen-go/blob/master/README.MD#configure\n当然工具并不是说强制使用，只是说有工具的帮助，生成的 commit 内容会更统一一点　;)\n当然也有 nodejs 版本的工具，可能更符合前端的使用: https://github.com/commitizen/cz-cli ，前端的同学也可以使用这个工具。\n参考   https://docs.google.com/document/d/1QrDFcIiPjSLDn3EL15IJygNPiHORgU1_OOAqWjiDU5Y/edit#heading=h.fpepsvr2gqby\n  http://www.ruanyifeng.com/blog/2016/01/commit_message_change_log.html\n  https://blog.csdn.net/noaman_wgs/article/details/103429171\n  ","excerpt":"为了方便代码提交记录的查看，以及以后的统计，我们制定了以下使用 Git 书写提交内容的规范，提交代码的时候请大家遵循以下的格式。\nGit 提 …","ref":"/v3.3/docs/contribute/git-convention/","title":"Git 提交内容规范"},{"body":"环境准备 OneCloud 相关的组件运行在 kubernetes 之上，环境以及相关的软件依赖如下:\n 操作系统: Centos 7.6 最低配置要求: CPU 4核, 内存 8G, 存储 150G 数据库: mariadb (CentOS 7自带的版本：Ver 15.1 Distrib 5.5.56-MariaDB） docker: ce-19.03.9 kubernetes: v1.15.8  需要能访问如下网址，如果企业有外网隔离规则，则需要打开相应白名单：\n CentOS YUM网络安装源 https://iso.yunion.cn/ https://registry.cn-beijing.aliyuncs.com https://meta.yunion.cn https://yunionmeta.oss-cn-beijing.aliyuncs.com  安装配置 mariadb mariadb 作为服务数据持久化的数据库，可以部署在其它节点或者使用单独维护的。下面假设还没有部署 mariadb，在控制节点上安装设置 mariadb。\n为了方便运行维护，mariadb推荐打开两个参数设施：\n skip_name_resolve：取消域名解析 expire_logs_days=30：设置binlog的超时时间为30天，超过30天的binglog自动删除  $ MYSQL_PASSWD='your-sql-passwd' # 安装 mariadb $ yum install -y epel-release mariadb-server $ systemctl enable --now mariadb $ mysqladmin -u root password \"$MYSQL_PASSWD\" $ cat \u003c\u003cEOF \u003e/etc/my.cnf [mysqld] datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 # Settings user and group are ignored when systemd is used. # If you need to run mysqld under a different user or group, # customize your systemd unit file for mariadb according to the # instructions in http://fedoraproject.org/wiki/Systemd # skip domain name resolve skip_name_resolve # auto delete binlog older than 30 days expire_logs_days=30 [mysqld_safe] log-error=/var/log/mariadb/mariadb.log pid-file=/var/run/mariadb/mariadb.pid # # include all files from the config directory # !includedir /etc/my.cnf.d EOF $ mysql -uroot -p$MYSQL_PASSWD \\  -e \"GRANT ALL ON *.* to 'root'@'%' IDENTIFIED BY '$MYSQL_PASSWD' with grant option; FLUSH PRIVILEGES;\" $ systemctl restart mariadb 安装配置 docker 安装 docker\n$ yum install -y yum-utils bash-completion # 添加 yunion onecloud rpm 源 $ yum-config-manager --add-repo https://iso.yunion.cn/yumrepo-3.3/yunion.repo $ yum install -y docker-ce-19.03.9 docker-ce-cli-19.03.9 containerd.io 配置 docker\n$ mkdir -p /etc/docker $ cat \u003c\u003cEOF \u003e/etc/docker/daemon.json { \"bridge\": \"none\", \"iptables\": false, \"exec-opts\": [ \"native.cgroupdriver=systemd\" ], \"data-root\": \"/opt/docker\", \"live-restore\": true, \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"registry-mirrors\": [ \"https://lje6zxpk.mirror.aliyuncs.com\", \"https://lms7sxqp.mirror.aliyuncs.com\", \"https://registry.docker-cn.com\" ] } EOF 启动 docker\n$ systemctl enable --now docker 安装 onecloud 依赖内核 这里需要安装我们编译的内核，这个内核是基于上游 Centos 3.10.0-1062 编译的，默认添加了 nbd 模块，nbd 模块用于镜像相关的操作。\n# 安装内核 $ yum install -y \\  kernel-3.10.0-1062.4.3.el7.yn20191203 \\  kernel-devel-3.10.0-1062.4.3.el7.yn20191203 \\  kernel-headers-3.10.0-1062.4.3.el7.yn20191203 # 重启系统进入内核 $ reboot # 重启完成后，查看当前节点内核信息 # 确保为 3.10.0-1062.4.3.el7.yn20191203.x86_64 $ uname -r 3.10.0-1062.4.3.el7.yn20191203.x86_64 安装配置 kubelet 从 yunion onecloud rpm 的 yum 源安装 kubernetes 1.15.8，并设置 kubelet 开机自启动\n$ yum install -y bridge-utils ipvsadm conntrack-tools \\  jq kubelet-1.15.8-0 kubectl-1.15.8-0 kubeadm-1.15.8-0 $ echo 'source \u003c(kubectl completion bash)' \u003e\u003e ~/.bashrc \u0026\u0026 source ~/.bashrc $ source /etc/profile $ systemctl enable kubelet 安装完 kubernetes 相关的二进制后，还需要对系统做一些配置并启用 ipvs 作为 kube-proxy 内部的 service 负载均衡\n# 禁用 swap $ swapoff -a # 如果设置了自动挂载 swap，需要去 /etc/fstab 里面注释掉挂载 swap 那一行 # 关闭 selinux $ setenforce 0 $ sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config # 禁用 firewalld $ systemctl stop firewalld $ systemctl disable firewalld # 禁用 NetworkManager $ systemctl stop NetworkManager $ systemctl disable NetworkManager # 做一些 sysctl 的配置, kubernetes 要求 $ modprobe br_netfilter $ cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 EOF $ sysctl -p # 配置并开启 ipvs $ cat \u003c\u003cEOF \u003e /etc/sysconfig/modules/ipvs.modules #!/bin/bash ipvs_modules=\"ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4\" for kernel_module in \\${ipvs_modules}; do /sbin/modinfo -F filename \\${kernel_module} \u003e /dev/null 2\u003e\u00261 if [ $? -eq 0 ]; then /sbin/modprobe \\${kernel_module} fi done EOF $ chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026\u0026 bash /etc/sysconfig/modules/ipvs.modules \u0026\u0026 lsmod | grep ip_vs 部署集群 提示  如果要部署高可用集群，请先搭建负载均衡集群，参考 部署 HA 环境。\n  安装部署工具 先安装部署工具 ocadm 和云平台的命令行工具 climc:\n# 安装 climc 云平台命令行工具 和 ocadm 部署工具 $ yum install -y yunion-climc yunion-ocadm # climc 在 /opt/yunion/bin 目录下，根据自己的需要加到 bash 或者 zsh 配置文件里面 $ echo 'export PATH=$PATH:/opt/yunion/bin' \u003e\u003e ~/.bashrc \u0026\u0026 source ~/.bashrc # 安装必要的服务，并启动和设置为开机自启 $ yum install -y yunion-executor-server \u0026\u0026 systemctl enable --now yunion-executor 部署 kubernetes 集群 接下来会现在当前节点启动 v1.15.8 的 kubernetes 服务，然后部署 OneCloud 控制节点相关的服务到 kubernetes 集群。\n拉取必要的 docker 镜像\n$ ocadm config images pull 使用 ocadm 部署 kubernetes 集群\n提示  如果要进行高可用部署，并已经搭建好了负载均衡集群，需要在 ocadm init 命令加上 --control-plane-endpoint \u003cvip\u003e:6443 参数，告诉 kubernetes 集群前端的 LoadBalancer vip，之后生成的配置就会都用这个 vip 当做控制节点的入口。\n  # 假设 mariadb 部署在本地，如果是使用已有的数据库，请改变对应的 ip $ MYSQL_HOST=$(ip route get 1 | awk '{print $NF;exit}') # 如果是高可用部署，记得在设置 EXTRA_OPT=' --control-plane-endpoint 10.168.222.18:6443' $ EXTRA_OPT=\"\" $ #EXTRA_OPT=' --control-plane-endpoint 10.168.222.18:6443' # 开始部署 kubernetes 以及 onecloud 必要的控制服务，稍等 3 分钟左右，kubernetes 集群会部署完成 $ ocadm init --mysql-host $MYSQL_HOST \\  --mysql-user root --mysql-password $MYSQL_PASSWD $EXTRA_OPT ... Your Kubernetes and Onecloud control-plane has initialized successfully! ...  提示  kubernetes 高可用部署需要 3 个节点，主要是 etcd 需要至少 3 个节点组成高可用集群。如果是高可用部署，请在另外两个节点执行 ocadm join --control-plane \u003cvip\u003e:6443 部署控制服务，join 的另外两个节点会自动和当前的控制节点组成高可用集群。参考: 加入控制节点\n  kubernetes 集群部署完成后，通过以下命令来确保相关的 pod (容器) 都已经启动, 变成 running 的状态。\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-648bb4447c-57gjb 1/1 Running 0 5h1m kube-system calico-node-j89jg 1/1 Running 0 5h1m kube-system coredns-69845f69f6-f6wnv 1/1 Running 0 5h1m kube-system coredns-69845f69f6-sct6n 1/1 Running 0 5h1m kube-system etcd-lzx-ocadm-test2 1/1 Running 0 5h kube-system kube-apiserver-lzx-ocadm-test2 1/1 Running 0 5h kube-system kube-controller-manager-lzx-ocadm-test2 1/1 Running 0 5h kube-system kube-proxy-2fwgf 1/1 Running 0 5h1m kube-system kube-scheduler-lzx-ocadm-test2 1/1 Running 0 5h kube-system traefik-ingress-controller-qwkfb 1/1 Running 0 5h1m local-path-storage local-path-provisioner-5978cff7b7-7h8df 1/1 Running 0 5h1m onecloud onecloud-operator-6d4bddb8c4-tkjkh 1/1 Running 0 3h37m 创建 onecloud 集群 当 kubernetes 集群部署完成后，就可以通过 ocadm cluster create 创建 onecloud 集群，该集群由 onecloud namespace 里面 onecloud-operator deployment 自动部署和维护。\n# 创建集群 # 如果要部署企业版的组件可以在 cluster create 的时候加上 --use-ee 参数 $ ocadm cluster create --wait 执行完 ocadm cluster create --wait 命令后，onecloud-operator 会自动创建各个服务组件对应的 pod，等待一段该命令执行完毕， 就可以通过访问 ‘https://本机IP:443’ 登入前端界面。\n创建登录用户 当控制节点部署完成后，需要创建一个用于前端登录的用户。云平台的管理员认证信息由 ocadm cluster rcadmin 命令可以得到 , 这些认证信息在使用 climc 控制云平台资源时会用到。\n# 获取连接 onecloud 集群的环境变量 $ ocadm cluster rcadmin export OS_AUTH_URL=https://10.168.222.218:30357/v3 export OS_USERNAME=sysadmin export OS_PASSWORD=3hV3qAhvxck84srk export OS_PROJECT_NAME=system export YUNION_INSECURE=true export OS_REGION_NAME=region0 export OS_ENDPOINT_TYPE=publicURL  提示  如果是高可用部署，这些 endpoint 的 public url 会是 vip，如果要在 kubernetes 集群外访问需要到 haproxy 节点上添加对应的 frontend 和 backend，其中frontend的端口对应 endpoint 里面的端口，backend 对应 3 个 controlplane node 的 ip 和对应端口。\n  创建用户\n# 初始化连接集群的管理员认证信息 $ source \u003c(ocadm cluster rcadmin) # 设置想要创建的用户名和密码 $ OC_USERNAME=demo $ OC_PASSWORD=demo@123 # 创建指定的用户 $ climc user-create --password $OC_PASSWORD --enabled $OC_USERNAME # 将用户加入 system 项目并赋予 admin 角色 $ climc project-add-user system $OC_USERNAME admin 访问前端 # 获取本机 IP $ ip route get 1 | awk '{print $NF;exit}' 10.168.222.218 # 测试连通性 $ curl -k https://10.168.222.218 用浏览器访问 ‘https://本机IP’ 会跳转到 web 界面，使用 创建登录用户 里面指定的用户名和密码登录后，界面如下:\n删除环境 如果安装过程中失败，或者想清理环境，可执行以下命令删除 kubernetes 集群。\n$ ocadm reset --force 后续 如果没有意外，现在应该已经部署好了 onecloud on kubernetes 的集群，以下是一些后续的环节说明，可以根据自己的需要来进行额外的操作。\n添加计算节点 当控制节点搭建完成后，可以参考 计算节点 一节的内容，添加计算节点，组建一套私有云集群。\n控制节点作为计算节点 默认情况下 ocadm init 创建的节点是控制节点，不会运行 onecloud 计算节点的服务。如果需要把控制节点也作为计算节点，需要执行以下步骤:\n  安装计算节点需要的依赖，参考 “计算节点/安装依赖”，这里主要是要安装我们的内核和运行虚拟机的 qemu 等软件。\n  在控制节点启用该节点作为计算节点，命令如下:\n  # 用 kubectl get nodes 拿到当前的节点名称 $ kubectl get nodes NAME STATUS ROLES AGE controller01 Ready master 116d controller02 Ready master 40d node01 Ready \u003cnone\u003e 25d # 假设我要把 controller01 和 controller02 作为计算节点 $ ocadm node enable-host-agent \\  --node controller01 \\  --node controller02 # 等待并查看运行在 controller01/02 上的计算节点服务 $ kubectl get pods -n onecloud -o wide | grep host default-host-7b5cr 2/2 Running 218 18h 192.168.222.4 controller01 default-host-ctx5s 2/2 Running 218 18h 192.168.222.5 controller02 升级/回滚组件版本 ocadm init 的时候使用 --onecloud-version 选项设置了组件的版本，可以使用 ocadm cluster update 命令升级组件到指定的版本，保持更新。\n# 查看现在 onecloud cluster 的版本 $ kubectl get oc -n onecloud default -o jsonpath='{.spec.version}' v3.0.0-20200112.0 # 升级到 v3.0.0-20200113.0 $ ocadm cluster update --version v3.0.0-20200113.0 --wait ","excerpt":"环境准备 OneCloud 相关的组件运行在 kubernetes 之上，环境以及相关的软件依赖如下:\n 操作系统: Centos 7.6  …","ref":"/v3.3/docs/setup/controlplane/","title":"部署集群"},{"body":"架构简介 Cloudpods 服务组件较多，接下来分别介绍每个组件的功能:\n   服务组件 功能用途     keystone 认证权限管理   region 多云资源控制器   scheduler 资源调度器   glance 虚拟机镜像管理   host 私有云虚拟机管理   baremetal 私有云物理机管理   esxi-agent vmware esxi 实例管理   lb-agent 私有云负载均衡   webconsole 提供 vnc, ssh 访问   logger 记录审计日志   apigateway api 网关，能通过该服务访问后端所有 api   climc 命令行管理工具    组件架构见下图，分为接入层，控制层和资源层三个主要部分。\n接入层 接入层实现云管平台的访问功能，允许用户通过如下3种方式访问云管平台的功能：\n  API访问: 通过REST API访问云管平台功能，用户可以直接通过http接口访问云管平台的REST API，也可以使用云管平台提供的SDK。目前SDK支持Java，Python和Golang等三种语言。\n  命令行访问: 通过云管平台提供的climc命令行工具访问云管平台功能，允许用户通过脚本调用climc，实现一些自动化运维功能。Climc使用Golang语言，基于云管平台的Golang SDK开发。\n  Web控制台访问: 通过Web UI访问云管平台的功能。允许用户通过主流web浏览器访问云管平台。Web控制台提供管理员使用的管理后台以及普通用户使用的普通功能页面，能够提供大部分的管理和使用功能。Web控制台基于Vue 2.0 JavaScript SPA框架实现。\n  控制层   控制层实现云管平台的管理和控制功能。主要由API网关，认证服务，镜像服务，云控制器和调度器，以及 webconsole vnc, ssh 代理服务等组件构成。\n  API网关提供Web控制台对各个服务的统一REST API访问接口。实现Web控制台的登录验证，session 控制，以及对后端各个服务的API调用。API网关由Golang完全自主开发，完全无状态架构，具备水平扩展能力。\n  认证服务提供平台的账户管理和认证体系，并提供基于项目的多租户支持，同时提供服务目录功能。认证服务支持多种认证源，允许和企业的LDAP／AD对接，允许用户以企业统一的账户体系登入系统。认证服务2.10之前版本基于OpenStack Keystone Pika版本，开发语言为Python。在开源版本基础上，我们修正了BUG，并做了若干改进。2.10之后版本采用golang语言开发。Keystone采用无状态架构，支持水平扩展，可以水平拆分实现服务高可用。\n  镜像服务提供云管平台各种主机资源的操作系统镜像的管理功能。提供镜像存储，元数据管理等功能。镜像服务1.x版本基于OpenStack Glance Folsom版本改进而来，开发语言为Python。在开源版本基础上，我们修正了BUG，并做了若干改进。2.x版本采用golang语言开发。Glance采用无状态架构，支持水平扩展，可以水平拆分实现服务高可用。\n  云控制器是整个云管平台的中枢，负责机房网络，宿主机，网络，存储，虚拟机等各类资源的元数据信息管理，以及对虚拟机，裸机等的自动化管理操作认证的调度，协调管理。云控制器内置基于REST API接口的分布式异步任务管理框架，实现对在计算节点进行的开关机，创建删除等耗时操作任务的管理协调工作。云控制器完全自主开发，云控制器采用无状态架构，可以水平扩展，通过水平拆分实现高可用。\n  调度器负责云管平台资源调度功能，是云管平台中资源获取决策的唯一执行者，根据用户对资源的要求，给出资源的最优提供者。调度器支持批量调度，调度性能优异，可扩展性好。调度器完全自主开发，基于Golang语言开发。\n  资源层  资源层实现对KVM虚拟机，裸机，VMWare虚拟机等计算资源的管理和控制功能。云管平台目前主要支持对KVM虚拟机，裸机，VMWare虚拟机，常用私有云openstack, zstack 以及公有云阿里云，Azure，腾讯云，AWS等公有云资源的管理。  ","excerpt":"架构简介 Cloudpods 服务组件较多，接下来分别介绍每个组件的功能:\n   服务组件 功能用途     keystone …","ref":"/v3.3/docs/contribute/services/","title":"服务组件介绍"},{"body":"介绍云平台后端服务所用的框架和相关库的使用方法，建议先阅读 “开发相关/服务组件介绍” 了解各个服务大概的功能。\n后端服务框架 keystone, region, glance 等后端服务，都是用的同一套后端服务框架，这个框架是我们自己定义实现的，核心模块如下:\n  REST API: 负责解析客户端发送的 CRUD http 请求，将不同的请求对应到 Model Dispatcher 模块。\n  Model Dispatcher: 将客户端的请求分发到对应资源的业务操作。\n  Model: 定义云平台各种资源，会进行数据库读写相关操作，如果具体业务需要进行耗时操作，会通过 Task 机制来执行耗时任务。\n  Task: 后台处理异步耗时任务的模块，会通过更新 Model 的状态来更新任务的执行结果。\n  Cloudpods 代码结构  build: 打包rpm脚本 cmd: 可执行binary入口程序 pkg: 库  appsrv: 通用http服务框架 cloudcommon: 云平台服务框架，基于appsrv扩展  cloudcommon/options: 通用options cloudcommon/app: 通用服务初始化代码 cloudcommon/db: Model dispatcher和Models的基础实现 cloudcommon/db/lockman: 锁实现 cloudcommon/db/taskman: 异步任务框架      认证部分  客户端向服务发起请求前，需要从keystone获得token 客户端通过携带用户名密码调用keystone的/v3/auth/tokens接口获得token 客户端向服务发起的每次API请求都会在HTTP头携带该token，比如: X-Auth-Token: {token} 后端服务向keystone验证该token，获得用户的身份信息，执行后续API的流程 每个服务都有一个keystone注册的服务用户账号（user/password)，并且以admin角色加入system项目 服务启动后，会向keystone发起认证，获得admin token 用户通过API访问服务时，将在header携带token 使用这个admin token访问keystone的token验证接口，验证这个token，获得用户的身份信息  Model Dispatcher 把 REST API 和 Model 的方法进行一一映射\n   REST 请求 Model 方法 说明     GET /\u003cresources\u003e AllowListItems List的权限判断   - ListItemFilter 过滤   - GetCustomizeColumns 获得扩展字段的信息   GET /\u003cresources\u003e/\u003cres_id\u003e AllowGetDetails Get 的权限判断   - GetExtraDetails 获取扩展字段的信息   GET /\u003cresources\u003e/\u003cres_id\u003e/\u003cspec\u003e AllowGetDetails\u003cSpec\u003e 获取资源特定属性的权限判断   - GetDetails\u003cSpec\u003e 获取资源特定属性   POST /\u003cresources\u003e AllowCreateItem 创建操作的鉴权   - ValidateCreateData 校验和处理创建的数据   - CustomizeCreate 自定义的创建操作   - PostCreate 创建后的hook   - OnCreateComplete 创建完成的hook   POST /\u003cresources\u003e/\u003cres_id\u003e/\u003caction\u003e AllowPerformAction\u003cAction\u003e 某个资源执行特定操作的鉴权判断   - Perform\u003cAction\u003e 某个资源执行特定操作   PUT /\u003cresources\u003e/\u003cres_id\u003e AllowUpdateItem 对指定资源更新操作的鉴权   - ValidateUpdateData 校验和处理更新操作的数据   - PreUpdate 自定义的创建操作   - PostUpdate 创建后的hook   DELETE /\u003cresources\u003e/\u003cres_id\u003e AllowDeleteItem 删除指定资源的鉴权   - CustomizeDelete 自定义的删除操作   - PreDelete 删除前的hook   - Delete 执行删除操作   - PostDelete 删除后的hook    具体 restful 请求的绑定函数在: pkg/appsrv/dispatcher/dispatcher.go 文件中的 AddModelDispatcher 函数。\n数据库 ORM 模型 代码位于 cloudcommon/db\n 接口  IModelManager: 对应资源在数据库里面的表 IModel: 对应资源在数据库里面的单条数据   数据结构  SResourceBase: 基础资源  SStandaloneResourceBase: 基础设施的物理资源，没有具体ownerId的资源，如zone, host  SVirtualResourceBase: 虚拟资源，如虚拟机（guest)  SSharableVirtualResourceBase: 虚拟的可以共享的虚拟资源，如disk, network  SAdminSharableVirtualInfoBase: 管理配置用的可共享虚拟资源，如security group       SJointResourceBase: 联合数据类型，如虚拟网卡是虚拟机和网络的联合，虚拟磁盘挂在：虚拟机和虚拟磁盘的联合      举例 用虚拟机的 model 来举例，代码在: pkg/compute/models/guests.go。\nGuestManager 对应数据库里面的 guests_tbl，该对象嵌套 db.SVirtualResourceBaseManager 表示是虚拟资源的 Manager，这样会默认实现 db.IModelManager 接口，然后根据业务需要重写一些方法会比较方便。\nSGuest 对应 guests_tbl 数据库里面的每一行数据，由 GuestManager 管理，嵌套 db.SVirtualResourceBase 结构，默认就会有虚拟资源所需要的表结构，然后再定义一些虚拟机独有的属性比如 VcpuCount 表示 cpu 核数，VmemSize 表示内存大小。 在代码抽象后表示虚拟机实例，该对象会绑定对虚拟机具体的业务操作实现函数。\nimport \"yunion.io/x/cloudpods/pkg/cloudcommon/db\" ...... type SGuestManager struct { db.SVirtualResourceBaseManager } var GuestManager *SGuestManager func init() { GuestManager = \u0026SGuestManager{ SVirtualResourceBaseManager: db.NewVirtualResourceBaseManager( SGuest{}, \"guests_tbl\", \"server\", \"servers\", ), } GuestManager.SetVirtualObject(GuestManager) GuestManager.SetAlias(\"guest\", \"guests\") } type SGuest struct { db.SVirtualResourceBase db.SExternalizedResourceBase SBillingResourceBase VcpuCount int `nullable:\"false\" default:\"1\" list:\"user\" create:\"optional\"` // Column(TINYINT, nullable=False, default=1) \tVmemSize int `nullable:\"false\" list:\"user\" create:\"required\"` // Column(Integer, nullable=False)  BootOrder string `width:\"8\" charset:\"ascii\" nullable:\"true\" default:\"cdn\" list:\"user\" update:\"user\" create:\"optional\"` // Column(VARCHAR(8, charset='ascii'), nullable=True, default='cdn')  DisableDelete tristate.TriState `nullable:\"false\" default:\"true\" list:\"user\" update:\"user\" create:\"optional\"` // Column(Boolean, nullable=False, default=True) \tShutdownBehavior string `width:\"16\" charset:\"ascii\" default:\"stop\" list:\"user\" update:\"user\" create:\"optional\"` // Column(VARCHAR(16, charset='ascii'), default=SHUTDOWN_STOP)  KeypairId string `width:\"36\" charset:\"ascii\" nullable:\"true\" list:\"user\" create:\"optional\"` // Column(VARCHAR(36, charset='ascii'), nullable=True)  HostId string `width:\"36\" charset:\"ascii\" nullable:\"true\" list:\"admin\" get:\"admin\" index:\"true\"` // Column(VARCHAR(36, charset='ascii'), nullable=True) \tBackupHostId string `width:\"36\" charset:\"ascii\" nullable:\"true\" list:\"user\" get:\"user\"` Vga string `width:\"36\" charset:\"ascii\" nullable:\"true\" list:\"user\" update:\"user\" create:\"optional\"` // Column(VARCHAR(36, charset='ascii'), nullable=True) \tVdi string `width:\"36\" charset:\"ascii\" nullable:\"true\" list:\"user\" update:\"user\" create:\"optional\"` // Column(VARCHAR(36, charset='ascii'), nullable=True) \tMachine string `width:\"36\" charset:\"ascii\" nullable:\"true\" list:\"user\" update:\"user\" create:\"optional\"` // Column(VARCHAR(36, charset='ascii'), nullable=True) \tBios string `width:\"36\" charset:\"ascii\" nullable:\"true\" list:\"user\" update:\"user\" create:\"optional\"` // Column(VARCHAR(36, charset='ascii'), nullable=True) \tOsType string `width:\"36\" charset:\"ascii\" nullable:\"true\" list:\"user\" update:\"user\" create:\"optional\"` // Column(VARCHAR(36, charset='ascii'), nullable=True)  FlavorId string `width:\"36\" charset:\"ascii\" nullable:\"true\" list:\"user\" create:\"optional\"` // Column(VARCHAR(36, charset='ascii'), nullable=True)  SecgrpId string `width:\"36\" charset:\"ascii\" nullable:\"true\" get:\"user\" create:\"optional\"` // Column(VARCHAR(36, charset='ascii'), nullable=True) \tAdminSecgrpId string `width:\"36\" charset:\"ascii\" nullable:\"true\" get:\"admin\"` // Column(VARCHAR(36, charset='ascii'), nullable=True)  Hypervisor string `width:\"16\" charset:\"ascii\" nullable:\"false\" default:\"kvm\" list:\"user\" create:\"required\"` // Column(VARCHAR(16, charset='ascii'), nullable=False, default=HYPERVISOR_DEFAULT)  InstanceType string `width:\"64\" charset:\"ascii\" nullable:\"true\" list:\"user\" create:\"optional\"` } ...... 数据库锁 代码位于 cloudcommon/db/lockman:\n LockClass/ReleaseClass: 锁住一类实例，一般创建资源时候需要锁 LockObject/ReleaseObject: 锁住一个实例，一般修改资源实例是需要锁 LockRawObject/RelaseRawObject: 通用的锁  举例 pkg/cloudcommon/db/db_dispatcher.go 里面的 DoCreate 函数会创建对应 Model 的对象并插入数据到数据库，这个时候就需要加锁。\nfunc DoCreate(manager IModelManager, ctx context.Context, userCred mcclient.TokenCredential, query jsonutils.JSONObject, data jsonutils.JSONObject, ownerId mcclient.IIdentityProvider) (IModel, error) { lockman.LockClass(ctx, manager, GetLockClassKey(manager, ownerId)) defer lockman.ReleaseClass(ctx, manager, GetLockClassKey(manager, ownerId)) return doCreateItem(manager, ctx, userCred, ownerId, nil, data) } worker队列管理 为了避免不可预期的并发度，所有异步执行的代码都应该在worker内执行，以便于管理并发度。\n代码位于 appsrv/workers.go\nworkerman := appsrv.NewWorkerManager(name, parallel_cnt, …) workerman.Run(func() {…}, nil, nil) Task 机制 云平台的异步耗时任务会放在 Task 机制里面去执行，比如创建虚拟机操作，用户提交了请求，region 控制器校验参数合格后，会记录数据到数据库，然后马上返回客户端对应的虚拟机记录，与此同时，会开始执行创建虚拟机的 task，这个 task 会立即在后台执行，会通过更新虚拟机 SGuest model 的状态和记录操作日志来表示执行的成功或失败。\ntask 也是记录在数据库 tasks_tbl 里面的记录，对应的定义在: pkg/cloudcommon/db/taskman/tasks.go 里面，数据结构如下:\ntype STaskManager struct { db.SResourceBaseManager } var TaskManager *STaskManager func init() { TaskManager = \u0026STaskManager{ SResourceBaseManager: db.NewResourceBaseManager(STask{}, \"tasks_tbl\", \"task\", \"tasks\") } TaskManager.SetVirtualObject(TaskManager) } type STask struct { db.SResourceBase Id string `width:\"36\" charset:\"ascii\" primary:\"true\" list:\"user\"` // Column(VARCHAR (36, charset='ascii'), primary_key=True, default=get_uuid) ObjName string `width:\"128\" charset:\"utf8\" nullable:\"false\" list :\"user\"` // Column(VARCHAR(128, charset='utf8'), nullable=False)  ObjId string `width:\"128\" charset:\"ascii\" nullable:\"false\" lis t:\"user\" index:\"true\"` // Column(VARCHAR(ID_LENGTH, charset='ascii'), nullable=False)  TaskName string `width:\"64\" charset:\"ascii\" nullable:\"false\" list :\"user\"` // Column(VARCHAR(64, charset='ascii'), nullable=False)  UserCred mcclient.TokenCredential `width:\"1024\" charset:\"utf8\" nullable:\"false\" get :\"user\"` // Column(VARCHAR(1024, charset='ascii'), nullable=False)  // OwnerCred string `width:\"512\" charset:\"ascii\" nullable:\"true\"` // Column(VARCHAR (512, charset='ascii'), nullable=True) Params *jsonutils.JSONDict `charset:\"utf8\" length:\"medium\" nullable:\"false\" get:\"us er\"` // Column(MEDIUMTEXT(charset='ascii'), nullable=False)  Stage string `width:\"64\" charset:\"ascii\" nullable:\"false\" default:\"on_init\" list:\"u ser\"` // Column(VARCHAR(64, charset='ascii'), nullable=False, default='on_init')  taskObject db.IStandaloneModel `ignore:\"true\"` taskObjects []db.IStandaloneModel `ignore:\"true\"` } ......  Id: STask 里面的 Id 是该 task 记录的 Id ObjId: 对应资源对象的 Id，用于记录执行该 task 的对应操作的资源，比如某台虚拟机、磁盘的 Id UserCred: 存储执行 task 的用户信息 Params: 执行 task 的参数 TaskName: 对应 task 的名称 Stage: task 执行的阶段，默认为 OnInit  举例 以虚拟机关机这个操作来举例:\n 客户端发起 POST /servers/\u003cserver_id\u003e/stop 请求后，通过服务框架会执行 func (self *SGuest) PerformStop 函数，代码片段如下(位于: pkg/compute/models/guest_actions.go):  func (self *SGuest) PerformStop(ctx context.Context, userCred mcclient.TokenCredential, query jsonutils.JSONObject, data jsonutils.JSONObject) (jsonutils.JSONObject, error) { // XXX if is force, force stop guest \tvar isForce = jsonutils.QueryBoolean(data, \"is_force\", false) if isForce || utils.IsInStringArray(self.Status, []string{api.VM_RUNNING, api.VM_STOP_FAILED}) { return nil, self.StartGuestStopTask(ctx, userCred, isForce, \"\") } else { return nil, httperrors.NewInvalidStatusError(\"Cannot stop server in status %s\", self.Status) } }  SGuest 会执行 self.StartGuestStopTask 函数，该函数会去调用虚拟机不同的 Driver 执行关机操作  // pkg/compute/models/guest_actions.go func (self *SGuest) StartGuestStopTask(ctx context.Context, userCred mcclient.TokenCredential, isForce bool, parentTaskId string) error { ...... return self.GetDriver().StartGuestStopTask(self, ctx, userCred, params, parentTaskId) } // pkg/compute/guestdrivers/virtualization.go import \"yunion.io/x/cloudpods/pkg/cloudcommon/db/taskman\" ...... func (self *SVirtualizedGuestDriver) StartGuestStopTask(guest *models.SGuest, ctx context.Context, userCred mcclient.TokenCredential, params *jsonutils.JSONDict, parentTaskId string) error { task, err := taskman.TaskManager.NewTask(ctx, \"GuestStopTask\", guest, userCred, params, parentTaskId, \"\", nil) if err != nil { return err } task.ScheduleRun(nil) return nil } ......   taskman.TaskManager.NewTask(ctx, “GuestStopTask”, …) 这里面的 GuestStopTask 对应 pkg/compute/tasks/guest_stop_task.go 里面的 GuestStopTask，是通过 taskman 里面维护的一个 map 查找的。\n  task.ScheduleRun(nil) 会开始执行对应的 Task，默认会从 task 的默认 Stage OnInit 函数开始执行，所以通过 task 机制就会执行到 GuestStopTask.OnInit 函数。OnInit 函数最终会调用对应虚拟机的 driver 执行 RequestStopOnHost 函数并更新设置自己的 Stage 为 OnMasterStopTaskComplete。\n  对于虚拟机来说 RequestStopOnHost 函数会请求虚拟机所在的 host agent 关闭虚拟机，关机成功后会回调 region task 框架，该框架会根据 taskId 从数据库 load 回来 GuestStopTask，接着它设置的 Stage OnMasterStopTaskComplete 执行。\n  提示  这里失败会自动调用 OnGuestStopTaskCompleteFailed 函数，所以编写对应 task stage 函数时如果写 \u003cOnSometingComplete\u003e 函数时，必须也同时写 \u003cOnSometingCompleteFailed\u003e 函数来处理失败情况。\n   如果成功关机，OnMasterStopTaskComplete 调用 OnGuestStopTaskComplete 函数，该函数会把虚拟机的状态设置为 ready，并记录一条关机操作日志；如果失败会调用 OnGuestStopTaskCompleteFailed 函数，该函数会虚拟机状态设置为关机失败，并记录失败的原因。  func (self *GuestStopTask) OnInit(ctx context.Context, obj db.IStandaloneModel, data jsonutils.JSONObject) { guest := obj.(*models.SGuest) db.OpsLog.LogEvent(guest, db.ACT_STOPPING, nil, self.UserCred) self.stopGuest(ctx, guest) } func (self *GuestStopTask) stopGuest(ctx context.Context, guest *models.SGuest) { host := guest.GetHost() if host == nil { self.OnGuestStopTaskCompleteFailed(ctx, guest, jsonutils.NewString(\"no associated host\")) return } if !self.IsSubtask() { guest.SetStatus(self.UserCred, api.VM_STOPPING, \"\") } self.SetStage(\"OnMasterStopTaskComplete\", nil) err := guest.GetDriver().RequestStopOnHost(ctx, guest, host, self) ...... } func (self *GuestStopTask) OnMasterStopTaskComplete(ctx context.Context, guest *models.SGuest, data jsonutils.JSONObject) { ...... self.OnGuestStopTaskComplete(ctx, guest, data) } func (self *GuestStopTask) OnMasterStopTaskCompleteFailed(ctx context.Context, obj db.IStandaloneModel, reason jsonutils.JSONObject) { guest := obj.(*models.SGuest) self.OnGuestStopTaskCompleteFailed(ctx, guest, reason) } func (self *GuestStopTask) OnGuestStopTaskComplete(ctx context.Context, guest *models.SGuest, data jsonutils.JSONObject) { ...... guest.SetStatus(self.UserCred, api.VM_READY, \"\") ...... logclient.AddActionLogWithStartable(self, guest, logclient.ACT_VM_STOP, \"\", self.Us erCred, true) } func (self *GuestStopTask) OnGuestStopTaskCompleteFailed(ctx context.Context, guest *models.SGuest, reason jsonutils.JSONObject) { ...... db.OpsLog.LogEvent(guest, db.ACT_STOP_FAIL, reason.String(), self.UserCred) self.SetStageFailed(ctx, reason.String()) logclient.AddActionLogWithStartable(self, guest, logclient.ACT_VM_STOP, reason.String(), self.UserCred, false) } 如何增加一个新的服务  在keystone注册一个服务启用用的账户 在keystone注册service和endpoint 参考 cloudpods/pkg/logger实现服务代码 为服务准备一个配置文件，包含以下基础信息  假设服务名为 svc，用户和密码为 svcuser, svcuserpassword，服务监听地址为: http://localhost:8866, region 为 LocalTest，对应操作如下:\n# 创建 service $ climc service-create --enabled svc svc # 创建 endpoint，对应的 service 为 svc $ climc endpoint-create svc LocalTest internal http://localhost:8866 # 创建 user $ climc user-create --password svcuserpassword --enabled svcuser # 把 user 加入 system 项目 $ climc project-add-user system svcuser admin 配置信息如下\nregion:LocalTestport:8866auth_url:https://\u003ckeystone_url\u003e:35357/v3admin_user:svcuseradmin_password:svcuserpasswordadmin_tenant_name:system","excerpt":"介绍云平台后端服务所用的框架和相关库的使用方法，建议先阅读 “开发相关/服务组件介绍” 了解各个服务大概的功能。 …","ref":"/v3.3/docs/contribute/framework/","title":"后端服务框架"},{"body":"部署完集群后，整个 kubernetes 集群只有一个节点，onecloud 相关服务都运行在了该节点上，为了服务的高可用，我们可以继续添加节点到 kubernetes 集群。\n环境准备 参考 “部署集群/环境准备” 的流程，安装好 docker 和 kubelet。\n获取加入集群 token 然后在控制节点使用 ocadm 拿到加入集群的 token，在待部署节点使用 ocadm 加入集群，操作如下:\n在控制节点获取加入节点的 token\n$ ocadm token list | grep bootstrap 4s4meb.xvgk2bwpmbospn3s 23h 2019-07-10T15:41:10+08:00 authentication,signing The default bootstrap token generated by 'ocadm init'. system:bootstrappers:kubeadm:default-node-token  提示  如果 token 过期了，可以在管理节点使用 ocadm token create 创建新的 token 。\n  加入节点 加入已有 kubernetes 集群的节点有两种角色，‘controlplane’ 和 ‘node’。\ncontrolplane 角色的节点会运行 kube-apiserver、kube-controller-manager、kube-scheduler 和 etcd，加入 controlplane 节点的好处是让 kubernetes 控制相关服务和 etcd 变为高可用。\nnode 角色的节点只会运行 kubelet，运行负载容器。\n加入 controlplane 加入控制节点需要从已有的 kubernetes 集群下载证书，证书使用 certificate key 加密，通过以下方法获取 certificate-key\n$ ocadm init phase upload-certs [upload-certs] Storing the certificates in Secret \"kubeadm-certs\" in the \"kube-system\" Namespace [upload-certs] Using certificate key: afa5e18bacb3f50b424cbf815fce6d1bd916fe91b58ba467053dc6b460198c55 # 这里的 10.168.222.18 是控制节点的 ip，如果是高可用部署则为负载均衡器的 vip，请根据你的环境修改 $ ocadm join --control-plane 10.168.222.18:6443 \\  --token 4s4meb.xvgk2bwpmbospn3s \\  --certificate-key afa5e18bacb3f50b424cbf815fce6d1bd916fe91b58ba467053dc6b460198c55 \\  --discovery-token-unsafe-skip-ca-verification 加入 node # 这里的 10.168.222.18 是控制节点的 ip，如果是高可用部署则为负载均衡器的 vip, 请根据你的环境修改 $ ocadm join 10.168.222.18:6443 \\  --token 4s4meb.xvgk2bwpmbospn3s \\  --discovery-token-unsafe-skip-ca-verification ... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. ","excerpt":"部署完集群后，整个 kubernetes 集群只有一个节点，onecloud 相关服务都运行在了该节点上，为了服务的高可用，我们可以继续添加 …","ref":"/v3.3/docs/setup/components/","title":"添加 K8S 节点"},{"body":"cloudpods-operator 是单独编写的一个组件，作为一个长期运行的服务运行在 Kubernetes 集群内部，作用是自动搭建和维护 Cloudpods 所有服务。详细的介绍可以参考 cloudpods-operator 工作原理。\n简介 operator 在 K8S 里面创建一个叫做 OnecloudCluster 的资源，该资源里面定义了各个服务组件要使用的 docker 镜像仓库和版本，通过修改 OnecloudCluster 这个资源，可以实现对各个服务多方面的镜像版本控制。\n# 查看 OnecloudCluster 资源 $ kubectl get onecloudclusters.onecloud.yunion.io -n onecloud NAME KEYSTONE default registry.cn-beijing.aliyuncs.com/yunionio/keystone:archdev-v36 # 查看 default OnecloudCluster 资源的 YAML 详情 $ kubectl get onecloudclusters.onecloud.yunion.io -n onecloud default -o yaml # 进入 edit 编辑界面 $ kubectl edit onecloudclusters -n onecloud default 镜像控制 OnecloudCluster 资源的镜像版本控制的关键属性简介如下：\n   属性 作用 默认值     .spec.imageRepository 控制所有服务的镜像仓库地址 registry.cn-beijing.aliyuncs.com/yunionio   .spec.version 控制所有服务镜像的 tag 由部署时指定，比如 ‘v3.6.9’   .spec.$(组件名).repository 控制该组件镜像的仓库地址 默认没有设置，可以通过设置该值单独控制组件的镜像仓库   .spec.$(组件名).tag 控制该组件镜像的 tag 默认没有设置，可以通过设置该值单独控制组件的 tag    统一修改版本 通过修改 default onecloudcluster spec.imageRepository 和 spec.version 属性，就会把所有服务的镜像统一更改，以下是使用场景举例:\n 把所有的服务镜像通一修改到 archdev-v36，修改 spec.version  # 这里的 oc 是 onecloudcluster 的简写，也可以识别 $ kubectl edit oc -n onecloud default ... tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/controlplane # 修改这里的 version ，然后所有服务对应 pod 里面的 image tag 都会变成 archdev-v36 version: archdev-v36 vpcAgent: disable: false image: registry.cn-beijing.aliyuncs.com/yunionio/vpcagent:archdev-v36 ... 修改所有服务镜像仓库到 registry.cn-beijing.aliyuncs.com/zexi 拉取镜像，修改 spec.imageRepository  $ kubectl edit oc -n onecloud default spec: ... tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/controlplane # 这里修改 imageRepository 的值，所有的服务对应的 pod 里面的 image 都会从 `registry.cn-beijing.aliyuncs.com/zexi` 这个仓库拉取 imageRepository: registry.cn-beijing.aliyuncs.com/zexi influxdb: ... spec.imageRepository 和 spec.version 可以同时组合使用，这样就可以统一配置各个服务的镜像地址。  修改完这些属性后，可以查看 pods 的状态，会发现所有的 pods 都在重新拉取镜像启动。\n单独修改组件版本 通过修改各个组件里面的 spec.$(component).repository 和 spec.$(component).tag 属性，就会把这个服务对应的 deployment 或者 daemonset 的 image 修改。\n这两个属性设置 image 的优先级要高于外层的 spec.imageRepository 和 spec.version。也就是说通过修改组件的 repository 和 tag 属性，可以做到其它组件镜像不变的情况下，修改单个组件的 image ，这种机制在开发的时候有用。\n下面以 region 为例：\n 指定 spec.regionServer.repository 为 192.168.0.1:5000/yunionio 和 spec.regionServer.tag 为 lzx-dev，将会把 default-region deployment 里面的 image 改为: 192.168.0.1:5000/yunionio/region:lzx-dev。  $ kubectl edit oc -n onecloud default spec: ... regionServer: disable: false dnsDomain: cloud.onecloud.io # 这里设置 repository repository: 192.168.0.1:5000/yunionio # 这里设置 tag tag: lzx-dev dnsServer: 10.127.40.252 image: registry.cn-beijing.aliyuncs.com/yunionio/region:archdev-v36 imagePullPolicy: IfNotPresent replicas: 1 ... # 现在查看 default-region 这个 deployment # 会发现里面的 image 已经被 operator 按照 $(spec.regionServer.repository)/region:$(spec.regionServer.tag) 的格式修改了 $ kubectl get deployment -n onecloud default-region -o yaml | grep image: image: 192.168.0.1:5000/yunionio/region:lzx-dev image: 192.168.0.1:5000/yunionio/region:lzx-dev ","excerpt":"cloudpods-operator 是单独编写的一个组件，作为一个长期运行的服务运行在 Kubernetes 集群内部，作用是自动搭建和维 …","ref":"/v3.3/docs/contribute/operator-intro/","title":"Operator 相关"},{"body":"查询物理机 # list baremetal 记录 climc host-list --baremetal true # list 已经安装系统的物理机 climc host-list --baremetal true --occupied # list 未安装系统的物理机 climc host-list --baremetal true --empty # 查询物理机详情，包括硬件信息，机房信息 climc host-show \u003chost_id\u003e 注册物理机 climc host-create \u003chost_id\u003e 重新准备物理机 climc host-prepare \u003chost_id\u003e 获取物理机登录信息 climc host-logininfo \u003chost_id\u003e 获取串口登录控制台 climc webconsole-baremetal \u003chost_id\u003e 开/关机 climc host-start/host-stop \u003chost_id\u003e 进入/退出维护模式 climc host-maintenance/host-unmaintenance \u003chost_id\u003e 删除物理机 climc host-delete \u003chost_id\u003e 转换/回收宿主机 climc host-convert-hypervisor climc host-undo-convert \u003chost_id\u003e 裸金属服务器相关 安装操作系统 climc server-create \\  --hypervisor baremetal \\ # 指定 server 的类型为 baremetal --ncpu 24 \\ # 创建到 24 核 cpu 的物理机 --raid-config 'raid1:2:MegaRaid' \\ # 第1块盘，使用 MegaRaid 控制器上的(0-1)号两块物理盘做 raid1 --raid-config 'none:1' \\ # 第2块盘，使用 MegaRaid 控制器上的(2)号物理盘，不做 raid --raid-config 'raid10:4:MegaRaid' \\ # 第3快盘, 使用 MegaRaid 控制器上的(3-6)号四块物理盘做raid10 --disk CentOS-7.5.qcow2:100g \\ # 系统盘使用 CentOS-7.5.qcow2 镜像作为操作系统，大小为 100g，使用第1块 raid1 的盘 --disk 'autoextend:ext4:/opt' \\ # 分区挂载到 /opt, 使用第1块 raid1 的盘，文件系统为 ext4, 大小为(第一块盘总大小 - 该盘其他分区的大小(100g)) --disk 'autoextend:xfs:/data-nonraid' \\ # 分区挂载到 /data-nonraid, 使用第2块没做 raid 的盘, 文件系统为 xfs，使用所有空间 --disk 'autoextend:ext4:/data-raid10' \\ # 分区挂载到 /data-raid10, 使用第3块 raid10 的盘，文件系统为 ext4, 使用所有空间 \u003cserver_name\u003e \\ # 裸金属服务器名称 64g # 创建到 64g 内存大小的物理机 raid 配置和分区 调用 server-create 接口时通过 ‘–raid-config’ 传递参数来配置 raid，每个 raid-config 对应到操作系统可见的磁盘设备(/dev/sdx)。\n‘–disk’ 参数对应不同磁盘上的分区，分区对应到磁盘的逻辑为: 分区按照顺序创建到第1块磁盘上，当 disk 设置 autoextend 参数后，表示接下来的 disk 分区会创建到下一个磁盘，以此类推。\n raid 配置 API 参数:     Key Type value 解释     type(磁盘类型) string rotate(机械盘), ssd(固态盘), hybrid(未知) -   conf (raid) string none, raid0, raid1, raid5, raid10 做raid几或者不做   count (磁盘数量) int e.g. 0, 2, 4 小于等于物理机实际的盘数   range (磁盘范围) []int e.g. [0,1,2,3], [4,7], [5,6] 物理磁盘在控制器上的索引号   splits (切割物理盘) string (30%,20%,), (300g,100g,) 做好 raid 的物理盘再切割为多块物理盘   adapter (控制器号) int 0, 1 对应driver的 Adapter 控制器   driver (控制器名称) string MegaRaid,HPSARaid,Mpt2SAS,MarvelRaid,Linux,PCIE 1台物理机上有多个控制器时用于选择盘   strip (设置raid strip 大小) *int e.g. 64*1024 设置strip size, 可选   ra *bool  设置读模式   wt   设置写模式   cachedbadbbu *bool     direct *bool        命令行格式:\n‘(none,raid0,raid1,raid5,raid10):%d:(MegaRaid|HPSARaid|Mpt2SAS|MarvelRaid|Linux|PCIE):(rotate|ssd|hybrid):[0-n]:strip%dk:adapter%d:ra:nora:wt:wb:direct:cachedbadbbu:nocachedbadbbu’\n  查询裸金属服务器 climc server-list --hypervisor baremetal climc server-show \u003cserver_id\u003e 重装操作系统 climc server-rebuild --image \u003cimage_id\u003e \u003cserver_id\u003e 开/关机 climc server-start \u003cserver_id\u003e climc server-stop \u003cserver_id\u003e 删除裸金属服务器 删除 server 裸金属服务器会销毁物理机上的操作系统和 raid 配置，对应的 baremetal 重新进入未分配状态\nclimc server-delete \u003cserver_id\u003e ","excerpt":"查询物理机 # list baremetal 记录 climc host-list --baremetal true # list 已经安装 …","ref":"/v3.3/docs/howto/baremetal/operator/","title":"操作相关"},{"body":"climc server-create 命令提供创建云主机的操作。 OneCloud 可以同时管理多个私有云和公有云，不同供应商有各自的认证方式，在创建云主机之前需要做一些不同的准备工作。\n环境准备 OneCloud 虚拟机 OneCloud 提供自研的 kvm 虚拟机私有云管理平台，创建 kvm 虚拟机时需要有相应的宿主机，如果还没有添加 kvm 宿主机，请参考 安装部署/计算节点 注册对应的宿主机到云平台。\nVMware ESXI 虚拟机 TODO\n私有云 私有云和公有云都有自己的认证体系，为了让 OneCloud 能够管理各个云平台，需要把他们的认证信息导入到 OneCloud 平台。\n   平台 准备工作     openstack TODO   zstack TODO    公有云    平台 准备工作     阿里云 TODO   腾讯云 TODO   华为云 TODO   AWS TODO   Azure TODO   UCloud TODO    创建机器 创建机器命令为 server-create，可以使用 climc help server-create 查看创建 server 的所有参数，常用的参数如下：\n   参数名称 类型 作用     –ncpu int 虚拟机 cpu 个数   –disk []string 指定创建的系统盘镜像，指定多次表示虚拟机创建多块磁盘   –net []string 指定虚拟机使用的网络，指定多次将在虚拟机里面添加多个网卡   –allow-delete bool 允许删除虚拟机   –auto-start bool 创建完自动启动   –password string 设置虚拟机密码   –tenant string 创建到指定的项目   –prefer-region string 创建到指定的 region   –prefer-zone string 创建到指定的 zone   –prefer-host string 创建到指定的 host    注意以下几点:\n 名称、内存或者套餐类型在创建主机时必须使用; 系统盘的镜像通过 image-list 或者 cached-image-list，公有云的镜像列表通过 cached-image-list 接口查询，参考: 查询镜像;  下面以举例的方式创建机器：\n私有云主机 待创建规格:\n   名称 平台 套餐 内存 cpu 系统盘 网络 其他     vm1 kvm - 4g 4 centos7.qcow2 60g net1 2块数据盘， 一块100g ext4 挂载到 /opt，另外一块 50g xfs 挂载到 /data; 自动启动   vm2 esxi - 2g 2 ubuntu18.04.qcow2 100g net2 允许删除   vm3 opnstack t2.nano - - centos6.qcow2 net3 -    # 创建 kvm vm1 $ climc server-create --hypervisor kvm --disk centos7.qcow2:60g --disk 100g:ext4:/opt --disk 50g:xfs:/data --ncpu 4 --net net1 --auto-start vm1 4g # 创建 esxi vm2 $ climc server-create --hypervisor esxi --disk ubuntu18.04.qcow2:100g --net net2 --ncpu 2 --allow-delete vm2 2g # 创建 openstack vm3 $ climc server-create --hypervisor openstack --disk centos6.qcow2 --net net3 vm3 t2.nano 公有云主机 创建共有云主机和虚拟机的参数一致，但通常情况下需要通过 cloud-region-list 、zone-list 和 vpc-list 子命令挑选出各个公有云可用的 region, zone 和 network。\n然后 server-create 的时候通过 --prefer-region 或 --prefer-zone 创建到指定的区域，--net 创建到指定的 vpc 子网。\n# 查询 aliyun 的可用的 vpc $ climc vpc-list --provider Aliyun --details +--------------------------------------+-------------------------------------------+---------+-----------+--------------------------------------+------------+----------------+------------------------+ | ID | Name | Enabled | Status | Cloudregion_Id | Is_default | Cidr_Block | Region | +--------------------------------------+-------------------------------------------+---------+-----------+--------------------------------------+------------+----------------+------------------------+ | 6aabd4c5-8a6a-4ffb-83cd-39f924f773b7 | test12 | false | available | 9b0fdc39-701b-44fc-8842-664fe89359f1 | false | 192.168.0.0/16 | 阿里云 华北2（北京） | | 8f4d444f-cce4-4797-8441-e1b58c72ed26 | ali-yunion-bj | false | available | 9b0fdc39-701b-44fc-8842-664fe89359f1 | true | 172.17.0.0/16 | 阿里云 华北2（北京） | | bb8c1ec5-4577-4f84-8117-efab6586b799 | ali-transit-bj | false | available | 9b0fdc39-701b-44fc-8842-664fe89359f1 | false | 10.0.0.0/8 | 阿里云 华北2（北京） | | c4e1a012-5f2a-48fc-80ef-4ac0371006eb | hello | false | available | dbbfea2f-8bf4-4676-8036-4ad6f6e6b1ea | false | 10.0.0.0/8 | 阿里云 阿联酋（迪拜） | ... # 查询 vpc 6aabd4c5-8a6a-4ffb-83cd-39f924f773b7 下可用的 network $ climc network-list --vpc 6aabd4c5-8a6a-4ffb-83cd-39f924f773b7 +--------------------------------------+------------+----------------+-----------------+---------------+--------------------------------------+-----------+--------------+-----------------+-------------+-----------+ | ID | Name | Guest_ip_start | Guest_ip_end | Guest_ip_mask | wire_id | is_public | public_scope | guest_gateway | server_type | Status | +--------------------------------------+------------+----------------+-----------------+---------------+--------------------------------------+-----------+--------------+-----------------+-------------+-----------+ | b3dee5e6-0dce-403c-80b2-ad62880b662f | esrdfsfsd | 192.168.0.1 | 192.168.127.252 | 17 | a421934d-9cb4-4163-85b9-ad0038e9cb89 | true | system | 192.168.127.254 | guest | available | | d131de82-1be5-4f70-8b22-2303f4f409bb | sdfsdfdsff | 192.168.128.1 | 192.168.255.252 | 17 | 8ccdbe42-0c62-456f-842d-bc279a5c2786 | true | system | 192.168.255.254 | guest | available | +--------------------------------------+------------+----------------+-----------------+---------------+--------------------------------------+-----------+--------------+-----------------+-------------+-----------+ # 查询 region 9b0fdc39-701b-44fc-8842-664fe89359f1 下可用的 sku $ climc server-sku-list --region 9b0fdc39-701b-44fc-8842-664fe89359f1 --provider Aliyun # 创建 ecs.t5-lc2m1.nano aliyun vm4 虚拟机到 region 9b0fdc39-701b-44fc-8842-664fe89359f1 的子网 b3dee5e6-0dce-403c-80b2-ad62880b662f $ climc server-create --prefer-region 9b0fdc39-701b-44fc-8842-664fe89359f1 vm4 --hypervisor aliyun --net b3dee5e6-0dce-403c-80b2-ad62880b662f vm4 ecs.t5-lc2m1.nano ","excerpt":"climc server-create 命令提供创建云主机的操作。 OneCloud 可以同时管理多个私有云和公有云，不同供应商有各自的认证 …","ref":"/v3.3/docs/howto/server/create/","title":"创建云主机"},{"body":"列表 # 查询所有镜像列表 $ climc image-list # 查询所有缓存的镜像列表 $ climc cached-image-list # 查询包含 ubuntu 关键字的镜像 $ climc image-list --search ubuntu # 查询公有云包含 centos 关键字的缓存 $ climc cached-image-list --search centos --public-cloud # image-list 支持的查询条件 $ climc help image-list # cached-image-list 支持的查询条件 $ climc help cached-image-list 详情 根据 image-list 可以获取镜像的列表，第1、2列包含镜像的 id 和 name，通过 id 或 name 可以获取镜像的详情。\n# 查询名称包含 CentOS 的镜像 $ climc image-list --search centos +--------------------------------------+-----------------------------------------+-------------+-----------+-----------+-----------+-------------+----------+---------+--------+----------------------------------+----------------------------------+--------+----------------+ | ID | Name | Disk_format | Size | Is_public | Protected | Is_Standard | Min_disk | Min_ram | Status | Checksum | Tenant_Id | Tenant | is_guest_image | +--------------------------------------+-----------------------------------------+-------------+-----------+-----------+-----------+-------------+----------+---------+--------+----------------------------------+----------------------------------+--------+----------------+ | abf0fd6e-ec40-44ef-8fa2-cfb7187ea656 | CentOS-7-x86_64-GenericCloud-1711.qcow2 | qcow2 | 876740608 | false | true | true | 8192 | 0 | active | 317ecf7d1128e0e53cb285b8704dc3d3 | d53ea650bfe144da8ee8f3fba417b904 | system | false | +--------------------------------------+-----------------------------------------+-------------+-----------+-----------+-----------+-------------+----------+---------+--------+----------------------------------+----------------------------------+--------+----------------+ *** Total: 1 Pages: 1 Limit: 20 Offset: 0 Page: 1 *** # 查看 CentOS-7-x86_64-GenericCloud-1711.qcow2 的详情 $ climc image-show CentOS-7-x86_64-GenericCloud-1711.qcow2 +--------------------+-------------------------------------------------------------------------------------------------------------------+ | Field | Value | +--------------------+-------------------------------------------------------------------------------------------------------------------+ | can_delete | false | | can_update | true | | checksum | 317ecf7d1128e0e53cb285b8704dc3d3 | | created_at | 2020-06-16T09:17:57.000000Z | | delete_fail_reason | {\"error\":{\"class\":\"ForbiddenError\",\"code\":403,\"data\":{\"id\":\"image is protected\"},\"details\":\"image is protected\"}} | | disk_format | qcow2 | | domain_id | default | | fast_hash | 4c53ba2c464213ddc2a77c9b4c5ad3b7 | | id | abf0fd6e-ec40-44ef-8fa2-cfb7187ea656 | | is_data | false | | is_emulated | false | | is_guest_image | false | | is_public | false | | is_standard | true | | is_system | false | | min_disk | 8192 | | min_ram | 0 | | name | CentOS-7-x86_64-GenericCloud-1711.qcow2 | | oss_checksum | 317ecf7d1128e0e53cb285b8704dc3d3 | | owner | d53ea650bfe144da8ee8f3fba417b904 | | pending_deleted | false | | project_domain | Default | | project_src | local | | properties | {\"os_arch\":\"x86_64\",\"os_type\":\"Linux\"} | | protected | true | | public_scope | system | | size | 876740608 | | status | active | | tenant | system | | tenant_id | d53ea650bfe144da8ee8f3fba417b904 | | update_version | 8 | | updated_at | 2020-06-16T09:19:24.000000Z | +--------------------+-------------------------------------------------------------------------------------------------------------------+ ","excerpt":"列表 # 查询所有镜像列表 $ climc image-list # 查询所有缓存的镜像列表 $ climc …","ref":"/v3.3/docs/howto/image/query/","title":"查询镜像"},{"body":"你可能需要自己定制发行版的镜像，用于给不同的业务使用。本文介绍如何制作镜像。\n可以通过下载发行版操作系统的 iso , 然后本地启动虚拟机，将 iso 安装到虚拟机的磁盘，然后保存该磁盘，这个磁盘就可以作为镜像上传到 glance，但是这种方法人工参与的步骤太多，容易出错。\n推荐使用 packer 这个工具来自动化制作镜像，详细操作可以参考对应的文档 https://www.packer.io/docs/index.html 。\nhttps://github.com/yunionio/service-images 仓库包含了一些我们使用 packer 制作镜像的配置，可以参考使用。\n","excerpt":"你可能需要自己定制发行版的镜像，用于给不同的业务使用。本文介绍如何制作镜像。\n可以通过下载发行版操作系统的 iso , 然后本地启动虚拟机， …","ref":"/v3.3/docs/howto/image/create/","title":"制作镜像"},{"body":"云主机(server)指云平台管理的虚拟机和裸金属服务器。\n  虚拟机: 又叫做云服务器，包括我们提供的 kvm 虚拟机、vmware、openstack 和各个公有云的虚拟机。\n  裸金属: 云平台提供物理机(baremetal)装机功能，安装完操作系统并被云平台管理的服务器称为裸金属服务器。\n  现在支持的主机和平台的对应关系如下：\n   类型 平台     kvm onecloud 私有云虚拟机   baremetal onecloud 私有云裸金属   esxi vmware 虚拟机   openstack openstack 私有云虚拟机   zstack zstack 私有云虚拟机   aliyun 阿里云虚拟机   qcloud 腾讯云虚拟机   aws AWS 虚拟机   azure Azure 虚拟机   huawei 华为云虚拟机   ucloud UCloud 虚拟机    ","excerpt":"云主机(server)指云平台管理的虚拟机和裸金属服务器。\n  虚拟机: 又叫做云服务器，包括我们提供的 kvm 虚拟 …","ref":"/v3.3/docs/howto/server/","title":"云主机"},{"body":"主要结合应用场景介绍云平台各个资源的操作管理，首先会介绍命令行工具 climc 的用法，然后再具体介绍每种资源的操作。建议先熟悉命令行工具 climc 的使用过后在看后面的章节。\n熟悉命令行工具后，会分不同的部分介绍各种资源的操作和一些概念，云平台的资源大概分为 “虚拟资源” 和 “基础设施” 两类，有了基础设施类型的资源才能在其之上构建虚拟化的资源，具体分类如下:\n infra: 表示基础设施类型 virtual: 表示虚拟资源类型，属于具体的项目     名称 抽象资源 作用 类型     cloudregion 云平台地域 标记数据中心所在地域 infra   zone 云平台数据中心 标记数据中心 infra   vpc 逻辑隔离网络空间 抽象虚拟化网络的集合 infra   wire 对应二层扁平网络的广播域 抽象二层扁平网络广播域 infra   storage 存储 标记存储，提供云硬盘能力 infra   host 服务器 标记服务器，提供计算虚拟化 infra   server 云主机 运行在 host 上，使用虚拟化技术提供计算能力 virtual   disk 云硬盘 创建在 storage 上，使用虚拟化技术提供存储能力 virtual   network 网络 创建在 vpc 中，使用虚拟化技术提供网络 virtual   image 镜像 安装了操作系统的虚拟机磁盘，也属于 disk 一类 virtual   eip 外网浮动 ip 对应外网可用 ip virtual   loadbalancer 负载均衡器 标记负载均衡器，提供服务负载均衡 virtual    除了上面介绍的常见资源外，为了做多云管理，我们还引入了以下的概念:\n   名称 资源 作用 类型     cloudaccount 云平台的账户 对应各个云平台的认证信息 infra   project 项目 OneCloud 内部对虚拟机资源的划分 infra   schedtag 调度标签 可以标记多种资源，提供资源调度能力 infra   sku 套餐信息 对应创建虚拟资源的规格信息 infra    ","excerpt":"主要结合应用场景介绍云平台各个资源的操作管理，首先会介绍命令行工具 climc 的用法，然后再具体介绍每种资源的操作。 …","ref":"/v3.3/docs/howto/","title":"操作管理"},{"body":"如果需要构建内部私有云，就需要部署计算节点(宿主机)。计算节点主要负责虚拟机、网络和存储的管理，需要安装的组件如下:\n   组件 用途 安装方式 运行方式     host 管理 kvm 虚拟机和存储 - docker   host-deployer 虚拟机部署服务 - docker   sdnagent 管理虚拟机网络和安全组 - docker   openvswitch 虚拟机网络端口和流表配置 rpm systemd   qemu 运行虚拟机 rpm process   kernel onecloud 提供的内核 rpm -    环境   操作系统: Centos 7.x\n  硬件要求:\n Virtualization: CPU 要支持虚拟化，用于虚拟机 KVM 加速 打开 iommu，VT-d: 用于 GPU 透传(不用GPU可以不开)    网络:\n 当前可用的网段: 虚拟机可以直接使用和计算节点所在的扁平网段，需要预先划分保留对应端给云平台虚拟机使用，防止被其它设备占用，最后 IP 冲突    备注:\n 如果是以测试为目的，可以拿一台虚拟机部署计算节点的服务，但可能无法使用 KVM 加速和 GPU 透传    安装依赖 计算节点所需的依赖以 rpm 的方式安装\n# 添加 yum 源 $ cat \u003c\u003cEOF \u003e/etc/yum.repos.d/yunion.repo [yunion] name=Packages for Yunion Multi-Cloud Platform baseurl=https://iso.yunion.cn/yumrepo-3.3 sslverify=0 failovermethod=priority enabled=1 gpgcheck=0 EOF # 禁用防火墙和selinux $ systemctl disable firewalld $ sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config 安装 rpm 包\n$ yum --disablerepo='*' --enablerepo='yunion*' install -y \\  epel-release libaio jq libusb lvm2 nc ntp fetchclient fuse fuse-devel fuse-libs \\  oniguruma pciutils spice spice-protocol sysstat tcpdump usbredir \\  yunion-qemu-2.12.1 yunion-executor-server \\  kernel-3.10.0-1062.4.3.el7.yn20191203 \\  kernel-devel-3.10.0-1062.4.3.el7.yn20191203 \\  kernel-headers-3.10.0-1062.4.3.el7.yn20191203 \\  kmod-openvswitch \\  openvswitch net-tools $ systemctl enable --now yunion-executor # 安装完成后需要重启进入我们的内核 $ reboot # 重启完成后，查看当前节点内核信息，确保为 yn 内核 $ uname -r 3.10.0-1062.4.3.el7.yn20191203.x86_64 安装 docker 和 kubelet 参考 “部署集群/环境准备” 的流程，安装好 docker 和 kubelet。\n控制节点操作 以下操作在控制节点进行。\n创建计算节点所在的网段 我的环境计算节点的 ip 为 10.168.222.140，就创建一个对应的 计算节点(host)网段。\n提示 需要根据自己的计算节点环境创建对应的网段，如果不创建该网段，计算节点就没法注册进来。  # 查看当前环境的 zone $ climc zone-list +--------------------------------------+-------+--------+----------------+ | ID | Name | Status | Cloudregion_ID | +--------------------------------------+-------+--------+----------------+ | f73a2120-1206-45fa-8d43-de374ab0f494 | zone0 | enable | default | +--------------------------------------+-------+--------+----------------+ # 在 zone0 里面创建一个 wire bcast0，该资源抽象计算节点所在的二层广播域信息 $ climc wire-create zone0 bcast0 1000 # 在 wire bcast0 之上创建一个计算节点的网络，计算节点的 host 服务注册会用到，如果 host 注册时没有在云平台找到对应的网络，将会注册失败 $ climc network-create --gateway 10.168.222.1 --server-type baremetal bcast0 adm0 10.168.222.140 10.168.222.140 24 计算节点(host)操作 以下操作在计算节点进行，计算节点也叫 host，私有云计算节点上面会运行 host 服务来管理 kvm 虚拟机。\n配置 host 服务 参考 “添加节点/获取加入集群token” 的流程获取join所需的信息\n# 使用 ocadm join 来创建一台计算节点 # 可选参数 --host-networks: 配置host服务的网络，比如: 'eth0/br0/10.168.222.140', eth0是物理网卡，br0是网桥名称，10.168.222.140是宿主机的ip # 获取计算节点 IP $ host_addr=$(ip route get 1 | awk '{print $NF;exit}') $ echo $host_addr 10.168.222.140 # 可选参数 --host-local-image-path: 配置host服务磁盘的存储路径，比如: '/opt/cloud/workspace/disks' # 注意：容器部署的host服务只会挂载/opt/cloud目录 # 如果有其他挂载点需要bind mount到/opt/cloud下，可在fstab中添加一行如'/src /opt/cloud/dst none defaults,bind 0 0' # 可选参数 --host-hostname: 配置宿主机的hostname, 比如: 'node1' $ ./ocadm join $api_server_addr \\  --enable-host-agent \\  --token $token \\  --discovery-token-unsafe-skip-ca-verification # 然后等待宿主机上的host pod和host-deployer pod为running状态 控制节点启用 host 回到控制节点，启用刚才上报的计算节点，只有启用的宿主机才能运行虚拟机。\n# 使用 climc 查看注册的 host 列表 $ climc host-list +--------------------------------------+-------------------------+-------------------+----------------+----------------------------+---------+---------+-------------+----------+-----------+------------+---------------+--------------+------------+-------------------------+--------------+ | ID | Name | Access_mac | Access_ip | Manager_URI | Status | enabled | host_status | mem_size | cpu_count | node_count | sn | storage_type | host_type | version | storage_size | +--------------------------------------+-------------------------+-------------------+----------------+----------------------------+---------+---------+-------------+----------+-----------+------------+---------------+--------------+------------+-------------------------+--------------+ | 3830870e-a499-459d-89df-bb6979b5e1ff | lzx-allinone-standalone | 00:22:39:4c:6c:e9 | 10.168.222.140 | http://10.168.222.140:8885 | running | false | online | 8192 | 4 | 1 | Not Specified | rotate | hypervisor | master(7ab047419092301) | 50141 | +--------------------------------------+-------------------------+-------------------+----------------+----------------------------+---------+---------+-------------+----------+-----------+------------+---------------+--------------+------------+-------------------------+--------------+ *** Total: 0 Pages: 0 Limit: 20 Offset: 0 Page: 1 *** # 启动 host $ climc host-enable lzx-allinone-standalone 创建虚拟机测试 上传 cirrors 测试镜像 # 下载 cirros 测试镜像 $ wget https://iso.yunion.cn/yumrepo-2.10/images/cirros-0.4.0-x86_64-disk.qcow2 # 将镜像上传到 glance $ climc image-upload --format qcow2 --os-type Linux --min-disk 10240 cirros-0.4.0-x86_64-disk.qcow2 ./cirros-0.4.0-x86_64-disk.qcow2 # 查看上传的镜像 $ climc image-list +--------------------------------------+--------------------------------+-------------+----------+-----------+----------+---------+--------+----------------------------------+ | ID | Name | Disk_format | Size | Is_public | Min_disk | Min_ram | Status | Checksum | +--------------------------------------+--------------------------------+-------------+----------+-----------+----------+---------+--------+----------------------------------+ | 63f6f2af-4db2-4e30-85f5-0ad3baa27bd9 | cirros-0.4.0-x86_64-disk.qcow2 | qcow2 | 22806528 | false | 30720 | 0 | active | 76dc07d1a730a92d0db7fb2d3c305ecd | +--------------------------------------+--------------------------------+-------------+----------+-----------+----------+---------+--------+----------------------------------+ # 如果使用虚拟机作为计算节点，存储可能不大，可以把镜像的默认大小30g调整到10g $ climc image-update --min-disk 10240 cirros-0.4.0-x86_64-disk.qcow2 创建测试网络 下面是随机创建了一个主机间不可达的网络用于测试，如果有划分好的扁平二层可用网络，可以直接拿来给虚拟机使用。\n$ climc network-create --gateway 10.20.30.1 --server-type guest bcast0 vnet0 10.20.30.2 10.20.30.254 24 $ climc network-public vnet0 创建虚拟机 # 创建虚拟机 testvm01，512M内存, 1个CPU, 系统盘 10g, 第二块磁盘 5g 格式化为 ext4 并挂载到 /opt 的虚拟机 $ climc server-create --auto-start --allow-delete \\ \t--disk cirros-0.4.0-x86_64-disk.qcow2:10g --disk 5g:ext4:/opt \\ \t--net vnet0 --ncpu 1 --mem-spec 512M testvm01 # 查看创建的虚拟机，1分钟后应该会变为 running 状态 $ climc server-list --details +--------------------------------------+----------+--------------+--------------+-------+---------+------------+-----------+----------+-----------------------------+------------+---------+-------------------------+--------+-----------+ | ID | Name | Billing_type | IPs | Disk | Status | vcpu_count | vmem_size | Secgroup | Created_at | Hypervisor | os_type | Host | Tenant | is_system | +--------------------------------------+----------+--------------+--------------+-------+---------+------------+-----------+----------+-----------------------------+------------+---------+-------------------------+--------+-----------+ | bcda7d18-decc-4b5f-8654-2d201a84d1fb | testvm01 | postpaid | 10.20.30.254 | 35840 | running | 1 | 512 | Default | 2019-09-23T05:08:49.000000Z | kvm | Linux | lzx-allinone-standalone | system | false | +--------------------------------------+----------+--------------+--------------+-------+---------+------------+-----------+----------+-----------------------------+------------+---------+-------------------------+--------+-----------+ *** Total: 0 Pages: 0 Limit: 20 Offset: 0 Page: 1 *** # 获取虚拟机登录信息 $ climc server-logininfo testvm01 +-----------+------------------------------------------+ | Field | Value | +-----------+------------------------------------------+ | login_key | 49wqh5OWGW3jSr1A8RfrMoH69iRRECzaMZITBA== | | password | zS27FwwUFr96 | | updated | 2019-09-23T05:11:29.306403Z | | username | root | +-----------+------------------------------------------+ # 在计算节点联通测试网络(如果你是直接用的二层网络，应该能直接 ping 通虚拟机的 ip 了，不需要做这一步) $ ip address add 10.20.30.1/24 dev br0 # 用之前 server-logininfo 命令获取的用户名密码，直接登录到虚拟机里面 $ ssh root@10.20.30.254 PING 10.20.30.254 (10.20.30.254) 56(84) bytes of data. 64 bytes from 10.20.30.254: icmp_seq=1 ttl=64 time=1.31 ms # 如果网络不通，也可以通过 vnc 的方式打开虚拟机的 tty 登录界面，操作如下 # 打开 vnc 链接，用浏览器打开下面的链接 # 打开 vnc 链接时会出现不安全认证，导致 websocket 无法握手，需要在浏览器信任 webconsole server 对应的 endpoint $ climc endpoint-list --details | grep webconsole | grep public | 3da1e476aa7b4ff68e206754aed72d8f | region0 | 16120e8f3eec46dc86c59b3e426b0502 | webconsole | webconsole | https://10.168.222.218:8899 | public | true | # 然后用浏览器访问下 https://10.168.222.218:8899 , 信任该链接即可 # 在通过 webconsole-server 命令获取 vnc web 界面的链接地址，然后用浏览器打开该地址 $ climc webconsole-server testvm01 https://console.yunion.cn/web-console?access_token=FI-VXQSAonhzfSnxVTKCCbwHinp7swlRkmi-4p6s-4OfZpg6TG9YhWuwbHEUA1D7XoKu_w%3D%3D\u0026api_server=https%3A%2F%2F10.168.222.216%3A8899\u0026password=65xB2kaE\u0026protocol=vnc ","excerpt":"如果需要构建内部私有云，就需要部署计算节点(宿主机)。计算节点主要负责虚拟机、网络和存储的管理，需要安装的组件如下:\n   组件 用途 安装 …","ref":"/v3.3/docs/setup/host/","title":"添加计算节点"},{"body":"启用 baremetal-agent 之前需要部署 onecloud 集群，详见 “安装部署/部署集群”\n待集群准备完毕后指定 node 来部署 baremetal-agent 服务\n启用 baremetal-agent 在通过 pxe 引导流程中，baremetal-agent 只会处理来自 dhcp relay 服务器的请求, 所以你需要事先在交换机配置 dhcp relay 或者使用 onecloud host 服务的 dhcp relay 功能。\n如何配置 host 服务 启用 dhcp relay # 登录到已经部署好计算节点的服务器上修改 /etc/yunion/host.conf，添加 dhcp_relay 配置项： dhcp_relay: - 10.168.222.198 # baremetal agent dhcp服务监听地址 - 67 # baremetal agent dhcp服务监听端口 # 然后重启host服务 $ kubectl get pods -n onecloud -o wide | grep host default-host-p6d8h 2/2 Running 0 78m 10.168.222.189 k8s-dev1 \u003cnone\u003e \u003cnone\u003e default-host-xdc7x 2/2 Running 0 78m 10.168.222.150 k8s-dev2 \u003cnone\u003e \u003cnone\u003e # 找到对应的 pod 删除等待 host 服务自动重启 $ kubectl delete pods -n onecloud default-host-xdc7x 启用 baremetal-agent 然后选择 node 启用 baremetal-agent。\n# $listen_interface 指的是 baremetal-agent 监听的网卡名称 $ ocadm baremetal enable --node $node_name --listen-interface $listen_interface # 观察 baremetal agent pod 状态查看是否启动成功 $ watch \"kubectl get pods -n onecloud | grep baremetal\" default-baremetal-agent-7c84996c9b-hhllw 1/1 Running 0 3m10s # 启动成功确认 baremetal-agent 注册到控制节点 $ climc agent-list +--------------------------------------+--------------------------+----------------+-----------------------------+---------+------------+------------------------------------------+--------------------------------------+ | ID | Name | Access_ip | Manager_URI | Status | agent_type | version | zone_id | +--------------------------------------+--------------------------+----------------+-----------------------------+---------+------------+------------------------------------------+--------------------------------------+ | f3c2c671-c41d-4f30-8d04-e022b49bb9b5 | baremetal-10.168.222.150 | 10.168.222.150 | https://10.168.222.150:8879 | enabled | baremetal | remotes/origin/master(5e415506120011509) | 6230b485-2e54-480e-8284-33360b8202a8 | +--------------------------------------+--------------------------+----------------+-----------------------------+---------+------------+------------------------------------------+--------------------------------------+ 部署完成后可以参考 “操作管理/物理机” 来进行对物理机的注册管理\n禁用 baremetal-agent 可以在启用 baremetal-agent 的节点中选择节点禁止 baremetal-agent 调度到该节点。\nocadm baremetal disable --node $node_name ","excerpt":"启用 baremetal-agent 之前需要部署 onecloud 集群，详见 “安装部署/部署集群”\n待集群准备完毕后指定 node 来 …","ref":"/v3.3/docs/setup/baremetal/","title":"物理机管理服务"},{"body":"默认情况下部署好的版本是 开源版本(CE: Community Edition)，可以使用 ocadm cluster update 命令切换成 企业版本(EE: Enterprise Edition)。\n切换操作 # 切换到企业版 $ ocadm cluster update --use-ee --wait # 切换到开源版的 web 前端 $ ocadm cluster update --use-ce --wait ocadm cluster update --use-ee/--use-ce 命令会更新替换当前的 default-web deployment，执行该命令后等到新的 pod 启动后，重新刷新前端页面，即可进入(开源版/企业版)前端。\n常见问题 访问前端出现错误 问题原因: 开源与企业版本的前端分别依赖不同的 default-web configmap，直接切换过去会导致 default-web configmap 没有更新，会造成企业版本使用开源版本 configmap 的问题。\n解决办法: 在控制节点上删除 web 服务的 nginx configmap 配置文件，并重启 web 服务即可。\n# 删除 default-web 的 configmap 文件 $ kubectl delete configmap -n onecloud default-web # 重启 default-web 服务 $ kubectl rollout restart deployment -n onecloud default-web ","excerpt":"默认情况下部署好的版本是 开源版本(CE: Community Edition)，可以使用 ocadm cluster update 命令切 …","ref":"/v3.3/docs/setup/ce-ee-switch/","title":"切换到企业版"},{"body":"","excerpt":"","ref":"/v3.3/docs/contact/","title":"联系我们"},{"body":"宿主机(host): 指运行虚拟机的机器，云平台的抽象的宿主机根据 hypervisor 字段判断不同平台的宿主机。现在支持的类型如下：\n   类型 平台     hypervisor onecloud 私有云宿主机   baremetal onecloud 私有云物理机   esxi vmware 宿主机   openstack openstack 私有云宿主机   zstack zstack 私有云宿主机    ","excerpt":"宿主机(host): 指运行虚拟机的机器，云平台的抽象的宿主机根据 hypervisor 字段判断不同平台的宿主机。现在支持的类型如下： …","ref":"/v3.3/docs/howto/host/","title":"宿主机"},{"body":"创建好主机后，登录的方式大概分为以下几种：\n ssh: linux 通用，要求主机网络可达; rdp: windows 远程桌面，要求主机网络可达； vnc: vnc 链接，对主机网络没有要求，只要能链接云平台 vnc proxy 即可; ipmi sol: 只对装有 BMC 的物理机可用;  针对以上的链接方式，我们提供以下接口链接云主机：\nvnc 链接 climc webconsole-server 命令提供通过 vnc 的方式链接虚拟机，该方式对裸金属服务器不可用。\n$ climc webconsole-server \u003cserver_id\u003e ssh 链接 查询 server 的 ip\n# 可通过 server-list --search --details 的方式找到主机的 ip $ climc server-list --search \u003cserver_name\u003e --details # 或者通过 server-show \u003cserver_id\u003e 的方式得到 ip $ climc server-show \u003cserver_name\u003e | grep ip | ips | 10.168.222.226 | 查询 server 的登录信息\n$ climc server-logininfo \u003cserver_name\u003e +----------+-----------------------------+ | Field | Value | +----------+-----------------------------+ | password | @2aWXB6AmCbV | | updated | 2019-07-03T10:00:20.801716Z | | username | root | +----------+-----------------------------+ ssh 登录\n$ ssh root@10.168.222.226 通过 webconsole 登录\n$ climc webconsole-ssh 10.168.222.226 https://console.yunion.cn/web-console?access_token=y7bjpBwtvJHLHpwOUMzNVvsYiAgY1vskIuVwB-aINfH4mm8MsZqwxKSfHqm2pCvY6O8bBA%3D%3D\u0026api_server=https%3A%2F%2Foffice.yunion.io\u0026protocol=tty 在浏览器打开 webconsole 放回的 url ，就会到对应虚拟机的登录界面\n","excerpt":"创建好主机后，登录的方式大概分为以下几种：\n ssh: linux 通用，要求主机网络可达; rdp: windows 远程桌面，要求主机网 …","ref":"/v3.3/docs/howto/server/connect/","title":"登录云主机"},{"body":"开关机 # 开机 $ climc server-start \u003cserver_id\u003e # 关机 $ climc server-stop \u003cserver_id\u003e # 强制关机 $ climc server-stop --is-force \u003cserver_id\u003e # 重启 $ climc server-restart \u003cserver_id\u003e 删除 # 删除至回收站 $ climc server-delete \u003cserver_id\u003e # 彻底删除 $ climc server-delete -f \u003cserver_id\u003e 重装密码 $ climc server-deploy --reset-password --password \u003cyour_password\u003e \u003cserver_id\u003e TODO\n","excerpt":"开关机 # 开机 $ climc server-start \u003cserver_id\u003e # 关机 $ climc server-stop …","ref":"/v3.3/docs/howto/server/others/","title":"其他操作"},{"body":"术语解释   Baremetal: 指尚未安装操作系统的服务器， 也叫作物理机\n  PXE (Preboot eXecution Environment): 使用网络接口启动计算机的机制。这种机制不依赖本地数据存储设备（如硬盘）或本地已安装的操作系统，使用 DHCP 协议查找引导服务器并获取 IP，再通过 TFTP 协议下载初始引导程序和附加文件启动\n  DHCP (Dynamic Host Configuration Protocol): 动态主机设置协议是一个局域网的网络协议，使用UDP协议工作，为机器分配 IP\n  TFTP (Trivial File Transfer Protocol): 小型文件传输协议，使用UDP协议传输文件\n  DHCP Relay: 在不同子网和物理网段之间处理和转发dhcp信息的功能\n  IPMI (Intelligent Platform Management Interface)：管理服务器硬件的标准，特性是独立于操作系统外自行运行，即使在缺少操作系统或系统管理软件、或受监控的系统关机但有接电源的情况下仍能远程管理系统，也能在操作系统引导后运行\n  BMC (Baseboard management controller): 基板管理控制器，支持行业标准的 IPMI 规范\n  SSH (Secure Shell): 用于远程登录控制服务器\n  RAID (Redundant Array of Independent Disks): 磁盘阵列，把多个硬盘组合成为一个逻辑扇区，操作系统只会把它当作一个硬盘\n  Region Service: 云平台控制服务，提供 baremetal 相关 API\n  Baremetal Agent: 云平台管理 baremetal 的服务\n  Glance Service: 云平台镜像服务，提供物理机装机的 Image 镜像\n  裸金属服务器: baremetal 物理机安装操作系统后，在云平台创建的 server 的记录\n  宿主机: 可以运行云平台虚拟机的节点\n  ","excerpt":"术语解释   Baremetal: 指尚未安装操作系统的服务器， 也叫作物理机\n  PXE (Preboot eXecution …","ref":"/v3.3/docs/howto/baremetal/","title":"物理机"},{"body":"支持将libvirt管理的虚拟机导入到OneCloud\n注意事项   首先需要在libvirt管理的宿主机上安装我们的计算节点\n  安装好计算节点后需要添加的虚拟机的网络到控制节点\n  确保libvirt服务关闭\n  相关操作  准备好需要导入虚拟机的信息文件servers.yaml， 格式如下:  hosts: - host_ip: 10.168.222.137 xml_file_path: /etc/libvirt/qemu monitor_path: /var/lib/libvirt/qemu servers: - mac: 52:54:00:4A:19:AF ip: 10.168.222.53 - mac: 52:54:00:4A:19:CC ip: 10.168.222.54 - host_ip: 10.168.222.130 xml_file_path: /etc/libvirt/qemu monitor_path: /var/lib/libvirt/qemu servers: - mac: 53:54:00:4A:19:EC ip: 11.168.222.50 - mac: 53:54:00:4A:19:EE ip: 11.168.222.51 - `host_ip` 是要导入的宿主机的ip - `xml_file_path`是libvirt存储虚拟机xml文件的路径， - `monitor_path`是libvirt存储虚拟机monitor socket文件的路径， - `servers`是需要导入的虚拟机，里面描述了虚拟机的ip和mac对应关系    执行 climc servers-import-from-libvirt 开始导入  # 导入前确认libvirt服务关闭 $ climc servers-import-from-libvirt servers.yaml ","excerpt":"支持将libvirt管理的虚拟机导入到OneCloud\n注意事项   首先需要在libvirt管理的宿主机上安装我们的计算节点\n  安装好计 …","ref":"/v3.3/docs/howto/server/import/","title":"libvirt管理虚机导入"},{"body":"目前仅支持 OneCloud kvm 虚拟机使用 GPU，使用的 PCI Passthrough 的方式将宿主机上的 Nvidia/AMD GPU 透传给虚拟机使用。\n相关操作 创建 GPU 云主机  查询 gpu 列表  $ climc isolated-device-list --gpu +--------------------------------------+----------+---------------------+---------+------------------+--------------------------------------+ | ID | Dev_type | Model | Addr | Vendor_device_id | Host_id | +--------------------------------------+----------+---------------------+---------+------------------+--------------------------------------+ | 273f4f72-06b6-49aa-8456-4beceec44997 | GPU-HPC | GeForce GTX 1050 Ti | 41:00.0 | 10de:1c82 | 3bce9607-2597-469f-8d9b-977345456739 | | a77333e9-08d9-45c6-87eb-a7d8d902c5f5 | GPU-HPC | Quadro FX 580 | 05:00.0 | 10de:0659 | 3bce9607-2597-469f-8d9b-977345456739 | +--------------------------------------+----------+---------------------+---------+------------------+--------------------------------------+  创建 server  server-create 中的 --isolated-device 参数指定透传的设备到云主机，可以重复使用多次，透传多个 gpu 到云主机，但要求透传到同一云主机的 gpu 必须在同一宿主机。其余创建参数和创建普通云主机是一样的。\n$ climc server-create --hypervisor kvm --isolated-device 273f4f72-06b6-49aa-8456-4beceec44997 ... 查询 GPU 云主机 $ climc server-list --gpu 关联 GPU 如果云主机所在的宿主机有可用的 gpu，在主机关机的情况下，可以通过 server-attach-isolated-device 命令将 gpu 和云主机关联起来，下次主机启动后就可以使用该 gpu 。\n$ climc server-attach-isolated-device \u003cserver_id\u003e \u003cdevice_id\u003e 卸载 GPU 如果云主机关联了 gpu，可以通过 server-detach-isolated-device 卸载主机的某一 gpu。\n$ climc server-detach-isolated-device \u003cserver_id\u003e \u003cdevice_id\u003e ","excerpt":"目前仅支持 OneCloud kvm 虚拟机使用 GPU，使用的 PCI Passthrough 的方式将宿主机上的 Nvidia/AMD …","ref":"/v3.3/docs/howto/server/gpu/","title":"GPU相关"},{"body":"TODO\n","excerpt":"TODO\n","ref":"/v3.3/docs/howto/network/","title":"网络"},{"body":"TODO\n","excerpt":"TODO\n","ref":"/v3.3/docs/howto/lb/","title":"负载均衡"},{"body":"TODO\n","excerpt":"TODO\n","ref":"/v3.3/docs/howto/storage/","title":"存储"},{"body":"TODO\n","excerpt":"TODO\n","ref":"/v3.3/docs/howto/multicloud/","title":"多云管理"},{"body":"TODO\n","excerpt":"TODO\n","ref":"/v3.3/docs/howto/auth/","title":"认证与权限"},{"body":"TODO\n","excerpt":"TODO\n","ref":"/v3.3/docs/howto/server/migrate/","title":"迁移相关"},{"body":"","excerpt":"","ref":"/v3.3/docs/howto/container/","title":"容器集群"},{"body":"TODO\n","excerpt":"TODO\n","ref":"/v3.3/docs/howto/server/backup/","title":"主备机"},{"body":"TODO\n","excerpt":"TODO\n","ref":"/v3.3/docs/howto/scheduler/","title":"资源调度"},{"body":"导入镜像 云平台的 glance 镜像服务支持从外部 url 导入镜像，对应 climc 的子命令为 image-import　。\n# 导入 https://iso.yunion.cn/yumrepo-3.2/images/cirros-0.4.0-x86_64-disk.qcow2 镜像 $ climc image-import --format qcow2 --os-type Linux cirros-test.qcow2 https://iso.yunion.cn/yumrepo-3.2/images/cirros-0.4.0-x86_64-disk.qcow2 使用 image-list 或 image-show 查询导入镜像的状态，变为 active 时表明可以使用。\n下载镜像 如果需要将云平台的镜像导出到本地，就需要用 climc image-download 把 glance 存的镜像下载下来。\n参考 查询镜像 查询你想要下载的镜像，获取镜像 id 或 name。\n下载镜像:\n# OUTPUT 指定镜像的保存路径和文件名称，如/root/test.qcow2 $ climc image-download [--output OUTPUT] \u003cimage_id\u003e 删除镜像 镜像默认启用了删除保护，当镜像确定不用了，需要先通过climc image-update禁用删除保护，再通过 climc image-delete 删除镜像。\n# 禁用镜像删除保护 $ climc image-update --unprotected \u003cimage_id\u003e # 删除镜像 $ climc image-delete \u003cimage_id\u003e ","excerpt":"导入镜像 云平台的 glance 镜像服务支持从外部 url 导入镜像，对应 climc 的子命令为 image-import　。\n# …","ref":"/v3.3/docs/howto/image/operation/","title":"其他操作"},{"body":"本文介绍从 v3.2.x 升级到 v3.3.x 的步骤以及注意事项。\n版本升级建议从相邻的版本升级，比如从 v3.0.x 升级到 v3.2.x 需要以下的步骤：\n v3.0.x =\u003e v3.1.x v3.1.x =\u003e v3.2.x  直接跨多个版本升级可能会出现问题，建议参考以下的内容选择升级步骤:\n v3.1.x 升级到 v3.2.x  总体来说，升级的步骤如下:\n 更新 rpm 源，升级 ocadm 使用 ocadm 升级 OneCloud 服务  查看当前版本 可以使用 kubectl 查看当前集群的版本\n# 使用 kubectl 获得当前集群的版本为 v3.2.3 $ kubectl -n onecloud get onecloudclusters default -o=jsonpath='{.spec.version}' v3.2.3 更新 rpm repo ocadm 和 climc 这些命令行工具是以 yum rpm 包的方式安装，所以升级之前需要先更新这两个工具，然后再使用 ocadm 升级 OneCloud 服务。\n# 修改 baseurl，把 3.2 改成 3.3 $ sed -i 's|baseurl.*|baseurl=https://iso.yunion.cn/3.3|g' /etc/yum.repos.d/yunion.repo # 更新 yunion-ocadm, yunion-climc $ yum clean all $ yum install -y yunion-ocadm yunion-climc # 查看 ocadm 版本 $ ocadm version -o short tags/v3.3.0(bece1be20080211) 更新 OneCloud 服务 # 使用 ocadm 更新 onecloud operator 以及相关服务到 v3.3.0 版本 # 该步骤会因为拉取 docker 镜像等待较长时间，请耐心等待 $ ocadm cluster update --operator-version v3.3.0 --version v3.3.0 --wait # 另外可以在升级的过程中使用 kubectl 查看对应 pods 的升级情况 $ kubectl get pods -n onecloud --watch ","excerpt":"本文介绍从 v3.2.x 升级到 v3.3.x 的步骤以及注意事项。\n版本升级建议从相邻的版本升级，比如从 v3.0.x …","ref":"/v3.3/docs/setup/upgrade/","title":"升级相关"},{"body":"多云资源对接 完善多云对接的广度和深度，接入更多的云平台和云产品\n 支持 Azure RDS，Redis，LoadBalance，WebApp 等 支持 AWS Redis 等 对接 NAS，MongoDB，MQ 等通用云产品 对接有较强客户需求的其他云平台，例如中国移动公有云，青云等  统一监控报警 完善多云监控数据采集的能力，实现多云统一监控报警\n 全面采用 Prometheus 方案，从现有基于 Influxdb 的监控存储方案逐步迁移到 Prometheus 的监控方案，将监控后端采用多实例基于对象存储的分布式 Prometheus 集群方案 基于 Agent 的监控数据的采集，实现自动化安装 Agent ，通过 Agent 采集监控指标，获得更准确，更丰富和更精细化的监控指标，不仅涵盖基础设施（计算网络存储）的监控指标，含可以包含应用的监控指标（nginx，mysql等） 实现跨云监控数据采集，通过代理网关打通监控服务和资源的网络隧道，实现跨云采集内网监控 对接云平台自身的报警API，实现云平台自身报警和 Prometheus 报警的统一  多云应用和编排 以容器为基础，应用为核心，发展多云应用部署和编排能力\n 多 Kubernetes 集群的统一管理：完善多集群容器集群的数据采集和呈现，实现应用在多容器集群的部署和升级 多云容器集群部署：进一步完善在多云环境的容器部署能力，提高可靠性和速度，提高容器集群的性能和稳定性 多云应用部署：基于多容器容器集群实现多云应用部署 多云应用迁移：实现应用在多云环境的迁移  ","excerpt":"多云资源对接 完善多云对接的广度和深度，接入更多的云平台和云产品\n 支持 Azure …","ref":"/v3.3/docs/roadmap/","title":"ROADMAP"},{"body":"作者: 李泽玺\nKubernetes 里面的 Pod 资源是最小的计算单元，抽象了一组（一个或多个）容器。容器也是 Linux 系统上的进程，但基于 Namespace 和 Cgroups(Control groups) 等技术实现了不同程度的隔离。 简单来说 Namespace 可以让每个进程有独立的 PID, IPC 和网络空间。Cgroups 可以控制进程的资源占用，比如 CPU ，内存和允许的最大进程数等等。\n今天主要介绍如何通过 Cgroups 里面的 pids 控制器限制 Kubernetes Pod 容器的最大进程数量。\n场景介绍 之前遇到过这样一个问题，我们的服务会调用执行外部的命令，每调用一次外部命令就会 fork 产生子进程。但是由于代码上的 bug ，没有及时对子进程回收，然后这个容器不断 fork 产生子进程，耗尽了宿主机的进程表空间，最终导致整个系统不响应，影响了其它的服务。\n这种问题除了让开发人员修复 bug 外，也需要在系统层面对进程数量进行限制。所以，如果一个容器里面运行的服务会 fork 产生子进程，就很有必要使用 Cgroups 的 pids 控制器限制这个容器能运行的最大进程数量。\n解决方法 Kubelet 开启 PodPidsLimit 功能 Kubernetes 里面的每个节点都会运行一个叫做 Kubelet 的服务，负责节点上容器的状态和生命周期，比如创建和删除容器。根据 Kubernetes 的官方文档 Process ID Limits And Reservations 内容，可以设置 Kubelet 服务的 –pod-max-pids 配置选项，之后在该节点上创建的容器，最终都会使用 Cgroups pid 控制器限制容器的进程数量。\n我们 Kubernetes 是在 CentOS 7 上使用 kubeadm 部署的 v1.15.9 版本，需要额外设置 SupportPodPidsLimit 的 feature-gate，对应操作如下（其它发行版应该也类似）：\n# kubelet 使用 systemd 启动的，可以通过编辑 /etc/sysconfig/kubelet # 添加额外的启动参数，设置 pod 最大进程数为 1024 $ vim /etc/sysconfig/kubelet KUBELET_EXTRA_ARGS=\"--pod-max-pids=1024 --feature-gates=\\\"SupportPodPidsLimit=true\\\"\" # 重启 kubelet 服务 $ systemctl restart kubelet # 查看参数是否生效 $ ps faux | grep kubelet | grep pod-max-pids root 104865 10.5 0.6 1731392 107368 ? Ssl 11:56 0:30 /usr/bin/kubelet ... --pod-max-pids=10 --feature-gates=SupportPodPidsLimit=true 验证 PodPidsLimit 通过配置 Kubelet 的 –pod-max-pids=1024 选项，限制了一个容器内允许的最大进程数为 1024 个。现在来测试下如果容器内不断 fork 子进程，数目到达 1024 个时会触发什么行为。\n参考 Fork bomb 的内容，可以创建一个 pod，不断 fork 子进程。\n# 创建普通的 nginx pod yaml $ cat \u003c\u003cEOF \u003e test-nginx.yaml apiVersion: v1 kind: Pod metadata: name: test-nginx spec: containers: - name: nginx image: nginx EOF # 创建到 Kubernetes 集群 $ kubectl apply -f test-nginx.yaml # 进入 nginx 容器模拟 fork bomb  $ kubectl exec -ti test-nginx bash root@test-nginx:/# bash -c \"fork() { fork | fork \u0026 }; fork\" environment: fork: retry: Resource temporarily unavailable environment: fork: retry: Resource temporarily unavailable environment: fork: retry: Resource temporarily unavailable 通过进入一个 nginx 容器里面使用 bash 运行 fork bomb 命令，我们会发现当 fork 的子进程达到限制的上限数目后，会报 retry: Resource temporarily unavailable 的错误，这个时候再看下宿主机的 fork 进程数目。\n# 通过在外部宿主机执行下面的命令，会发现 fork 的进程数目接近 1024 个 $ ps faux | grep fork | wc -l 1019 通过以上的实验，发现能够通过设置 Kubelet 的 –pod-max-pids 选项，限制容器类的进程数，避免容器进程数不断上升最终耗尽宿主机资源，拖垮整个宿主机系统。\n原理实现 通过之前描述的解决方法，已经能够限制容器的进程数了。\n现在从代码的层面看下 Kubelet 如何设置 Cgroups pids 控制器。\nKubelet 代码调用 首先来看下 Kubelet 代码里面 –pod-max-pids 是怎么生效的，Kubernetes 的版本为 v1.15.9。\n–pid-max-pids 选项是在 cmd/kubelet/app/options/options.go 里面的 AddKubeletConfigFlags 函数设置的，对应代码如下。\n// cmd/kubelet/app/options/options.go func AddKubeletConfigFlags(mainfs *pflag.FlagSet, c *kubeletconfig.KubeletConfiguration) { ... // 这里定义了 '--pod-max-pids' 的选项  // 对应参数的值通过命令行解析到 kubeletconfig.KubeletConfiguration.PodPidsLimit 里面  fs.Int64Var(\u0026c.PodPidsLimit, \"pod-max-pids\", c.PodPidsLimit, \"Set the maximum number of processes per pod. If -1, the kubelet defaults to the node allocatable pid capacity.\") ... } PodPidsLimit 配置参数解析完成后，kubelet 会在启动的时候把值设置到 ContainerManager 里面，对应代码在 cmd/kubelet/app/server.go 里面的 run 函数，注释如下。\n// cmd/kubelet/app/server.go func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh \u003c-chan struct{}) (err error) { ... kubeDeps.ContainerManager, err = cm.NewContainerManager( ... cm.NodeConfig{ ... // 容器 runtime，默认使用 docker  ContainerRuntime: s.ContainerRuntime, // 使用 Cgroups 控制 pod 的服务质量  CgroupsPerQOS: s.CgroupsPerQOS, // 操作 Cgroups 的驱动，有 cgroupfs 和 systemd 两种  // 我们默认配置使用 systemd 来控制 Cgroups  CgroupDriver: s.CgroupDriver, ... // 这里就是 PodPidsLimit 的设置了，通过刚才 options 的解析  // 赋值到了 ContainerManager.ExperimentalPodPidsLimit 属性  ExperimentalPodPidsLimit: s.PodPidsLimit, // 限制容器 CPU 使用率的参数  EnforceCPULimits: s.CPUCFSQuota, CPUCFSQuotaPeriod: s.CPUCFSQuotaPeriod.Duration, }, ... ...) ... } 初始化 ContainerManager 后，会在 pkg/kubelet/cm/container_manager_linux.go 里面调用 NewPodContainerManager 创建 PodContainerManager，代码如下。\n// pkg/kubelet/cm/container_manager_linux.go ... func (cm *containerManagerImpl) NewPodContainerManager() PodContainerManager { // 默认情况下已经打开了 CgroupsPerQOS 的选项  if cm.NodeConfig.CgroupsPerQOS { // 这里返回 PodContainerManager 接口的实现  return \u0026podContainerManagerImpl{ qosContainersInfo: cm.GetQOSContainersInfo(), subsystems: cm.subsystems, cgroupManager: cm.cgroupManager, // 这里设置 podPidsLimit  podPidsLimit: cm.ExperimentalPodPidsLimit, enforceCPULimits: cm.EnforceCPULimits, cpuCFSQuotaPeriod: uint64(cm.CPUCFSQuotaPeriod / time.Microsecond), } } return \u0026podContainerManagerNoop{ cgroupRoot: cm.cgroupRoot, } } ... 从之前的代码能发现 PodContainerManager 是一个接口，对应的实现在 pkg/kubelet/cm/pod_container_manager_linux.go 里面，与 Cgroup 相关的函数则是 podContainerManagerImpl.EnsureExists 函数。\n// pkg/kubelet/cm/pod_container_manager_linux.go ... // podContainerManagerImpl 就是实现 PodContainerManager 接口的结构体 // EnsureExists 会根据 api 里面 Pod 的定义，在当前系统创建对应容器的 cgroup 配置 func (m *podContainerManagerImpl) EnsureExists(pod *v1.Pod) error { // podContainerName 也会作为 cgroup name，根据 pod 的 QOS 级别和 UUID 生成  // 查看容器是否存在  alreadyExists := m.Exists(pod) if !alreadyExists { // 创建 pod 对应容器的 cgroup  containerConfig := \u0026CgroupConfig{ Name: podContainerName, ResourceParameters: ResourceConfigForPod(pod, m.enforceCPULimits, m.cpuCFSQuotaPeriod), } // 如果启用了 SupportPodPidsLimit feature-gate ，并且 podPidsLimit 大于 0  if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.SupportPodPidsLimit) \u0026\u0026 m.podPidsLimit \u003e 0 { // 这里就会配置 PidsLimit  containerConfig.ResourceParameters.PidsLimit = \u0026m.podPidsLimit } // 调用 cgroupManager 根据 containerConfig 创建对应的 cgroup  if err := m.cgroupManager.Create(containerConfig); err != nil { return fmt.Errorf(\"failed to create container for %v : %v\", podContainerName, err) } } ... return nil } ... 接下来看 cgroupManager.Create 函数的实现，对应代码实现在 pkg/kubelet/cm/cgroup_manager_linux.go 里面的 cgroupManagerImpl.Create。\n... func (m *cgroupManagerImpl) Create(cgroupConfig *CgroupConfig) error { ... resources := m.toResources(cgroupConfig.ResourceParameters) libcontainerCgroupConfig := \u0026libcontainerconfigs.Cgroup{ Resources: resources, } // libcontainer consumes a different field and expects a different syntax  // depending on the cgroup driver in use, so we need this conditional here.  if m.adapter.cgroupManagerType == libcontainerSystemd { // 我们使用 systemd 管理 cgroup ，所以这里会更新下 systemd 对应 cgroup 的配置  updateSystemdCgroupInfo(libcontainerCgroupConfig, cgroupConfig.Name) } else { libcontainerCgroupConfig.Path = cgroupConfig.Name.ToCgroupfs() } if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.SupportPodPidsLimit) \u0026\u0026 cgroupConfig.ResourceParameters != nil \u0026\u0026 cgroupConfig.ResourceParameters.PidsLimit != nil { // 设置 libcontainerCgroupConfig 里面的 PidsLimit  // 这里 PidsLimit 就是一开始参数指定的 --pod-max-pids 的值  libcontainerCgroupConfig.PidsLimit = *cgroupConfig.ResourceParameters.PidsLimit } // 这里根据 cgroup 的配置返回 libcontainercgroups.Manager 接口的实现  // 这里的实现是 systemd 的实现  manager, err := m.adapter.newManager(libcontainerCgroupConfig, nil) if err != nil { return err } // 调用 libcontainer 里面的 cgroups manager Apply 接口把 pod 的 cgroup 配置应用到系统  // 在我们的环境中，这个 Apply 函数会由 libcontainer/cgroupfs/systemd.Manager 实现  if err := manager.Apply(-1); err != nil { return err } ... return nil } ... 在看下最后的 Apply 函数，该函数会调用到 vendor/github.com/opencontainers/runc/libcontainer/cgroups/systemd/apply_systemd.go 里面的 systemd 实现。\n// vendor/github.com/opencontainers/runc/libcontainer/cgroups/systemd/apply_systemd.go ... func (m *Manager) Apply(pid int) error { // 初始化 systemd cgroup 需要的一些变量  var ( c = m.Cgroups // systemd unit name  unitName = getUnitName(c) slice = \"system.slice\" // systemd unit 里面的配置属性  properties []systemdDbus.Property ) ... // Always enable accounting, this gets us the same behaviour as the fs implementation,  // plus the kernel has some problems with joining the memory cgroup at a later time.  properties = append(properties, newProp(\"MemoryAccounting\", true), newProp(\"CPUAccounting\", true), newProp(\"BlockIOAccounting\", true)) ... if c.Resources.Memory != 0 { // 设置 cgroup memory limit  properties = append(properties, newProp(\"MemoryLimit\", uint64(c.Resources.Memory))) } if c.Resources.CpuShares != 0 { // 设置 cgroup cpu shares  properties = append(properties, newProp(\"CPUShares\", c.Resources.CpuShares)) } ... if c.Resources.BlkioWeight != 0 { // 设置 cgroup block io weight  properties = append(properties, newProp(\"BlockIOWeight\", uint64(c.Resources.BlkioWeight))) } if c.Resources.PidsLimit \u003e 0 { // 这里设置了本文关注的 PidsLimit 参数  // 发现会对应 systemd 里面的 TasksAccounting 和 TasksMax 属性  properties = append(properties, newProp(\"TasksAccounting\", true), newProp(\"TasksMax\", uint64(c.Resources.PidsLimit))) } ... // 通过 systemdDbus 根据之前的 cgroup 设置创建对应的 unit  if _, err := theConn.StartTransientUnit(unitName, \"replace\", properties, statusChan); err == nil { ... } // 最后加入 Cgroups  if err := joinCgroups(c, pid); err != nil { return err } ... } ... Systemd Cgroup slice 通过对 Kubelet 调用 libcontainer，最后由 systemd 创建 pod 容器对应 cgroup unit 的代码调用分析，在这里看下对应 pod 的 systemd unit 配置。\n从之前代码看，最终生成的 systemd unit 和 cgroup 和 pod 的 uid 和 qosClass 有关系，所以先通过以下的命令拿到 pod 的 uid 和 qosClass。\n$ kubectl get pods test-nginx -o yaml | grep -E 'uid|qos' uid: 2ac1e32c-d8d6-4533-8eab-d04d60465065 qosClass: BestEffort 然后找到对应的 systemd unit .slice 文件。\n# uid 取前 8 位，qosClass 小写 # 找到对应的 kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice $ systemctl | grep 2ac1e32c | grep besteffort kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice loaded active active libcontainer container kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice # 查看对应 slice 的配置 $ systemctl status kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice ● kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice - libcontainer container kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice Loaded: loaded (/run/systemd/system/kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice; static; vendor preset: disabled) Drop-In: /run/systemd/system/kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice.d └─50-BlockIOAccounting.conf, 50-CPUAccounting.conf, 50-CPUShares.conf, 50-DefaultDependencies.conf, 50-Delegate.conf, 50-Description.conf, 50-MemoryAccounting.conf, 50-TasksAccounting.conf, 50-TasksMax.conf, 50-Wants-kubepods-besteffort\\x2eslice.conf Active: active since Fri 2021-06-25 16:21:25 CST; 7min ago Tasks: 6 (limit: 1024) Memory: 6.8M CGroup: /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice ├─docker-2d151786c9985db74632c09412207fa99755473fde93d09920604e097f25a2b7.scope │ ├─32662 nginx: master process nginx -g daemon off; │ ├─32703 nginx: worker process │ ├─32704 nginx: worker process │ ├─32705 nginx: worker process │ └─32706 nginx: worker process └─docker-966047566d9e90d9ef64126b605101c174d750ec0cde3d3a83c5b313c7af9a21.scope └─32544 /pause Jun 25 16:21:25 centos7-oc-dev systemd[1]: Created slice libcontainer container kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice. # 通过 systemctl status 能发现 50-TasksMax.conf 的文件 # 从之前的代码分析，发现 PodPidsLimit 会对应到 systemd 的 TasksMax 属性 # 现在在看下这个文件的内容 $ cat /run/systemd/system/kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice.d/50-TasksMax.conf [Slice] TasksMax=1024 # TasksMax 设置为了 1024 ，限制了这个进程最大子进程（Task）数量 Cgroup FS 查看之前的 vendor/github.com/opencontainers/runc/libcontainer/cgroups/systemd/apply_systemd.go 代码，发现在创建完 pod 容器对应的 systemd cgroup slice 后，还会调用一次 joinCgroups 这个函数。这个函数会使用 Cgroup FS 原生的方法，在 /sys/fs/cgroup 里面创建对应 pod 容器的 group 。\n所以再看下 Cgroup FS 里面 pod 设置 pid limit 的配置。\n# 找到 Cgroup FS pids 控制器的挂载点 $ cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids) # 看下 /sys/fs/cgroup/pids 目录下的文件 $ ls -alh /sys/fs/cgroup/pids ... # 发现有一个由 Kubelet 创建的 kubepods.slice drwxr-xr-x 4 root root 0 Jun 25 04:49 kubepods.slice ... # 再通过查看 /sys/fs/cgroup/pids/kubepods.slice 目录 # 会发现 kubepods-besteffort.slice 和 kubepods-burstable.slice 两个目录 # 分别对应 pod 容器的 QOS 级别 $ ls -alh /sys/fs/cgroup/pids/kubepods.slice ... drwxr-xr-x 42 root root 0 Jun 25 16:21 kubepods-besteffort.slice drwxr-xr-x 8 root root 0 Jun 25 04:49 kubepods-burstable.slice ... # 结合刚才的代码片段，也可以想到原生 Cgroup FS 的目录和 systemd 的应该是差不多的层级 # 现在直接用 find 命令查看 pids 控制器下面的 cgroup 设置 $ find /sys/fs/cgroup/pids/kubepods.slice -type f | grep pod2ac1e32c ... # 能发现 pids.current 和 pids.max 两个 cgroup 的配置 # pids.current 表示当前 pod 里面的进程（Task）数量 /sys/fs/cgroup/pids/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice/pids.current # pids.max 则表示 pod 里面能运行的进程（Task）上限 /sys/fs/cgroup/pids/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice/pids.max ... # 查看 pod pids.max 设置，结果为 1024 $ cat /sys/fs/cgroup/pids/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2ac1e32c_d8d6_4533_8eab_d04d60465065.slice/pids.max 1024 另外这篇内核文档 Process Number Controller 对 cgroup pids 控制器的使用进行了介绍，可以了解下。\n","excerpt":"作者: 李泽玺\nKubernetes 里面的 Pod 资源是最小的计算单元，抽象了一组（一个或多个）容器。容器也是 Linux 系统上的进 …","ref":"/v3.3/blog/2021/06/25/cgroups-kubernetes-pid-limits/","title":"使用 Cgroups 限制 Kubernetes Pod 进程数"},{"body":"作者: 李泽玺\nLinux 上虚拟机 GPU 透传需要使用 vfio 的方式。主要是因为在 vfio 方式下对虚拟设备的权限和 DMA 隔离上做的更好。但是这么做也有个缺点，这个物理设备在主机和其他虚拟机都不能使用了。\nqemu 直接使用物理设备本身命令行是很简单的，关键在于事先在主机上对系统、内核和物理设备的一些配置。\n单纯从 qemu 的命令行来看，其实和普通虚拟机启动就差了最后那个 -device 的选项。这个选项也比较容易理解，就是把主机上的设备 0000:00:01.0 传给了虚拟机使用。\n$ qemu-system-x86_64 -m 4096 -smp 4 --enable-kvm \\  -drive file=~/guest/fedora.img \\  -device vfio-pci,host=0000:00:01.0 系统及硬件准备 BIOS中打开IOMMU 设备直通在 x86 平台上需要打开 iommu 功能。这是 Intel 虚拟技术 VT-d(Virtualization Technology for Device IO) 中的一个部分。有时候这部分的功能没有被打开。\n打开的方式在 BIOS 设置中 Security-\u003eVirtualization-\u003eVT-d 这个位置。当然不同的 BIOS 位置可能会略有不同。记得在使用直通设备前要将这个选项打开。\n内核配置勾选IOMMU INTEL_IOMMU │ Location: │ │ -\u003e Device Drivers │ │ (2) -\u003e IOMMU Hardware Support (IOMMU_SUPPORT [=y]) 内核启动参数enable IOMMU BIOS 中打开，内核编译选项勾选还不够。还需要在引导程序中添加上内核启动参数\n# 对应编辑 /etc/default/grub, 设置 GRUB_CMDLINE_LINUX= $ cat /etc/default/grub ... GRUB_CMDLINE_LINUX=\"intel_iommu=on iommu=pt vfio_iommu_type1.allow_unsafe_interrupts=1 rdblacklist=nouveau nouveau.modeset=0\" ... # 重新生成 grub 引导配置文件 $ grub2-mkconfig -o /boot/grub2/grub.cfg # 将vfio相关 module 设置为开机load $ cat /etc/modules-load.d/vfio.conf vfio vfio_iommu_type1 vfio_pci vfio_virqfd Setting up IOMMU Kernel parameters\n找到 nvidia GPU BusID record PCI addresses and hardware IDs of the GPU\n$ lspci -k | grep -i nvidia -A 3 41:00.0 VGA compatible controller: NVIDIA Corporation GP107 [GeForce GTX 1050 Ti] (rev a1) Subsystem: Device 1b4c:11bf Kernel driver in use: vfio-pci Kernel modules: nouveau 41:00.1 Audio device: NVIDIA Corporation GP107GL High Definition Audio Controller (rev a1) Subsystem: Device 1b4c:11bf Kernel driver in use: snd_hda_intel Kernel modules: snd_hda_intel # pci address =\u003e 41:00.0,41:00.1 # device id =\u003e 1b4c:11bf # 这里找到了两张 nvidia 卡，它们的 device id 都是 1b4c:11bf, 一张是 Audio device # 这样是不能 passthrough 进去的，因为: # vfio-pci use your vendor and device id pair to identify which device they need to bind to at boot, # if you have two GPUs sharing such an ID pair you will not be able to get your passthough driver to bind with just one of them # 使用下面的脚本解决这种情况： $ cat /usr/bin/vfio-pci-override.sh #!/bin/sh for i in $(find /sys/devices/pci* -name boot_vga); do if [ $(cat \"$i\") -eq 0 ]; then GPU=\"${i%/boot_vga}\" AUDIO=\"$(echo \"$GPU\" | sed -e \"s/0$/1/\")\" echo \"vfio-pci\" \u003e \"$GPU/driver_override\" if [ -d \"$AUDIO\" ]; then echo \"vfio-pci\" \u003e \"$AUDIO/driver_override\" fi fi done modprobe -i vfio-pci # 把脚本传入 /etc/modprobe.d/vfio.conf $ cat /etc/modprobe.d/vfio.conf install vfio-pci /usr/bin/vfio-pci-override.sh options vfio-pci ids=10de:1c82 disable_vga=1 使用 vfio 管理 GPU # /etc/modprobe.d/vfio.conf, ids 为 lspci 找到的 hardware id, 多个设备的话用','分割 $ cat /etc/modprobe.d/vfio.conf options vfio-pci ids=10de:134d disable_vga=1 # 禁用NVIDIA nouveau 开源驱动, /etc/modprobe.d/blacklist.conf $ cat /etc/modprobe.d/blacklist.conf blacklist nouveau # kvm 模块配置, /etc/modprobe.d/kvm.conf $ cat /etc/modprobe.d/kvm.conf options kvm ignore_msrs=1 重启系统，启动完成后查看当前的 nvidia GPU 是否被 vfio-pci 模块使用, 确认IOMMU功能确实打开。\n$ dmesg | grep -e DMAR -e IOMMU | grep enabled # 如果能搜索到 DMAR: IOMMU enabled # 表示上述配置成功。 # 查看 GPU 是否被 vfio-pci 使用 # 另外注意检查看看 41:00.1 Audio device 是否也被 vfio-pci 使用 $ lspci -k | grep -i -e nvidia -A 3 41:00.0 VGA compatible controller: NVIDIA Corporation GP107 [GeForce GTX 1050 Ti] (rev a1) Subsystem: Device 1b4c:11bf Kernel driver in use: vfio-pci # GTX 1050 Ti GPU 被 vfio-pci 使用 Kernel modules: nouveau 41:00.1 Audio device: NVIDIA Corporation GP107GL High Definition Audio Controller (rev a1) Subsystem: Device 1b4c:11bf Kernel driver in use: vfio-pci # 发现 Audio device 也被 vfio-pci 使用了 Kernel modules: snd_hda_intel ... # list GPU IOMMU group $ find /sys/kernel/iommu_groups/ -type l | grep 41:00 /sys/kernel/iommu_groups/27/devices/0000:41:00.0 /sys/kernel/iommu_groups/27/devices/0000:41:00.1 # 找到IOMMU Group 管理的 PCI 设备 #!/bin/bash shopt -s nullglob for d in /sys/kernel/iommu_groups/*/devices/*; do n=${d#*/iommu_groups/*}; n=${n%%/*} printf 'IOMMU Group %s ' \"$n\" lspci -nns \"${d##*/}\" done 使用 qemu 透传 nvidia GPU 准备好centos7镜像，然后在虚拟机里面安装 nvidia 官方闭源驱动和 cuda SDK\n# 我从服务器上拷贝过来的是 vmdk 的镜像，先把它转换成 qcow2 的格式 $ /usr/local/qemu-2.9.0/bin/qemu-img convert -f vmdk -O qcow2 centos-7.3.1611-20180104.vmdk centos-7.3.1611-20180104.qcow2 # 使用 qemu 启动，注意-cpu 需要 kvm=off 参数 # kvm=off will hide the kvm hypervisor signature, this is required for NVIDIA cards # since its driver will refuse to work on an hypervisor and result in Code 43 on windows $ cat startvm.sh #!/bin/sh /usr/local/qemu-2.9.0/bin/qemu-system-x86_64 -enable-kvm \\ -m 4096 -cpu host,kvm=off -smp 4,sockets=1,cores=4,threads=1 \\ -drive file=./centos-7.3.1611-20180104.qcow2 \\ -device vfio-pci,host=41:00.0,multifunction=on,addr=0x16 \\ -device vfio-pci,host=41:00.1 \\ -net nic,model=e1000 -net user,hostfwd=tcp::5022-:22 \\ -vnc :1 # 这台虚拟机开了vnc和ssh 端口转发，可以使用vnc或者ssh访问 # 从host进入虚拟机 $ ssh 127.0.0.1 -p 5022 # 查看虚拟机透传进来的显卡 $ lspci -k | grep -i nvidia -A 3 00:04.0 Audio device: NVIDIA Corporation Device 0fb9 (rev a1) Subsystem: Device 1b4c:11bf Kernel driver in use: snd_hda_intel Kernel modules: snd_hda_intel 00:16.0 VGA compatible controller: NVIDIA Corporation GP107 (rev a1) Subsystem: Device 1b4c:11bf Kernel modules: nouveau 安装nvidia 驱动和 Cuda nvidia 驱动需要从官方下载，如果先安装 cuda 的话会一同安装 nvidia 驱动。 接下来采用虚拟机先安装驱动再安装 cuda 的步骤。\n参考： installing-nvidia-drivers-centos-7 NVIDIA CUDA GETTINGS STARTED GUIDE FOR LINUX\n安装 nvidia 驱动 下载地址：http://www.nvidia.com/object/unix.html\n# update 后如果更新内核，需要重启 $ yum -y update # 安装 gcc、make、glibc等工具和库 $ yum -y groupinstall \"Development Tools\" $ yum -y install kernel-devel # Download the latest NVIDIA driver for unix. $ wget http://us.download.nvidia.com/XFree86/Linux-x86_64/390.42/NVIDIA-Linux-x86_64-390.42.run $ yum -y install epel-release $ yum -y install dkms # Edit /etc/default/grub. Append the following to “GRUB_CMDLINE_LINUX” rd.driver.blacklist=nouveau nouveau.modeset=0 # Generate a new grub configuration to include the above changes. $ grub2-mkconfig -o /boot/grub2/grub.cfg # Edit/create /etc/modprobe.d/blacklist.conf and append: blacklist nouveau # Backup your old initramfs and then build a new one $ mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r)-nouveau.img $ dracut /boot/initramfs-$(uname -r).img $(uname -r) # 重启again # Run the NVIDIA driver installer and enter yes to all options. $ sh NVIDIA-Linux-x86_64-*.run # 装好后再一次重启，lspci -k 看下gpu使用的驱动是否是nvidia $ lspci -k | grep -i nvidia -A 3 00:04.0 Audio device: NVIDIA Corporation GP107GL High Definition Audio Controller (rev a1) 00:16.0 VGA compatible controller: NVIDIA Corporation GP107 [GeForce GTX 1050 Ti] (rev a1) Kernel driver in use: nvidia # 发现已经使用nvidia驱动 Kernel modules: nouveau, nvidia_drm, nvidia # 执行 nvidia-smi 看下输出和温度 $ nvidia-smi Thu Mar 15 01:31:09 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.42 Driver Version: 390.42 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 105... Off | 00000000:00:16.0 Off | N/A | | 40% 32C P0 N/A / 100W | 0MiB / 4040MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ $ nvidia-smi -q -d TEMPERATURE ==============NVSMI LOG============== Timestamp : Thu Mar 15 01:32:42 2018 Driver Version : 390.42 Attached GPUs : 1 GPU 00000000:00:16.0 Temperature GPU Current Temp : 32 C GPU Shutdown Temp : 102 C GPU Slowdown Temp : 99 C GPU Max Operating Temp : N/A Memory Current Temp : N/A Memory Max Operating Temp : N/A 安装 cuda 下载地址： https://developer.nvidia.com/cuda-downloads 这里选择 runfile，以后为了方便也可以选择 rpm(network)的方式，会自动帮我们安装 nvidia 驱动\n$ wget https://developer.nvidia.com/compute/cuda/9.1/Prod/local_installers/cuda_9.1.85_387.26_linux # Say no to installing the NVIDIA driver. # The standalone driver you already installed is typically newer than what is packaged with CUDA. # Use the default option for all other choices. $ sh cuda_*.run # 添加 CUDA 相关的环境变量 export PATH=$PATH:/usr/local/cuda/bin export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH # make samples $ cd ~/NVIDIA_CUDA-9.1_Samples; make -j 4 $ cd bin/x86_64/linux/release $ ./deviceQuery # 查询gpu信息 ./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \"GeForce GTX 1050 Ti\" CUDA Driver Version / Runtime Version 9.1 / 9.1 CUDA Capability Major/Minor version number: 6.1 Total amount of global memory: 4040 MBytes (4236312576 bytes) ( 6) Multiprocessors, (128) CUDA Cores/MP: 768 CUDA Cores GPU Max Clock rate: 1481 MHz (1.48 GHz) Memory Clock rate: 3504 Mhz Memory Bus Width: 128-bit L2 Cache Size: 1048576 bytes ... $ ./bandwidtTest # 使用 cuda 测试gpu bandwidth Running on... Device 0: GeForce GTX 1050 Ti Quick Mode Host to Device Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(MB/s) 33554432 9719.0 Device to Host Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(MB/s) 33554432 9215.8 Device to Device Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(MB/s) 33554432 95525.1 Result = PASS NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. ","excerpt":"作者: 李泽玺\nLinux 上虚拟机 GPU 透传需要使用 vfio 的方式。主要是因为在 vfio 方式下对虚拟设备的权限和 DMA 隔离 …","ref":"/v3.3/blog/2021/06/07/nvidia-gpu-passthrough-record/","title":"使用Linux vfio将Nvidia GPU透传给QEMU虚拟机"},{"body":"作者: 高现起\n大家好，我是高现起，目前在负责云联壹云融合云管理平台的产品工作，今天很高兴由我给大家介绍一下最新发布的3.7版本的新功能。\n我将围绕以下目录展开我今天的内容：\n 产品简介 3.7版本新功能介绍 功能规划  一、产品简介 目前，企业的IT资源属于无处不在的状态，根据 Flexera 2020 云状态（原 RightScale 云状态报告）报告，93%的企业实施了多云战略。虽然这一调查的依据是全球750家企业的云决策者，但是从国内的实际情况来说，数字可能没有那么大，趋势还是有的，就是多云已经是更多企业的选择了。\n多云环境，我们又统称为异构IT基础设施，那么这种异构IT基础设施在给客户带来更优性能更优成本的同时，也会带来一些新的问题，例如我下边列举的5种不同角色的用户所遇到的问题：\n 运维工程师/开发工程师-如何统一管理所有的云计算资源？ 财务专员-如何获取统一易读的公有云账单？ 运维主管/开发主管-如何及时获取资源整体的使用情况？ 财务主管-如何及时获取IT资源成本的使用结构？ CTO/CIO-如何提高IT资源的ROI（投资回报率）？  我们早在2017年就关注到多云带来客户的问题，开始做云联壹云这款产品，希望能够帮助企业很好的解决这些问题。\n云联壹云是一站式管理云计算资源的解决方案，希望帮助客户在一个地方管理本地IDC、私有云、公有云等IT资源，整体提高企业IT基础设施的管理效率。\n从产品架构图上，我们可以看出，底层是一些异构资源，上层是一些应用或服务，云联壹云处于承上启下的作用，向下屏蔽异构的差异，向上提供一致的资源管理能力。\n以上是我们最新版本也就是3.7版本的功能概览。\n最底层是异构资源，中层就是我们云管理平台的功能概览，上层应用和服务。云管理平台的底层是多云的适配层，往上是基础的资源管理，包括计算、存储、网络、PaaS服务等，再往上是针对企业客户做的三大块的功能，依次是权限管理、统一监控和报警、成本管理包括私有云的计量计费公有云的账单分析和成本优化等等，面向用户层，我们针对不同的用户有不同的用户界面，包括管理员的系统管理视图，租户的租户管理视图，普通用户的项目视图，针对运维人员的climc命令行工具，针对开发人员，我们也有丰富的REST API以及各个开发语言的SDK。\n这款产品能够带给客户3种价值：\n 省人省事-一站式采买、批量操作、一次配置，全网生效、统一获取易读账单 省钱-算好账、摊好钱、节成本 安全-统一认证、统一授权、免密登录  总而言之，云联壹云希望帮助用户解决异构IT基础设施带来的问题，充分发挥异构IT基础设施的优势。\n二、3.7版本新功能介绍 3.7版本增加了不下30多项功能或更新，由于时间关系，今天我主要给大家介绍这5个方面的功能：\n 多云资源对接 多云账单管理 多云监控告警 多云权限管理 标签管理  1. 多云资源对接 在3.7版本，我们不断围绕客户需求，支持更多公有云资源的纳管：\n AWS平台对接Redis Azure平台对接RDS 阿里云和华为云NAS文件存储对接 移动云对接 阿里云、华为云NAT网关支持  2. 多云账单管理 2.1 支持AWS的预留实例分析，包括RI推荐及RI覆盖率，帮助客户用好RI，节约成本\n2.2 支持价格对比功能，同配置全网比价，帮助客户发现便宜资源\n2.3 支持所有平台费用报告及定期发送\n2.4 支持预算管理功能，并且可以配置预算告警，防止费用超支\n2.5 费用优化迭代，更多纬度节省费用，帮助用户发现更多可以优化成本的途径\n3. 多云监控告警 3.1 支持自定义监控面板，一次定义，随时查看\n3.2 报警策略支持函数及静默期设置，有效避免毛刺、报警泛滥等问题\n3.3 支持自动化安装监控插件，可显示内存使用率等更多监控指标\n3.4 大屏增加地图，可根据资源城市进行分布\n3.5 增加监控总览，可以查看告警情况及平台各纬度使用情况\n4. 多云权限管理  在一个地方创建及管理所有云平台的子账号，并分配权限 通过云管壹云控制台可一键跳转免密登录公有云控制台 员工离职，自动注销对应的公有云账号  5. 标签管理  标签支持CMP与公有云平台双向同步 标签搜索增强，支持特殊符号复杂搜索  除以上5大功能的更新外，我们其实还有很多细节更新，在这里就不在展开给大家介绍，感兴趣的同学可以看一下以下总结\n三、功能规划 我们近期的功能规划主要是以下6个方面\n 京东云对接，我们有客户对京东云资源的纳管有需求 Azure RI\u0026CPP使用率、覆盖率以及推荐 产品内置负载均衡支持VPC网络（这个是我们私有云的功能） 公有云账单的二次定价，我们很多MSP厂商的需求 公有云WAF产品对接 监控报警的迭代，后期也会增加监控数据报表的功能  总结 好的，今天直播咱们就到这里，咱们简单的总结一下今天的内容：\n产品简介，在多云趋势的背景下，云联壹云产品希望帮助用户解决异构资源管理问题，提升企业IT基础设施管理效率，以及云联壹云带给客户省人省事、省钱、安全三大产品价值；\n3.7版本新功能介绍，主要给大家介绍了多云对接、多云账单管理、多云监控告警、多云权限管理以及标签管理5大模块的功能或更新；\n功能规划，我们近期将会围绕京东云对接、AzureCPP、内置负载均衡支持VPC、公有云账单二次定价、公有云WAF对接、监控迭代5大方面展开产品的功能设计与实现。\n感谢大家百忙之中参加我们的Meetup，下次直播再见。\n","excerpt":"作者: 高现起\n大家好，我是高现起，目前在负责云联壹云融合云管理平台的产品工作，今天很高兴由我给大家介绍一下最新发布的3.7版本的新功能。\n …","ref":"/v3.3/blog/2021/05/31/cloudpods-3.7-new-feature-introduction/","title":"直播回顾：Cloudpods 3.7版本新功能介绍"},{"body":"作者: 周有松\n各位朋友大家好，欢迎大家在周三的晚上参加我们的线上Meetup分享。我是周有松，目前负责云联壹云网络相关功能的开发工作，今天分享的题目是“云上负载均衡产品的应用”。\n今天的内容会从以下几个方面展开：\n 负载均衡产品简介。主要介绍负载均衡作为一个云上产品，它的功能模型是怎样的，日常使用中会遇到的业务词汇 负载均衡的功能与典型应用场景。这部分主要结合业务词汇，对负载均衡服务中常见的一些功能选项进行介绍，并举例介绍一些典型的应用场景 最后，我们做一下总结，讨论一下负载均衡产品相比传统方式的优点  一、产品简介 1. 以NGINX为例 提到负载均衡，我们以前一般先会想到NGINX，或者淘宝的分支Tengine。我们先来看看\n 它是怎样工作的 它解决了什么样的问题 它适合什么样的应用场景  在加入了NGINX之后，客户端（Client IP）首先与NGINX建立连接（Virtual IP+Virtual Port），请求也先发给NGINX，再由NGINX从多个后端服务器中选择一台，建立连接后把请求转发给后端服务器（Real IP）。\nNGINX作为网络转发节点，不参与后端服务的业务逻辑处理。而相比客户端直连后端，多个后端服务器可同时处理业务请求，应用的服务能力得到水平扩展。同时，转发节点上可以对后端做健康检查，自动屏蔽掉不健康的后端服务器，保障业务的高可用，使得单个后端服务器在故障、升级、过载时依然对用户连续可用。\n因此，我们说水平扩展、高可用是负载均衡解决的最基本的两个问题。从另外一方面来说，使用负载均衡的业务，在架构设计上应该是能够水平扩展的。比如，一个应用的多个实例之间不需要通信，相互之间没有复杂的状态维护。\n2. 业务词汇 在使用云上负载均衡的时候，不管哪个厂商的产品，我们会遇到一些常用的业务词汇。我们围绕一张图来简要介绍。\n负载均衡实例，除了区域、可用区之外，每个实例至少有一个IP地址。同一个负载均衡实例下可以有多个监听，每个监听一般至少有协议、端口两个属性。后面还会介绍监听的其它属性，比如调度算法、健康检查、转发策略等。\n实例和监听对应到NGINX上，其实是Virtual IP和Virtual Port的组合。每个监听有一个后端服务器组，组内可以有多个后端服务器，监听将来自客户端的请求转发给后端服务器（Real IP、Real Port）。\n对于HTTPS协议的监听，我们还会遇到TLS证书的概念。\n通常每个监听还可以绑定一个访问控制列表，用来设置黑名单、白名单，限定业务的服务范围。\n将业务词汇放到转发模型上，我们可以得到这样一张图。在壹云的负载均衡实现中，转发节点使用HAProxy负责具体的流量转发。实例和监听的配置被转换为HAProxy的配置应用到转发节点上。HAProxy据此将流量转发到后端服务器组。\n二、功能与应用场景 1. 监听协议 一般来说，负载均衡监听支持的传输层协议为TCP、UDP，应用层一般支持HTTP、HTTPS。其中对HTTP协议的支持一般包括HTTP/1.0，HTTP/1.1，HTTP/2，以及WS（WebSocket），WSS（WebSocket Secure）等。\n除此之外，许多厂商的负载均衡产品也陆续开始了QUIC协议的支持。QUIC有时也被称作HTTP/3，它的传输层协议类型为UDP。壹云的负载均衡产品基于HAProxy，它将会随着HAProxy 2.5的发版实现对QUIC协议的支持。\n同实例下的多个监听，传输层协议相同时，端口必须不能冲突。比如，同一实例下，无法TCP/80和HTTP/80的两个监听只能选其一。但是TCP/53和UDP/53的两个监听可以同时存在。\n从开销来说，HTTPS因为涉及到TLS协商和传输加解密，开销最大。HTTP其次，相比TCP协议转发节点需要对传输的数据做内容解析。TCP作为有状态的连接，相比UDP开销又要大一些。\n图中展示的是壹云控制台中负载均衡监听创建的第1步。其中，端口对所有协议类型都是必选的项。创建HTTPS监听时，要求必须关联相应的TLS证书。对于WS、WSS的支持是默认就有的。\n主流的浏览器要求HTTP/2必须运行在加密信道上，一般来说，负载均衡产品对于HTTPS监听默认开启HTTP/2的支持。极个别情况下，某些应用的客户端遇到HTTP/2时会有兼容性问题，所以负载均衡产品通常也会提供开关，允许显式将HTTP/2支持关闭。\n2. 调度算法 监听收到客户端的请求后，从后端服务器组中选出一个后端做转发。这个算法通常就叫“调度算法”。一般来说我们常用的有3种：\n 轮询，round robin。这种策略比较适用于短连接、短生命周期的业务，比如网页浏览。每打开一个网页，通常需要下载HTML、CSS、JS、字体等文档，每个请求大约秒级、毫秒级即可完成。这种场景下，轮询作为一种简单的算法，能够实现较好的均衡效果。 最小连接数，least connected。转发节点会记录它与每个监听的后端服务器的当前活跃的连接数，在转发请求时，从中选择一个活跃连接数最少的后端。这种算法较适合长连接、生命周期较长的业务，比如大文件上传下载、长时的交互会话如SSH等。 源一致性哈希。在这类算法中，转发节点通常会对请求来源标记一个身份，比如源IP地址、请求中的某个HTTP cookie值等。对于来自同一个标记的请求，将其转发到上次选择的后端服务器。这种算法较适合需要维护会话缓存、关联状态的业务。例如，某些业务后端收到请求后，需要从别处获取并缓存该请求相关的信息，比如订单详情、关联的用户详情等等，如果来自同一个用户的请求依然送到同一个后端，使得缓存使中率提高。这种对Locality的利用对应用的体验提升较好。  3.健康检查 健康检查是一种负载均衡服务实现高可用，保障业务连续性的机制。通过健康检查结果，转发节点将不健康的后端向用户屏蔽，实现故障、升级等行为对用户的无感知。\n云上负载均衡的健康检查通常有以下几种：\n TCP检查。即通过TCP连接是否能够成功建立判断后端的健康与否，连接建立成功后会立即发出RST报文中断连接，减少后端资源占用。使用这种健康检查时，在后端的日志中可能会周期出现“connection reset”字样的信息，是预期中的效果。 UDP检查。一般的机制是，向后端服务器发送指定内容的请求，在约定时间内若能收到来自后端的响应，并且响应内容匹配，则认为此次检查结果为健康。由于UDP是无状态连接协议，因此配置的参数里除了指定请求、响应的内容，还需指定响应超时时间。 HTTP检查。通常通过向后端发送HEAD请求，对响应的状态码做检查来判断健康与否。默认的配置一般是2xx、3xx的响应判定为健康，4xx、5xx判定为故障。对状态码的归类一般也是可配置的。  云上负载均衡产品一般会某个后端的检查结果，作为一个初步诊断。一般会有连接超时、连接错误、响应错误等。当所有后端都被判定为不可用时，对于HTTP/HTTPS类型的监听，转发节点通常会直接返回503 Service Unavailable。\n4. HTTP转发策略 转发策略是针对HTTP/HTTPS监听为言的。简单来说，它通过匹配用户请求中的Host、Path字段来决定将请求转发到哪个后端服务器组。\n首先，同一监听下的多个转发策略共用一个IP、端口组合，实现了复用，并且业务的域名和路径统一管理，无需配置到每一台后端服务器上。例如，我们可以将wiki.example.com, task.example.com解析到同负载均衡实例的IP地址，然后创建一个HTTPS监听，再创建2个转发策略，分别匹配这两个域名，实现不同的转发路径。也可以通过共用域名解析，分配不同URL路径（/wiki，/task）的方式实现相同的效果。\n另外，同一监听下的转发策略可以共用监听自身的ACL规则，共享监听的带单和速率控制，因此管理、配置会明晰简单。\n5. 会话保持 会话保持也叫Sticky Session，适用于HTTP/HTTPS类型的监听。它是一种通过在HTTP cookie中嵌入后端标识的方式指定请求的转发路径。根据Set-Cookie的时间不同，通常有两个选项：\nServer模式。后端服务器知道自己的标识，在返回响应的时候给出Set-Cookie，经过转发节点交给浏览器后，记录在浏览器。转发请求下次收到该浏览器的请求时会看到相应的Cookie值，将其转发到上次的后端服务器。\nInsert模式。转发节点收到后端服务器的响应后，再转发给浏览器前会加入Set-Cookie头部，这个值之后会记录在浏览器中。转发节点下次收到浏览器的请求时，若其中包含此cookie，它会将其去除之后再转发给后端服务器。因此，在Insert模式下，后端服务器感知不到此cookie的存在。\n6. 跳转 HTTP/HTTPS协议的监听可以指定跳转目标。这种情况下，监听无需绑定后端服务器组。\n跳转类型的监听通常用来实现HTTP到HTTPS的跳转，实现全站加密访问。例如，在明文HTTP协议下，有些宽带运营商可能会篡改应用的响应，在其中插入广告等信息，某些情况下还可能导致信息安全问题。强制HTTPS跳转作为一种应对措施，可以帮助实现用户到站点的端到端保密信道。\n另外，跳转还可以用来实现站点的迁移、应用的升级场景里。当迁移、升级完成后，通过跳转将用户请求转移到新站点，旧的URL依然可用，迁移升级过程对用户无感知，回滚也非常便捷。\n7. 访问控制 监听可以指定访问控制列表（ACL），并指定该列表的类型是黑名单还是白名单。\n有时我们有一些内部接口、服务需要只能够有限开放，比如同公司另外一个机房，或者不同公司的合作伙伴等。此时通过访问控制列表实现就非常方便，在界面上统一管理、备注。\n另外一种情况下，有些应用我们需要限定服务区域，比如版权、监管的原因等。此时我们可以将服务区域的来源地址做成白名单，仅开放应用给这些区域。网络控制在负载均衡一处完成，整个过程应用本身无需配置变更。\n8. 获取客户端真实IP 从产品简介中我们可以看到，客户端的连接和请求终结在转发节点上，后端服务器看到的连接都来自转发节点。此时，后端看到的网络层源IP也都是转发节点的地址。负载均衡产品一般提供提供了以下几种方式让后端服务器能够获取客户端的源IP地址：\n对于HTTP/HTTPS协议，转发节点可以通过HTTP头部来传递客户端的源地址，头部的名称一般为X-Forwarded-For。\n对于其它协议，一般通过PROXY协议来向后端服务器传递此信息。PROXY协议由HAProxy项目制订，AWS的负载均衡即支持此规范，另外还有NGINX、LightHTTPd也是支持此协议的。举例来说，对于TCP连接，PROXY协议会在连接建立后先进行连接信息传递，之后才是正常的数据交换。因此，如果PROXY协议直接对接应用服务，需要对应用服务的连接入口进行微小的修改。\n一部分云厂商在整个负载均衡转发面实施会话记录、地址转换，使得虚机作为后端服务器时看到源地址即为客户请求的源地址，此时无需额外机制获得客户端的真实IP地址。需要注意的是，对于内网负载均衡，服务器在访问自身作为后端监听时，会出现因为源、目的IP地址都是自己而造成路径不通，这是这种机制的一个小限制。\n9. 请求速率控制 壹云的负载均衡对于HTTP/HTTPS类型的监听可以实施请求速率控制。分为两个方面：\n一是对监听整体进行速率控制。例如，某个应用现在的服务能力是每秒3000个请求，为了保护后端服务器，避免过载造成雪崩效应等，我们可以设定整体的速率控制。\n二是对每个来源IP地址进行控制。这个可以用于阻止来自单个客户的高频异常请求，比如刷单、爬虫类请求等\n10. TLS证书 最后，我们简单介绍一下TLS证书。用户在使用负载均衡时，经常反馈在证书上传这一步产生困惑。\n首先，从非对称加密算法的类型来说，证书一般分别为RSA、EC证书两类。其中EC证书也叫椭圆曲线证书，相对较新一些，大部分平台也是支持的。\n上传证书时，一般要求同时上传证书、私钥两个文件或文件的内容。这两者是成对出现的。云平台一般支持PEM格式的证书和私钥，为base64编码带标识头的文本格式。与之对应通常叫DER格式，它是ASN.1编码的二进制格式。如果用文本编辑器打开证书和私钥文件，看到的是规整的ASCII字符串，那么应该是PEM格式，如果是乱码状态，很可能就是DER格式了。\n图中是常见的3种私钥PEM文件头。PKCS是RSA Laboratory制定的一系列标准。其中PKCS#8支持封装多种加密算法的私钥，因此此种格式的PEM私钥有可能是RSA或者EC格式的私钥。\n图中展示了壹云控制台中证书列表的菜单位置。从中可以看到证书上传后，可以直接看到证书的CN（Common Name）。SAN（Subject Alternative Name），以及过期时间。在“操作”列中，若“删除”操作为灰色禁用状态，一般表示该证书正被HTTPS监听关联使用\n我们可以从证书的详情中，找到与它关联的监听列表，实现对所有监听批量更新证书。\n三、总结 以上结合实际的场景对负载均衡产品做了一个简略的介绍，我们经常被问到，这些功能特性都可以通过自己搭建NGINX一步一步配置出来，为什么还要花钱用云上的产品呢？\n首先，作为一种模块化封装的产品，负载均衡的主要优势在于简便、高效。自建NGINX时，我们需要自己配置集群高可用，考虑集群的水平扩展和扩容。在生产环境时还需要配置自己的监控、告警策略。在管理层面，权限管理和操作过程审计也必不可少，然而NGINX作为基础组件与这些上层概念距离较远，需要较复杂的过程才可实现相等效果。\n其次，云上负载均衡通常都有开放的API，可通过编程与周边系统对接，灵活应对需求。\n从企业IT资产管理的角度来说，负载均衡代表着更先进的生产力。壹云控制台上的信息呈现相比命令行中单字符控制来说，更加一目了然，操作也更加简便，使得对于操作结果的可预期性更强。\n","excerpt":"作者: 周有松\n各位朋友大家好，欢迎大家在周三的晚上参加我们的线上Meetup分享。我是周有松，目前负责云联壹云网络相关功能的开发工作，今天 …","ref":"/v3.3/blog/2021/05/31/cloudpods-lb-application-intro/","title":"直播回顾：Cloudpods负载均衡的功能介绍"},{"body":"  开源、云原生的多云和混合云融合平台\n   快速上手  了解更多        The one cloud contains all your clouds Cloudpods不仅可以管理本地的虚拟机和物理机资源，还可以管理多个云平台和云账号。Cloudpods隐藏了这些异构基础设施资源的数据模型和API的差异，对外暴露了一套统一的API，允许用户就像用一个云一样地访问多云。从而大大降低了访问多云的复杂度，提升了管理多云的效率。       云原生 运行在Kubernetes中，使用Kubernetes Operator管理和维护\n   私有云 一个可以管理海量KVM虚拟机的轻量级私有云\n   裸金属 一个能进行物理机全生命周期管理的裸机云\n   多云管理 管理多云资源的功能，可以管理大多数的主流云，包括私有云以及公有云\n   多租户IAM 一套完整的多租户认证和访问控制体系\n   多云SSO 多云SSO允许以统一的联邦身份访问各个云平台原生控制台\n   CLIMC 使用命令行工具 climc 高效管理Cloudpods\n   统一API 一套功能丰富、统一一致的REST API和模型访问以上的云资源和功能\n     支持的技术和云平台          谁在用Cloudpods? 请在此GitHub Issue查看Cloudpods用户列表            有问题? 请在 GitHub 提 issue\n   欢迎贡献! 请在 GitHub 提 Pull Request\n给Cloudpods贡献代码 …\n   加入微信群     ","excerpt":"  开源、云原生的多云和混合云融合平台\n   快速上手  了解更多        The one cloud contains all …","ref":"/v3.3/","title":"Cloudpods"},{"body":"","excerpt":"","ref":"/v3.3/search/","title":"Search Results"},{"body":"","excerpt":"","ref":"/v3.3/blog/","title":"Cloudpods 博客"},{"body":"欢迎查看 Cloudpods 文档，本文档会介绍 Cloudpods 服务安装部署、资源操作管理和开发贡献等内容。\n什么是 Cloudpods? Cloudpods 是一个开源的多云平台。构建在用户分布于多云基础设施之上，通过技术手段将分布于多云的异构IT资源统一管理，将多底层多云的差异向用户屏蔽，并通过网络和调度实现资源的融合与打通，在多云之上进行抽象，向用户呈现统一的使用界面和API接口，让用户就像使用一个云平台一样使用分布于多云的资源，实现一个统一的“云上之云”的云平台。\nCloudpods 具备以下功能特性:\n  多云资源统一管理\n统一API、镜像、调度、账号体系、监控和计费等操作，能够全面管理 On premises、 私有云、公有云资源。\n  内置私有云\nCloudpods内置完备的私有云实现，提供对用户本地IDC的虚拟机、物理机和负载均衡等资源管理。\n  为多云Kubernetes提供运行环境\nCloudpods自身为运行在Kubernetes的云原生应用，并且能在多云环境部署运行Kubernetes集群。\n  从哪开始? 文档分为以下部分：\n  快速开始: 快速搭建部署服务\n  安装部署: 详细介绍安装和部署 Cloudpods 各个服务与组件\n  开发相关: 搭建开发环境，提交 PR\n  操作管理: 介绍如何操作云平台资源和管理服务\n  ","excerpt":"欢迎查看 Cloudpods 文档，本文档会介绍 Cloudpods 服务安装部署、资源操作管理和开发贡献等内容。 …","ref":"/v3.3/docs/","title":"欢迎来到Cloudpods"}]