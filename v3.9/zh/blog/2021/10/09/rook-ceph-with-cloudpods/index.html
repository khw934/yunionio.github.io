<!doctype html><html lang=zh class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.120.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/v3.9/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/v3.9/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/v3.9/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/v3.9/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/v3.9/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/v3.9/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/v3.9/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/v3.9/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/v3.9/favicons/android-192x192.png sizes=192x192><title>Cloudpods + Rook + Ceph: 轻松实现云原生的超融合私有云
</title><meta property="og:title" content="Cloudpods + Rook + Ceph: 轻松实现云原生的超融合私有云"><meta property="og:description" content="作者: 李泽玺
背景介绍 Cloudpods: 是我们开源的多云管理平台，运行在 Kubernetes 之上，里面包含一个完整的私有云实现。 Rook: 是一个分布式存储编排系统，目的是在 Kubernetes 上提供存储解决方案，本身不提供存储，而是在 Kubernetes 和存储系统之间提供适配层，简化存储系统的部署和维护工作。其支持的 Ceph 存储为 Stable 生产可用的状态。 Ceph: 是开源的分布式存储系统，主要功能包含 RBD 块存储以及 CephFS 分布式文件系统存储。 Cloudpods 服务以容器化的方式运行在 Kubernetes 集群之上，按照 部署文档/多节点安装 文档部署完 Cloudpods 之后，环境就有了一个完整的 Kubernetes 集群。
但 Cloudpods 内置私有云虚拟机使用的是本地存储，本文主要介绍使用 Rook 在 Cloudpods Kubernetes 集群里面的计算节点上部署 Ceph 集群，然后把 Rook 管理的 Ceph 集群暴露出来对接 Cloudpods 的私有云虚拟机。
Cloudpods 内置私有云提供虚拟化功能， Rook 管理的 Ceph 提供分布式存储，并且这些服务都是容器化，基于 Kubernetes 运行的。Cloudpods 运行虚拟机的节点也叫计算节点，计算节点也是 Kubernetes 的 Node，只要计算节点上有独立的裸盘，就可以使用 Rook 把 Ceph 部署到计算节点上，把这些技术结合起来可以轻松实现一个云原生的超融合私有云。
环境准备 Cloudpods：v3.6 以上的多节点部署版本 3 台计算节点，有单独的裸盘给 Ceph 使用(同时作为存储节点使用) Kubernetes：v1."><meta property="og:type" content="article"><meta property="og:url" content="https://www.cloudpods.org//v3.9/zh/blog/2021/10/09/rook-ceph-with-cloudpods/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2021-10-09T00:00:00+00:00"><meta property="article:modified_time" content="2021-10-09T00:00:00+00:00"><meta itemprop=name content="Cloudpods + Rook + Ceph: 轻松实现云原生的超融合私有云"><meta itemprop=description content="作者: 李泽玺
背景介绍 Cloudpods: 是我们开源的多云管理平台，运行在 Kubernetes 之上，里面包含一个完整的私有云实现。 Rook: 是一个分布式存储编排系统，目的是在 Kubernetes 上提供存储解决方案，本身不提供存储，而是在 Kubernetes 和存储系统之间提供适配层，简化存储系统的部署和维护工作。其支持的 Ceph 存储为 Stable 生产可用的状态。 Ceph: 是开源的分布式存储系统，主要功能包含 RBD 块存储以及 CephFS 分布式文件系统存储。 Cloudpods 服务以容器化的方式运行在 Kubernetes 集群之上，按照 部署文档/多节点安装 文档部署完 Cloudpods 之后，环境就有了一个完整的 Kubernetes 集群。
但 Cloudpods 内置私有云虚拟机使用的是本地存储，本文主要介绍使用 Rook 在 Cloudpods Kubernetes 集群里面的计算节点上部署 Ceph 集群，然后把 Rook 管理的 Ceph 集群暴露出来对接 Cloudpods 的私有云虚拟机。
Cloudpods 内置私有云提供虚拟化功能， Rook 管理的 Ceph 提供分布式存储，并且这些服务都是容器化，基于 Kubernetes 运行的。Cloudpods 运行虚拟机的节点也叫计算节点，计算节点也是 Kubernetes 的 Node，只要计算节点上有独立的裸盘，就可以使用 Rook 把 Ceph 部署到计算节点上，把这些技术结合起来可以轻松实现一个云原生的超融合私有云。
环境准备 Cloudpods：v3.6 以上的多节点部署版本 3 台计算节点，有单独的裸盘给 Ceph 使用(同时作为存储节点使用) Kubernetes：v1."><meta itemprop=datePublished content="2021-10-09T00:00:00+00:00"><meta itemprop=dateModified content="2021-10-09T00:00:00+00:00"><meta itemprop=wordCount content="1701"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Cloudpods + Rook + Ceph: 轻松实现云原生的超融合私有云"><meta name=twitter:description content="作者: 李泽玺
背景介绍 Cloudpods: 是我们开源的多云管理平台，运行在 Kubernetes 之上，里面包含一个完整的私有云实现。 Rook: 是一个分布式存储编排系统，目的是在 Kubernetes 上提供存储解决方案，本身不提供存储，而是在 Kubernetes 和存储系统之间提供适配层，简化存储系统的部署和维护工作。其支持的 Ceph 存储为 Stable 生产可用的状态。 Ceph: 是开源的分布式存储系统，主要功能包含 RBD 块存储以及 CephFS 分布式文件系统存储。 Cloudpods 服务以容器化的方式运行在 Kubernetes 集群之上，按照 部署文档/多节点安装 文档部署完 Cloudpods 之后，环境就有了一个完整的 Kubernetes 集群。
但 Cloudpods 内置私有云虚拟机使用的是本地存储，本文主要介绍使用 Rook 在 Cloudpods Kubernetes 集群里面的计算节点上部署 Ceph 集群，然后把 Rook 管理的 Ceph 集群暴露出来对接 Cloudpods 的私有云虚拟机。
Cloudpods 内置私有云提供虚拟化功能， Rook 管理的 Ceph 提供分布式存储，并且这些服务都是容器化，基于 Kubernetes 运行的。Cloudpods 运行虚拟机的节点也叫计算节点，计算节点也是 Kubernetes 的 Node，只要计算节点上有独立的裸盘，就可以使用 Rook 把 Ceph 部署到计算节点上，把这些技术结合起来可以轻松实现一个云原生的超融合私有云。
环境准备 Cloudpods：v3.6 以上的多节点部署版本 3 台计算节点，有单独的裸盘给 Ceph 使用(同时作为存储节点使用) Kubernetes：v1."><link rel=preload href=/v3.9/scss/main.min.b676790e8da09c7e6e8cae8287ff1ee8daaf7462ded6eddc94099ad4192e8f97.css as=style><link href=/v3.9/scss/main.min.b676790e8da09c7e6e8cae8287ff1ee8daaf7462ded6eddc94099ad4192e8f97.css rel=stylesheet integrity><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/fuse.js@6.6.2></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-LX0MD5KG60"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LX0MD5KG60")</script></head><body class="td-page td-blog"><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark navbar-ce flex-column flex-md-row td-navbar"><a id=cloudpods-top href=/v3.9/zh/ class="logo flex-shrink-0 navbar-brand"><img src=https://www.cloudpods.org//v3.9/images/cloudpods_logo_white.png height=46></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/v3.9/zh/docs/quickstart/><span>快速开始</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/v3.9/zh/docs/><span>文档</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://www.yunion.cn/subscription/index.html target=_blank><span>服务订阅</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/v3.9/zh/blog/><span class=active>博客</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/yunionio/cloudpods target=_blank><span>GitHub</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/v3.9/zh/docs/swagger><span>API</span></a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>3.9</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/>v3.10 (latest)</a>
<a class=dropdown-item href=/v3.9>v3.9</a></div></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>中文</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/v3.9/en/>English</a></div></li></ul></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center"><input type=search class="form-control td-search-input" placeholder="&#xf002; 站内搜索…" aria-label=站内搜索… autocomplete=off data-offline-search-index-json-src=/v3.9/offline-search-index.1492ff54d42c532edcfa5c68ad0b16e4.json data-offline-search-base-href=/ data-offline-search-max-results=10>
<button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type=button data-toggle=collapse data-target=#td-section-nav aria-controls=td-docs-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="collapse td-sidebar-nav" id=td-section-nav><div class="nav-item dropdown d-block d-lg-none"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>中文</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/v3.9/en/>English</a></div></div><ul class="td-sidebar-nav__section pr-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-v39zhblog-li><a href=/v3.9/zh/blog/ title="Cloudpods 博客" class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-v39zhblog><span>博客</span>
<span class=td-sidebar-section_icon-wrapper></span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20221129cloudpods-k8s-experience-sharing-li><a href=/v3.9/zh/blog/2022/11/29/cloudpods-k8s-experience-sharing/ class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20221129cloudpods-k8s-experience-sharing><span>Cloudpods容器化经验分享</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20220202cloudpods-golang-li><a href=/v3.9/zh/blog/2022/02/02/cloudpods-golang/ class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20220202cloudpods-golang><span>Cloudpods Golang实践</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20220121skycomputing-li><a href=/v3.9/zh/blog/2022/01/21/skycomputing/ class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20220121skycomputing><span>天空计算——云计算的下一个时代？</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20211009rook-ceph-with-cloudpods-li><a href=/v3.9/zh/blog/2021/10/09/rook-ceph-with-cloudpods/ class="td-sidebar-section_flex pl-0 active td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20211009rook-ceph-with-cloudpods><span class=td-sidebar-nav-active-item>Cloudpods + Rook + Ceph: 轻松实现云原生的超融合私有云</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20210925calico-customized-node-firewall-li><a href=/v3.9/zh/blog/2021/09/25/calico-customized-node-firewall/ class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20210925calico-customized-node-firewall><span>用Calico网络策略设置主机node防火墙规则</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20210702ocfs2-as-san-filesystem-li><a href=/v3.9/zh/blog/2021/07/02/ocfs2-as-san-filesystem/ class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20210702ocfs2-as-san-filesystem><span>QEMU+OCFS2: 使用OCFS2作为虚拟机磁盘文件的SAN存储文件系统</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20210701cloudpods-lb-application-intro-li><a href=/v3.9/zh/blog/2021/07/01/cloudpods-lb-application-intro/ class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20210701cloudpods-lb-application-intro><span>问题分析：为什么keystone的本地用户认证接口压测性能很差？</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20210625cgroups-kubernetes-pid-limits-li><a href=/v3.9/zh/blog/2021/06/25/cgroups-kubernetes-pid-limits/ class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20210625cgroups-kubernetes-pid-limits><span>使用 Cgroups 限制 Kubernetes Pod 进程数</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20210607nvidia-gpu-passthrough-record-li><a href=/v3.9/zh/blog/2021/06/07/nvidia-gpu-passthrough-record/ class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20210607nvidia-gpu-passthrough-record><span>使用Linux vfio将Nvidia GPU透传给QEMU虚拟机</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20210531cloudpods-37-new-feature-introduction-li><a href=/v3.9/zh/blog/2021/05/31/cloudpods-3.7-new-feature-introduction/ class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20210531cloudpods-37-new-feature-introduction><span>直播回顾：Cloudpods 3.7版本新功能介绍</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20210531cloudpods-lb-application-intro-li><a href=/v3.9/zh/blog/2021/05/31/cloudpods-lb-application-intro/ class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20210531cloudpods-lb-application-intro><span>直播回顾：Cloudpods负载均衡的功能介绍</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-v39zhblog20190613unified-iaas-for-future-li><a href=/v3.9/zh/blog/2019/06/13/unified-iaas-for-future/ class="td-sidebar-section_flex pl-0 td-sidebar-link td-sidebar-link__page" id=m-v39zhblog20190613unified-iaas-for-future><span>面向未来的 IT 基础设施管理架构——融合云（Unified IaaS）</span></a></li></ul></li></ul></nav></div></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"><a href=https://github.com/yunionio/docs/edit/master/content/zh/blog/_posts/2021-10-09-rook-ceph-with-cloudpods/index.md target=_blank><i class="fa fa-edit fa-fw"></i> 编辑此页</a>
<a href="https://github.com/yunionio/docs/new/master/content/zh/blog/_posts/2021-10-09-rook-ceph-with-cloudpods/index.md?filename=change-me.md&amp;value=---%0Atitle%3A+%22Long+Page+Title%22%0AlinkTitle%3A+%22Short+Nav+Title%22%0Aweight%3A+100%0Adescription%3A+%3E-%0A+++++Page+description+for+heading+and+indexes.%0A---%0A%0A%23%23+Heading%0A%0AEdit+this+template+to+create+your+new+page.%0A%0A%2A+Give+it+a+good+name%2C+ending+in+%60.md%60+-+e.g.+%60getting-started.md%60%0A%2A+Edit+the+%22front+matter%22+section+at+the+top+of+the+page+%28weight+controls+how+its+ordered+amongst+other+pages+in+the+same+directory%3B+lowest+number+first%29.%0A%2A+Add+a+good+commit+message+at+the+bottom+of+the+page+%28%3C80+characters%3B+use+the+extended+description+field+for+more+detail%29.%0A%2A+Create+a+new+branch+so+you+can+preview+your+new+file+and+request+a+review+via+Pull+Request.%0A" target=_blank><i class="fa fa-edit fa-fw"></i> 添加子页面</a>
<a href=https://github.com/yunionio/cloudpods/issues/new/choose target=_blank><i class="fas fa-tasks fa-fw"></i> 提交项目问题</a>
<a href="https://github.com/yunionio/cloudpods/issues/new?title=[Docs]%20Cloudpods%20+%20Rook%20+%20Ceph:%20%e8%bd%bb%e6%9d%be%e5%ae%9e%e7%8e%b0%e4%ba%91%e5%8e%9f%e7%94%9f%e7%9a%84%e8%b6%85%e8%9e%8d%e5%90%88%e7%a7%81%e6%9c%89%e4%ba%91" target=_blank><i class="fab fa-github fa-fw"></i> 提交文档问题</a></div><div id=toc-hidden><nav id=TableOfContents><ul><li><a href=#背景介绍>背景介绍</a></li><li><a href=#环境准备>环境准备</a></li><li><a href=#使用-rook-部署-ceph>使用 Rook 部署 Ceph</a><ul><li><a href=#节点信息>节点信息</a></li><li><a href=#部署-rook-组件>部署 Rook 组件</a></li><li><a href=#创建-ceph-集群>创建 ceph 集群</a></li></ul></li><li><a href=#cloudpods-虚拟机使用-rook-部署的-ceph>Cloudpods 虚拟机使用 Rook 部署的 Ceph</a></li><li><a href=#其它操作>其它操作</a></li></ul></nav></div><script>let tocNode=document.getElementById("toc-hidden"),list=clectionLis1(),ul=createList(list);tocNode.innerHTML="",tocNode.appendChild(ul),setTimeout(()=>{initActiveToc(),addEventListener()},0);function addEventListener(){document.getElementById("toc-hidden").onclick=function(e){let t=e||window.event,n=t.target||t.srcElement;n.nodeName.toLowerCase()=="a"&&setTimeout(()=>{changeTocActive(n.parentNode)},0)},window.onscroll=function(){initActiveToc()}}function initActiveToc(){let e=getBoundingClientNode("h2"),t=getBoundingClientNode("h3"),n=e.currentNode||t.currentNode;if(e.currentNode&&t.currentNode&&(n=Math.abs(e.y)<Math.abs(t.y)?e.currentNode:t.currentNode),!n)return;changeTocActive(n)}function changeTocActive(e){let s=e.getAttribute("id")||e.getAttribute("text")?.replace("#",""),t=document.getElementsByClassName("li1-item"),n=document.getElementsByClassName("li2-item");for(let e=0;e<t.length;e++){let n=t[e].getAttribute("text")||"";n.replace("#","")==s?t[e].classList.add("active"):t[e].classList.remove("active")}for(let e=0;e<n.length;e++){let t=n[e].getAttribute("text")||"";t.replace("#","")==s?n[e].classList.add("active"):n[e].classList.remove("active")}}function getBoundingClientNode(e){let t={nodes:[],currentNode:null,index:-1,y:0},n=Array.from(document.getElementsByTagName(e));t.nodes=n;for(let e=0;e<n.length;e++){let s=n[e].getBoundingClientRect();if(s.y<=0&&(t.y=s.y,t.currentNode=n[e],t.index=e),s.y>0&&s.y<window.innerHeight){t.y=s.y,t.currentNode=n[e],t.index=e;break}}return t}function createList(e){let t=createNode("ul");return e.map((n,s)=>{let a=["li1-item"],r=["li1-item-topborder"];s!==0&&r.push("show");let c=["li1-item-bottomborder"];s!==e.length-1?c.push("show"):a.push("no-border");let i=createNode("li",{classList:a,text:n.href}),l=createNode("a",{href:n.href,innerHTML:n.text}),o=createNode("div",{classList:["li1-item-style"]}),d=createNode("div",{classList:r}),u=createNode("div",{classList:c}),h=createNode("div",{classList:["mask-line"]}),m=createNode("div",{classList:["li1-item-style-dotted"]});if(o.appendChild(d),o.appendChild(u),o.appendChild(h),o.appendChild(m),i.appendChild(l),i.appendChild(o),n.ul.length){let e=createNode("ul");n.ul.map((t)=>{let s=createNode("li",{classList:["li2-item"],text:t.href}),o=createNode("a",{href:t.href,innerHTML:t.text}),i=createNode("div",{classList:["li2-item-style"]});s.appendChild(o),s.appendChild(i),e.appendChild(s)}),i.appendChild(e)}t.appendChild(i)}),t}function createNode(e,t={}){let n=document.createElement(e);const{classList:s,innerHTML:o,href:i,text:a}=t;return s&&s.map(e=>{n.classList.add(e)}),o&&(n.innerHTML=o),i&&(n.href=i),a&&n.setAttribute("text",a),n}function clectionLis1(){let t=[],e=document.getElementById("TableOfContents")?.firstElementChild||{children:[]};for(let n=0;n<e.children.length;n++){let o=e.children[n].firstElementChild,a=e.children[n].lastChild,s=e.children[n].children[1]||"",i={text:o.innerHTML,href:o.getAttribute("href"),ul:[]};if(s)for(let e=0;e<s.children.length;e++){let t=s.children[e].firstElementChild;i.ul.push({text:t.innerHTML,href:t.getAttribute("href")})}t.push(i)}return t}</script></div><main class="col-12 col-md-9 col-xl-8 pl-md-5 pr-md-4" role=main><a class="btn btn-lg -bg-orange td-rss-button d-none d-lg-block" href=https://www.cloudpods.org//v3.9/zh/blog/index.xml target=_blank>RSS <i class="fa fa-rss ml-2"></i></a><div class=td-content><h1>Cloudpods + Rook + Ceph: 轻松实现云原生的超融合私有云</h1><div class="td-byline mb-4"><time datetime=2021-10-09 class=text-muted>09.10.2021</time></div><p><strong>作者:</strong> 李泽玺</p><h2 id=背景介绍>背景介绍</h2><ul><li><a href=https://www.cloudpods.org>Cloudpods</a>: 是我们开源的多云管理平台，运行在 Kubernetes 之上，里面包含一个完整的私有云实现。</li><li><a href=https://rook.io/docs/rook/v1.7>Rook</a>: 是一个分布式存储编排系统，目的是在 Kubernetes 上提供存储解决方案，本身不提供存储，而是在 Kubernetes 和存储系统之间提供适配层，简化存储系统的部署和维护工作。其支持的 Ceph 存储为 Stable 生产可用的状态。</li><li><a href=https://docs.ceph.com/>Ceph</a>: 是开源的分布式存储系统，主要功能包含 RBD 块存储以及 CephFS 分布式文件系统存储。</li></ul><p>Cloudpods 服务以容器化的方式运行在 Kubernetes 集群之上，按照 <a href=/zh/docs/quickstart/nodes/>部署文档/多节点安装</a> 文档部署完 Cloudpods 之后，环境就有了一个完整的 Kubernetes 集群。</p><p>但 Cloudpods 内置私有云虚拟机使用的是本地存储，本文主要介绍使用 Rook 在 Cloudpods Kubernetes 集群里面的计算节点上部署 Ceph 集群，然后把 Rook 管理的 Ceph 集群暴露出来对接 Cloudpods 的私有云虚拟机。</p><p>Cloudpods 内置私有云提供虚拟化功能， Rook 管理的 Ceph 提供分布式存储，并且这些服务都是容器化，基于 Kubernetes 运行的。Cloudpods 运行虚拟机的节点也叫计算节点，计算节点也是 Kubernetes 的 Node，只要计算节点上有独立的裸盘，就可以使用 Rook 把 Ceph 部署到计算节点上，把这些技术结合起来可以轻松实现一个云原生的超融合私有云。</p><h2 id=环境准备>环境准备</h2><ul><li>Cloudpods：v3.6 以上的多节点部署版本<ul><li>3 台计算节点，有单独的裸盘给 Ceph 使用(同时作为存储节点使用)</li></ul></li><li>Kubernetes：v1.15.9 版本(Cloudpods 默认的集群)</li><li>Rook: v1.7 版本</li><li>操作系统：CentOS 7</li><li>内核版本：3.10.0-1062.4.3.el7.yn20191203.x86_64<ul><li>如果使用 CephFS 建议的内核版本是 4.17 以上的版本，可以升级我们官方提供的 5.4 版本内核</li></ul></li></ul><p>其中 Ceph 相关的环境准备工作和限制可以参考 Rook 提供的文档：<a href=https://rook.io/docs/rook/v1.7/pre-reqs.html>https://rook.io/docs/rook/v1.7/pre-reqs.html</a> 。</p><h2 id=使用-rook-部署-ceph>使用 Rook 部署 Ceph</h2><p>接下来介绍使用 Rook 在已有的 Cloudpods Kubernetes 集群上部署 Ceph 集群，这里有个前提是已经按照 <a href=/zh/docs/quickstart/nodes/>部署文档/多节点安装</a> 文档，部署了一个多节点的 Cloudpods 集群。</p><h3 id=节点信息>节点信息</h3><p>假设已有的 3 个节点为 node-{0,1,2}，每个节点的磁盘信息如下，sd{b,c,d} 都是没有分区的裸盘，留给 Ceph 使用：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ lsblk
</span></span><span style=display:flex><span>NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
</span></span><span style=display:flex><span>sda      8:0    <span style=color:#0000cf;font-weight:700>0</span> 931.5G  <span style=color:#0000cf;font-weight:700>0</span> disk
</span></span><span style=display:flex><span>├─sda1   8:1    <span style=color:#0000cf;font-weight:700>0</span>     1M  <span style=color:#0000cf;font-weight:700>0</span> part
</span></span><span style=display:flex><span>├─sda2   8:2    <span style=color:#0000cf;font-weight:700>0</span>   512M  <span style=color:#0000cf;font-weight:700>0</span> part /boot
</span></span><span style=display:flex><span>└─sda3   8:3    <span style=color:#0000cf;font-weight:700>0</span>   931G  <span style=color:#0000cf;font-weight:700>0</span> part /
</span></span><span style=display:flex><span>sdb      8:16   <span style=color:#0000cf;font-weight:700>0</span> 931.5G  <span style=color:#0000cf;font-weight:700>0</span> disk
</span></span><span style=display:flex><span>sdc      8:32   <span style=color:#0000cf;font-weight:700>0</span>   3.7T  <span style=color:#0000cf;font-weight:700>0</span> disk
</span></span><span style=display:flex><span>sdd      8:48   <span style=color:#0000cf;font-weight:700>0</span>   3.7T  <span style=color:#0000cf;font-weight:700>0</span> disk
</span></span></code></pre></div><p>使用 <code>kubectl get nodes</code> 可以看到已经在 Kubernetes 集群中的节点：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME      STATUS   ROLES    AGE   VERSION
</span></span><span style=display:flex><span>cloudbox  Ready    master   34d   v1.15.9-beta.0
</span></span><span style=display:flex><span>node-0    Ready    &lt;none&gt;   12d   v1.15.9-beta.0
</span></span><span style=display:flex><span>node-1    Ready    &lt;none&gt;   11d   v1.15.9-beta.0
</span></span><span style=display:flex><span>node-2    Ready    &lt;none&gt;   11d   v1.15.9-beta.0
</span></span></code></pre></div><p>然后给对应的节点打上 role=storage-node 的标签：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 打标签</span>
</span></span><span style=display:flex><span>$ kubectl label node node-0 <span style=color:#000>role</span><span style=color:#ce5c00;font-weight:700>=</span>storage-node
</span></span><span style=display:flex><span>$ kubectl label node node-1 <span style=color:#000>role</span><span style=color:#ce5c00;font-weight:700>=</span>storage-node
</span></span><span style=display:flex><span>$ kubectl label node node-2 <span style=color:#000>role</span><span style=color:#ce5c00;font-weight:700>=</span>storage-node
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 查看标签对应的节点</span>
</span></span><span style=display:flex><span>$ kubectl get nodes -L role
</span></span><span style=display:flex><span>NAME       STATUS   ROLES    AGE   VERSION          ROLE
</span></span><span style=display:flex><span>cloudbox   Ready    master   34d   v1.15.9-beta.0
</span></span><span style=display:flex><span>node-0     Ready    &lt;none&gt;   12d   v1.15.9-beta.0   storage-node
</span></span><span style=display:flex><span>node-1     Ready    &lt;none&gt;   11d   v1.15.9-beta.0   storage-node
</span></span><span style=display:flex><span>node-2     Ready    &lt;none&gt;   11d   v1.15.9-beta.0   storage-node
</span></span></code></pre></div><p>另外执行 <code>climc host-list</code> 命令(climc 是云平台的命令行工具)，也可以看到这 3 个节点作为 Cloudpods 的计算节点加入到了云平台:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ climc host-list
</span></span><span style=display:flex><span>+--------------------------------------+----------+-------------------+----------------+--------------+-----------------------------+---------+---------+-------------+----------+-----------+------------+------------+--------------+------------+
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>|</span>                  ID                  <span style=color:#000;font-weight:700>|</span>   Name   <span style=color:#000;font-weight:700>|</span>    Access_mac     <span style=color:#000;font-weight:700>|</span>   Access_ip    <span style=color:#000;font-weight:700>|</span>   Ipmi_Ip    <span style=color:#000;font-weight:700>|</span>         Manager_URI         <span style=color:#000;font-weight:700>|</span> Status  <span style=color:#000;font-weight:700>|</span> enabled <span style=color:#000;font-weight:700>|</span> host_status <span style=color:#000;font-weight:700>|</span> mem_size <span style=color:#000;font-weight:700>|</span> cpu_count <span style=color:#000;font-weight:700>|</span> node_count <span style=color:#000;font-weight:700>|</span>     sn     <span style=color:#000;font-weight:700>|</span> storage_type <span style=color:#000;font-weight:700>|</span> host_type  <span style=color:#000;font-weight:700>|</span>
</span></span><span style=display:flex><span>+--------------------------------------+----------+-------------------+----------------+--------------+-----------------------------+---------+---------+-------------+----------+-----------+------------+------------+--------------+------------+
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>|</span> 0d8023ad-ebf9-4a3c-8294-fd170f4ce5c6 <span style=color:#000;font-weight:700>|</span> node-0   <span style=color:#000;font-weight:700>|</span> 38:ea:a7:8d:94:78 <span style=color:#000;font-weight:700>|</span> 172.16.254.127 <span style=color:#000;font-weight:700>|</span> 172.16.254.2 <span style=color:#000;font-weight:700>|</span> https://172.16.254.127:8885 <span style=color:#000;font-weight:700>|</span> running <span style=color:#000;font-weight:700>|</span> <span style=color:#204a87>true</span>    <span style=color:#000;font-weight:700>|</span> online      <span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>128695</span>   <span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>32</span>        <span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>2</span>          <span style=color:#000;font-weight:700>|</span> 6CU3505M2G <span style=color:#000;font-weight:700>|</span> rotate       <span style=color:#000;font-weight:700>|</span> hypervisor <span style=color:#000;font-weight:700>|</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>|</span> c02470b3-9666-46f7-852e-9bda8074a72e <span style=color:#000;font-weight:700>|</span> node-1   <span style=color:#000;font-weight:700>|</span> ec:f4:bb:d7:c4:e0 <span style=color:#000;font-weight:700>|</span> 172.16.254.124 <span style=color:#000;font-weight:700>|</span> 172.16.254.5 <span style=color:#000;font-weight:700>|</span> https://172.16.254.124:8885 <span style=color:#000;font-weight:700>|</span> running <span style=color:#000;font-weight:700>|</span> <span style=color:#204a87>true</span>    <span style=color:#000;font-weight:700>|</span> online      <span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>96432</span>    <span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>48</span>        <span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>2</span>          <span style=color:#000;font-weight:700>|</span> 62CNF52    <span style=color:#000;font-weight:700>|</span> rotate       <span style=color:#000;font-weight:700>|</span> hypervisor <span style=color:#000;font-weight:700>|</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>|</span> 5811c2d9-2b45-47e4-8c08-a5d479d03009 <span style=color:#000;font-weight:700>|</span> node-2   <span style=color:#000;font-weight:700>|</span> d4:ae:52:7e:90:9c <span style=color:#000;font-weight:700>|</span> 172.16.254.126 <span style=color:#000;font-weight:700>|</span> 172.16.254.3 <span style=color:#000;font-weight:700>|</span> https://172.16.254.126:8885 <span style=color:#000;font-weight:700>|</span> running <span style=color:#000;font-weight:700>|</span> <span style=color:#204a87>true</span>    <span style=color:#000;font-weight:700>|</span> online      <span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>128723</span>   <span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>24</span>        <span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>2</span>          <span style=color:#000;font-weight:700>|</span> 8Q1PB3X    <span style=color:#000;font-weight:700>|</span> rotate       <span style=color:#000;font-weight:700>|</span> hypervisor <span style=color:#000;font-weight:700>|</span>
</span></span><span style=display:flex><span>+--------------------------------------+----------+-------------------+----------------+--------------+-----------------------------+---------+---------+-------------+----------+-----------+------------+------------+--------------+------------+
</span></span></code></pre></div><h3 id=部署-rook-组件>部署 Rook 组件</h3><p>下载 Rook 相关代码：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># clone rook 源码</span>
</span></span><span style=display:flex><span>$ git clone --single-branch --branch release-1.7 https://github.com/rook/rook.git
</span></span><span style=display:flex><span>$ <span style=color:#204a87>cd</span> rook
</span></span></code></pre></div><p>部署 rook operator 服务：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># apply rook 相关 CRD</span>
</span></span><span style=display:flex><span>$ <span style=color:#204a87>cd</span> cluster/examples/kubernetes/ceph/pre-k8s-1.16/
</span></span><span style=display:flex><span>$ kubectl apply -f crds.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># apply rook operator 服务</span>
</span></span><span style=display:flex><span>$ <span style=color:#204a87>cd</span> ..
</span></span><span style=display:flex><span>$ kubectl apply -f common.yaml -f operator.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 查看 operator 服务的状态，等待 rook-ceph-operator pod 变为 Running</span>
</span></span><span style=display:flex><span>$ kubectl -n rook-ceph get pods
</span></span><span style=display:flex><span>NAME                                  READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>rook-ceph-operator-68964f4b87-pc87m   1/1     Running   <span style=color:#0000cf;font-weight:700>0</span>          7m38s
</span></span></code></pre></div><h3 id=创建-ceph-集群>创建 ceph 集群</h3><p>首先根据自己的环境，修改 rook 提供的 <code>cluster.yaml</code> 里面的内容：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ cp cluster.yaml cluster-env.yaml
</span></span></code></pre></div><p>下面是修改后的 cluster.yaml 和 cluster-env.yaml 的 diff 内容：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ diff -u cluster.yaml cluster-env.yaml
</span></span></code></pre></div><p>请根据自己的节点环境配置，参考 diff 修改，需要注意的地方如下：</p><ul><li>spec.image: 改为 registry.cn-beijing.aliyuncs.com/yunionio/ceph:v14.2.22 ，这里需要用 v14 版本的镜像，对应的 ceph 版本为 nautilus，更高的版本可能会出现 cloudpods 不兼容的情况</li><li>spec.network.provider: 改为 host ，表示 ceph 相关容器使用 hostNetwork ，这样才能给 Kubernetes 集群之外的服务使用</li><li>spec.placement: 修改了里面的 Kubernetes 调度策略，表示把 ceph pod 调度到 role=storage-node 的节点上</li><li>spec.storage: 表示存储的配置<ul><li>useAllNodes: 我们指定了role=storage-node 的节点运行 ceph，该值必须设置为false</li><li>nodes: 分别设置各个节点的存储路径，可以是磁盘或者目录</li></ul></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span><span style=color:#a40000>--- cluster.yaml    2021-10-09 10:49:53.731596210 +0800
</span></span></span><span style=display:flex><span><span style=color:#a40000></span><span style=color:#00a000>+++ cluster-env.yaml    2021-10-09 17:50:01.859112585 +0800
</span></span></span><span style=display:flex><span><span style=color:#00a000></span><span style=color:purple;font-weight:700>@@ -21,7 +21,7 @@
</span></span></span><span style=display:flex><span><span style=color:purple;font-weight:700></span>     # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
</span></span><span style=display:flex><span>     # If you want to be more precise, you can always use a timestamp tag such quay.io/ceph/ceph:v16.2.6-20210918
</span></span><span style=display:flex><span>     # This tag might not contain a new Ceph version, just security fixes from the underlying operating system, which will reduce vulnerabilities
</span></span><span style=display:flex><span><span style=color:#a40000>-    image: quay.io/ceph/ceph:v16.2.6
</span></span></span><span style=display:flex><span><span style=color:#a40000></span><span style=color:#00a000>+    image: registry.cn-beijing.aliyuncs.com/yunionio/ceph:v14.2.22
</span></span></span><span style=display:flex><span><span style=color:#00a000></span>     # Whether to allow unsupported versions of Ceph. Currently `nautilus`, `octopus`, and `pacific` are supported.
</span></span><span style=display:flex><span>     # Future versions such as `pacific` would require this to be set to `true`.
</span></span><span style=display:flex><span>     # Do not set to true in production.
</span></span><span style=display:flex><span><span style=color:purple;font-weight:700>@@ -81,7 +81,7 @@
</span></span></span><span style=display:flex><span><span style=color:purple;font-weight:700></span>     rulesNamespace: rook-ceph
</span></span><span style=display:flex><span>   network:
</span></span><span style=display:flex><span>     # enable host networking
</span></span><span style=display:flex><span><span style=color:#a40000>-    #provider: host
</span></span></span><span style=display:flex><span><span style=color:#a40000></span><span style=color:#00a000>+    provider: host
</span></span></span><span style=display:flex><span><span style=color:#00a000></span>     # enable the Multus network provider
</span></span><span style=display:flex><span>     #provider: multus
</span></span><span style=display:flex><span>     #selectors:
</span></span><span style=display:flex><span><span style=color:purple;font-weight:700>@@ -135,22 +135,22 @@
</span></span></span><span style=display:flex><span><span style=color:purple;font-weight:700></span>   # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
</span></span><span style=display:flex><span>   # The example under &#39;all&#39; would have all services scheduled on kubernetes nodes labeled with &#39;role=storage-node&#39; and
</span></span><span style=display:flex><span>   # tolerate taints with a key of &#39;storage-node&#39;.
</span></span><span style=display:flex><span><span style=color:#a40000>-#  placement:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#    all:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#      nodeAffinity:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#        requiredDuringSchedulingIgnoredDuringExecution:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#          nodeSelectorTerms:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#          - matchExpressions:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#            - key: role
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#              operator: In
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#              values:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#              - storage-node
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#      podAffinity:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#      podAntiAffinity:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#      topologySpreadConstraints:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#      tolerations:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#      - key: storage-node
</span></span></span><span style=display:flex><span><span style=color:#a40000>-#        operator: Exists
</span></span></span><span style=display:flex><span><span style=color:#a40000></span><span style=color:#00a000>+  placement:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+    all:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+      nodeAffinity:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+        requiredDuringSchedulingIgnoredDuringExecution:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+          nodeSelectorTerms:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+          - matchExpressions:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+            - key: role
</span></span></span><span style=display:flex><span><span style=color:#00a000>+              operator: In
</span></span></span><span style=display:flex><span><span style=color:#00a000>+              values:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+              - storage-node
</span></span></span><span style=display:flex><span><span style=color:#00a000>+      podAffinity:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+      podAntiAffinity:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+      topologySpreadConstraints:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+      tolerations:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+      - key: storage-node
</span></span></span><span style=display:flex><span><span style=color:#00a000>+        operator: Exists
</span></span></span><span style=display:flex><span><span style=color:#00a000></span> # The above placement information can also be specified for mon, osd, and mgr components
</span></span><span style=display:flex><span> #    mon:
</span></span><span style=display:flex><span> # Monitor deployments may contain an anti-affinity rule for avoiding monitor
</span></span><span style=display:flex><span><span style=color:purple;font-weight:700>@@ -207,8 +207,8 @@
</span></span></span><span style=display:flex><span><span style=color:purple;font-weight:700></span> #    osd: rook-ceph-osd-priority-class
</span></span><span style=display:flex><span> #    mgr: rook-ceph-mgr-priority-class
</span></span><span style=display:flex><span>   storage: # cluster level storage configuration and selection
</span></span><span style=display:flex><span><span style=color:#a40000>-    useAllNodes: true
</span></span></span><span style=display:flex><span><span style=color:#a40000>-    useAllDevices: true
</span></span></span><span style=display:flex><span><span style=color:#a40000></span><span style=color:#00a000>+    useAllNodes: false
</span></span></span><span style=display:flex><span><span style=color:#00a000>+    useAllDevices: false
</span></span></span><span style=display:flex><span><span style=color:#00a000></span>     #deviceFilter:
</span></span><span style=display:flex><span>     config:
</span></span><span style=display:flex><span>       # crushRoot: &#34;custom-root&#34; # specify a non-default root label for the CRUSH map
</span></span><span style=display:flex><span><span style=color:purple;font-weight:700>@@ -219,17 +219,22 @@
</span></span></span><span style=display:flex><span><span style=color:purple;font-weight:700></span>       # encryptedDevice: &#34;true&#34; # the default value for this option is &#34;false&#34;
</span></span><span style=display:flex><span> # Individual nodes and their config can be specified as well, but &#39;useAllNodes&#39; above must be set to false. Then, only the named
</span></span><span style=display:flex><span> # nodes below will be used as storage resources.  Each node&#39;s &#39;name&#39; field should match their &#39;kubernetes.io/hostname&#39; label.
</span></span><span style=display:flex><span><span style=color:#a40000>-    # nodes:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-    #   - name: &#34;172.17.4.201&#34;
</span></span></span><span style=display:flex><span><span style=color:#a40000>-    #     devices: # specific devices to use for storage can be specified for each node
</span></span></span><span style=display:flex><span><span style=color:#a40000>-    #       - name: &#34;sdb&#34;
</span></span></span><span style=display:flex><span><span style=color:#a40000>-    #       - name: &#34;nvme01&#34; # multiple osds can be created on high performance devices
</span></span></span><span style=display:flex><span><span style=color:#a40000>-    #         config:
</span></span></span><span style=display:flex><span><span style=color:#a40000>-    #           osdsPerDevice: &#34;5&#34;
</span></span></span><span style=display:flex><span><span style=color:#a40000>-    #       - name: &#34;/dev/disk/by-id/ata-ST4000DM004-XXXX&#34; # devices can be specified using full udev paths
</span></span></span><span style=display:flex><span><span style=color:#a40000>-    #     config: # configuration can be specified at the node level which overrides the cluster level config
</span></span></span><span style=display:flex><span><span style=color:#a40000>-    #   - name: &#34;172.17.4.301&#34;
</span></span></span><span style=display:flex><span><span style=color:#a40000>-    #     deviceFilter: &#34;^sd.&#34;
</span></span></span><span style=display:flex><span><span style=color:#a40000></span><span style=color:#00a000>+    nodes:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+      - name: &#34;node-0&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000>+        devices: # specific devices to use for storage can be specified for each node
</span></span></span><span style=display:flex><span><span style=color:#00a000>+          - name: &#34;sdb&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000>+          - name: &#34;sdc&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000>+          - name: &#34;sdd&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000>+      - name: &#34;node-1&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000>+        devices:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+          - name: &#34;sdb&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000>+          - name: &#34;sdc&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000>+          - name: &#34;sdd&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000>+      - name: &#34;node-2&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000>+        devices:
</span></span></span><span style=display:flex><span><span style=color:#00a000>+          - name: &#34;sdb&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000>+          - name: &#34;sdc&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000>+          - name: &#34;sdd&#34;
</span></span></span><span style=display:flex><span><span style=color:#00a000></span>     # when onlyApplyOSDPlacement is false, will merge both placement.All() and placement.osd
</span></span><span style=display:flex><span>     onlyApplyOSDPlacement: false
</span></span><span style=display:flex><span>   # The section for configuring management of daemon disruptions during upgrade or fencing.
</span></span></code></pre></div><p>编辑好 <code>cluster-env.yaml</code> 后，使用下面的命令创建 ceph 集群：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f cluster-env.yaml
</span></span><span style=display:flex><span>cephcluster.ceph.rook.io/rook-ceph created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 查看 rook-ceph namespace 里面的 pod 健康状况</span>
</span></span><span style=display:flex><span>$ kubectl -n rook-ceph get pods
</span></span><span style=display:flex><span>NAME                                                 READY   STATUS      RESTARTS   AGE
</span></span><span style=display:flex><span>rook-ceph-crashcollector-dl380p-55f6cc56c9-b8ghc     1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          3m3s
</span></span><span style=display:flex><span>rook-ceph-crashcollector-r710-7d8659858-mrqgq        1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          2m20s
</span></span><span style=display:flex><span>rook-ceph-crashcollector-r720xd-1-5b686487c5-hvzdb   1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          3m10s
</span></span><span style=display:flex><span>rook-ceph-csi-detect-version-ffdsf                   0/1     Completed   <span style=color:#0000cf;font-weight:700>0</span>          26m
</span></span><span style=display:flex><span>rook-ceph-mgr-a-759465b6c7-cslkp                     1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          3m13s
</span></span><span style=display:flex><span>rook-ceph-mon-a-657c4c6769-ljtr9                     1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          18m
</span></span><span style=display:flex><span>rook-ceph-mon-b-7db98b99d4-99pft                     1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          18m
</span></span><span style=display:flex><span>rook-ceph-mon-c-7f84fc475d-5v599                     1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          10m
</span></span><span style=display:flex><span>rook-ceph-operator-68964f4b87-pc87m                  1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          68m
</span></span><span style=display:flex><span>rook-ceph-osd-0-7cc5cb94cb-dxznm                     1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          2m32s
</span></span><span style=display:flex><span>rook-ceph-osd-1-f4d47ddf9-7vgh7                      1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          2m35s
</span></span><span style=display:flex><span>rook-ceph-osd-2-5d7667b8d8-d5tnp                     1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          2m20s
</span></span><span style=display:flex><span>rook-ceph-osd-3-c9c56cd77-8sbzj                      1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          2m32s
</span></span><span style=display:flex><span>rook-ceph-osd-4-88565589c-rnpmg                      1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          2m35s
</span></span><span style=display:flex><span>rook-ceph-osd-5-7d7c554b6c-pvsfx                     1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          2m35s
</span></span><span style=display:flex><span>rook-ceph-osd-6-6c7596c844-jg9qt                     1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          2m20s
</span></span><span style=display:flex><span>rook-ceph-osd-7-55f9987ddf-pjthz                     1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          2m32s
</span></span><span style=display:flex><span>rook-ceph-osd-8-6949b69dd6-685wp                     1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          2m20s
</span></span><span style=display:flex><span>rook-ceph-osd-prepare-dl380p-c6nc8                   0/1     Completed   <span style=color:#0000cf;font-weight:700>0</span>          3m3s
</span></span><span style=display:flex><span>rook-ceph-osd-prepare-r710-zkmjz                     0/1     Completed   <span style=color:#0000cf;font-weight:700>0</span>          3m3s
</span></span><span style=display:flex><span>rook-ceph-osd-prepare-r720xd-1-fswnf                 0/1     Completed   <span style=color:#0000cf;font-weight:700>0</span>          3m2s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 查看 ceph 集群的健康状况</span>
</span></span><span style=display:flex><span>$ kubectl -n rook-ceph get cephcluster
</span></span><span style=display:flex><span>NAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH
</span></span><span style=display:flex><span>rook-ceph   /var/lib/rook     <span style=color:#0000cf;font-weight:700>3</span>          29m   Ready   Cluster created successfully   HEALTH_OK
</span></span></code></pre></div><p>ceph 集群部署完后，我们需要部署 <code>toolbox.yaml</code> pod 获取集群连接信息：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f toolbox.yaml
</span></span><span style=display:flex><span>deployment.apps/rook-ceph-tools created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl -n rook-ceph get pods <span style=color:#000;font-weight:700>|</span> grep tools
</span></span><span style=display:flex><span>rook-ceph-tools-885579f55-qpnhh                      1/1     Running     <span style=color:#0000cf;font-weight:700>0</span>          3m44s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 进入 toolbox pod</span>
</span></span><span style=display:flex><span>$ kubectl -n rook-ceph <span style=color:#204a87>exec</span> -it <span style=color:#204a87;font-weight:700>$(</span>kubectl -n rook-ceph get pod -l <span style=color:#4e9a06>&#34;app=rook-ceph-tools&#34;</span> -o <span style=color:#000>jsonpath</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#39;{.items[0].metadata.name}&#39;</span><span style=color:#204a87;font-weight:700>)</span> -- bash
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 查看 mon_host 为: 172.16.254.126:6789,172.16.254.124:6789,172.16.254.127:6789</span>
</span></span><span style=display:flex><span><span style=color:#ce5c00;font-weight:700>[</span>root@rook-ceph-tools-885579f55-qpnhh /<span style=color:#ce5c00;font-weight:700>]</span>$ cat /etc/ceph/ceph.conf
</span></span><span style=display:flex><span><span style=color:#ce5c00;font-weight:700>[</span>global<span style=color:#ce5c00;font-weight:700>]</span>
</span></span><span style=display:flex><span><span style=color:#000>mon_host</span> <span style=color:#ce5c00;font-weight:700>=</span> 172.16.254.126:6789,172.16.254.124:6789,172.16.254.127:6789
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ce5c00;font-weight:700>[</span>client.admin<span style=color:#ce5c00;font-weight:700>]</span>
</span></span><span style=display:flex><span><span style=color:#000>keyring</span> <span style=color:#ce5c00;font-weight:700>=</span> /etc/ceph/keyring
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 查看 keyring 为: AQBHTWFhFQzrORAALLIngo/OOTDdnUf4vNPRoA==</span>
</span></span><span style=display:flex><span><span style=color:#ce5c00;font-weight:700>[</span>root@rook-ceph-tools-885579f55-qpnhh /<span style=color:#ce5c00;font-weight:700>]</span>$ cat /etc/ceph/keyring
</span></span><span style=display:flex><span><span style=color:#ce5c00;font-weight:700>[</span>client.admin<span style=color:#ce5c00;font-weight:700>]</span>
</span></span><span style=display:flex><span><span style=color:#000>key</span> <span style=color:#ce5c00;font-weight:700>=</span> AQBHTWFhFQzrORAALLIngo/OOTDdnUf4vNPRoA<span style=color:#ce5c00;font-weight:700>==</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 查看集群健康状态</span>
</span></span><span style=display:flex><span><span style=color:#ce5c00;font-weight:700>[</span>root@rook-ceph-tools-885579f55-qpnhh /<span style=color:#ce5c00;font-weight:700>]</span>$ ceph status
</span></span><span style=display:flex><span>  cluster:
</span></span><span style=display:flex><span>    id:     233cf123-7a1a-4a7b-b6db-1cee79ec752b
</span></span><span style=display:flex><span>    health: HEALTH_OK
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  services:
</span></span><span style=display:flex><span>    mon: <span style=color:#0000cf;font-weight:700>3</span> daemons, quorum a,b,c <span style=color:#ce5c00;font-weight:700>(</span>age 38m<span style=color:#ce5c00;font-weight:700>)</span>
</span></span><span style=display:flex><span>    mgr: a<span style=color:#ce5c00;font-weight:700>(</span>active, since 33m<span style=color:#ce5c00;font-weight:700>)</span>
</span></span><span style=display:flex><span>    osd: <span style=color:#0000cf;font-weight:700>9</span> osds: <span style=color:#0000cf;font-weight:700>9</span> up <span style=color:#ce5c00;font-weight:700>(</span>since 33m<span style=color:#ce5c00;font-weight:700>)</span>, <span style=color:#0000cf;font-weight:700>9</span> in <span style=color:#ce5c00;font-weight:700>(</span>since 34m<span style=color:#ce5c00;font-weight:700>)</span><span style=color:#000;font-weight:700>;</span> <span style=color:#0000cf;font-weight:700>30</span> remapped pgs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  data:
</span></span><span style=display:flex><span>    pools:   <span style=color:#0000cf;font-weight:700>1</span> pools, <span style=color:#0000cf;font-weight:700>256</span> pgs
</span></span><span style=display:flex><span>    objects: <span style=color:#0000cf;font-weight:700>0</span> objects, <span style=color:#0000cf;font-weight:700>0</span> B
</span></span><span style=display:flex><span>    usage:   <span style=color:#0000cf;font-weight:700>57</span> MiB used, <span style=color:#0000cf;font-weight:700>20</span> TiB / <span style=color:#0000cf;font-weight:700>20</span> TiB avail
</span></span><span style=display:flex><span>    pgs:     <span style=color:#0000cf;font-weight:700>226</span> active+clean
</span></span><span style=display:flex><span>             <span style=color:#0000cf;font-weight:700>30</span>  active+clean+remapped
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 查看 osd 状态，可以发现对应节点的设备都添加了进来</span>
</span></span><span style=display:flex><span><span style=color:#ce5c00;font-weight:700>[</span>root@rook-ceph-tools-885579f55-qpnhh /<span style=color:#ce5c00;font-weight:700>]</span>$ ceph osd status
</span></span><span style=display:flex><span>ID  HOST    USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE
</span></span><span style=display:flex><span> <span style=color:#0000cf;font-weight:700>0</span>  node-0  6172k  3725G      <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>       <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>   exists,up
</span></span><span style=display:flex><span> <span style=color:#0000cf;font-weight:700>1</span>  node-1  7836k   279G      <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>       <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>   exists,up
</span></span><span style=display:flex><span> <span style=color:#0000cf;font-weight:700>2</span>  node-2  5596k   931G      <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>       <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>   exists,up
</span></span><span style=display:flex><span> <span style=color:#0000cf;font-weight:700>3</span>  node-0  6044k  3725G      <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>       <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>   exists,up
</span></span><span style=display:flex><span> <span style=color:#0000cf;font-weight:700>4</span>  node-1  5980k   279G      <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>       <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>   exists,up
</span></span><span style=display:flex><span> <span style=color:#0000cf;font-weight:700>5</span>  node-1  5980k   279G      <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>       <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>   exists,up
</span></span><span style=display:flex><span> <span style=color:#0000cf;font-weight:700>6</span>  node-2  6236k  3726G      <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>       <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>   exists,up
</span></span><span style=display:flex><span> <span style=color:#0000cf;font-weight:700>7</span>  node-0  7772k  3725G      <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>       <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>   exists,up
</span></span><span style=display:flex><span> <span style=color:#0000cf;font-weight:700>8</span>  node-2  7836k  3726G      <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>       <span style=color:#0000cf;font-weight:700>0</span>        <span style=color:#0000cf;font-weight:700>0</span>   exists,up
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 创建一个 cloudpods-test 的 pool 用于后面的虚拟机测试</span>
</span></span><span style=display:flex><span><span style=color:#ce5c00;font-weight:700>[</span>root@rook-ceph-tools-885579f55-qpnhh /<span style=color:#ce5c00;font-weight:700>]</span>$ ceph osd pool create cloudpods-test <span style=color:#0000cf;font-weight:700>64</span> <span style=color:#0000cf;font-weight:700>64</span>
</span></span><span style=display:flex><span>pool <span style=color:#4e9a06>&#39;cloudpods-test&#39;</span> created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 初始化这个pool 为 RBD</span>
</span></span><span style=display:flex><span><span style=color:#ce5c00;font-weight:700>[</span>root@rook-ceph-tools-885579f55-qpnhh /<span style=color:#ce5c00;font-weight:700>]</span>$ rbd pool init cloudpods-test
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ce5c00;font-weight:700>[</span>root@rook-ceph-tools-885579f55-qpnhh /<span style=color:#ce5c00;font-weight:700>]</span>$ ceph osd lspools
</span></span><span style=display:flex><span><span style=color:#0000cf;font-weight:700>1</span> cloudpods-test
</span></span></code></pre></div><h2 id=cloudpods-虚拟机使用-rook-部署的-ceph>Cloudpods 虚拟机使用 Rook 部署的 Ceph</h2><p>通过之前的步骤，已经使用 Rook 在 Kubernetes 集群里面部署了 Ceph 集群，接下来将 Ceph 集群的连接信息导入到 Cloudpods 私有云平台，就可以给虚拟机使用了。</p><p>从之前的步骤得到 ceph 的连接信息如下：</p><ul><li>mon_host: 172.16.254.126:6789,172.16.254.124:6789,172.16.254.127:6789</li><li>keyring: AQBHTWFhFQzrORAALLIngo/OOTDdnUf4vNPRoA==</li><li>rbd pool: cloudpods-test</li></ul><p>然后登录 cloudpods 前端创建 ceph rbd 存储并且关联宿主机，操作如下：</p><ol><li>创建名为 rook-ceph 的块存储，填入上面的信息：</li></ol><p><img src=./create-ceph-storage.png alt></p><ol start=2><li>默认创建好的 rook-ceph 块存储为离线状态，需要关联平台的私有云宿主机，宿主机会探测 ceph 集群的连通性，并且获取对应的 pool 容量信息：</li></ol><p><img src=./attach-storage-host.png alt></p><ol start=3><li>rook-ceph 块存储关联好宿主机后，状态就会变为“在线”，并且获取到了 20T 的容量：</li></ol><p><img src=./rook-ceph-status.png alt></p><ol start=4><li>创建虚拟机使用 <code>rook-ceph</code> 存储，这里主要是在虚拟机创建页面添加磁盘，选择存储类型为 <strong>Ceph RBD</strong>：</li></ol><p><img src=./vm-with-ceph.png alt></p><ol start=5><li>等待虚拟机创建完成，通过 vnc 或者 ssh 登录虚拟机：</li></ol><p><img src=./vm-running.png alt></p><p>可以发现虚拟机里面挂载了 /dev/sda(系统盘) 和 /dev/sdb(数据盘)，底层都为 ceph 的 RBD 块设备，因为 ceph 底层块设备用的机械盘，用 dd 简单测试速度在 99 MB/s ，符合预期。</p><ol start=6><li>然后使用平台的 climc 命令查看虚拟机所在的宿主机，发现虚拟机 ceph-test-vm 运行在 node-1 计算节点上，同时改节点也是 Ceph 集群的存储节点，实现了超融合的架构：</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ climc server-list --details --search ceph-test-vm
</span></span><span style=display:flex><span>+--------------------------------------+--------------+--------+---------------+--------+---------+------------+-----------+------------+---------+
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>|</span>                  ID                  <span style=color:#000;font-weight:700>|</span>     Name     <span style=color:#000;font-weight:700>|</span>  Host  <span style=color:#000;font-weight:700>|</span>     IPs       <span style=color:#000;font-weight:700>|</span>  Disk  <span style=color:#000;font-weight:700>|</span> Status  <span style=color:#000;font-weight:700>|</span> vcpu_count <span style=color:#000;font-weight:700>|</span> vmem_size <span style=color:#000;font-weight:700>|</span> Hypervisor <span style=color:#000;font-weight:700>|</span> os_type <span style=color:#000;font-weight:700>|</span>
</span></span><span style=display:flex><span>+--------------------------------------+--------------+--------+---------------+--------+---------+------------+-----------+------------+---------+
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>|</span> ffd8ec7c-1e2d-4427-89e0-81b6ce184185 <span style=color:#000;font-weight:700>|</span> ceph-test-vm <span style=color:#000;font-weight:700>|</span> node-1 <span style=color:#000;font-weight:700>|</span> 172.16.254.252<span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>235520</span> <span style=color:#000;font-weight:700>|</span> running <span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>2</span>          <span style=color:#000;font-weight:700>|</span> <span style=color:#0000cf;font-weight:700>2048</span>      <span style=color:#000;font-weight:700>|</span> kvm        <span style=color:#000;font-weight:700>|</span> Linux   <span style=color:#000;font-weight:700>|</span>
</span></span><span style=display:flex><span>+--------------------------------------+--------------+--------+---------------+--------+---------+------------+-----------+------------+---------+
</span></span></code></pre></div><h2 id=其它操作>其它操作</h2><ul><li>删除 Rook 部署的 Ceph 集群请参考：<a href=https://rook.io/docs/rook/v1.7/ceph-teardown.html>Cleaning up a Cluster</a></li></ul><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><li><a href=/v3.9/zh/blog/2021/09/25/calico-customized-node-firewall/ class="btn btn-primary"><span class=mr-1>←</span> 上一页</a></li><a href=/v3.9/zh/blog/2022/01/21/skycomputing/ class="btn btn-primary">下一页 <span class=ml-1>→</span></a></li></ul></div></main></div></div><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?3c5253cd6530122d0f774cab69e3c07f",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><footer class="bg-dark pt-4 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>Copyright &copy; 2017-2023 The Cloudpods Authors.</small>
<a href=https://beian.miit.gov.cn/ style=text-decoration:none;color:inherit;display:block class=text-white>京ICP备2021005745号-3</a></div></div></div></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js integrity=sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy crossorigin=anonymous></script><script src=/v3.9/js/main.min.e38614a3d7988ac625b9f049be3f42dc3a8de71254a9c6ffd5850eac7849597a.js integrity="sha256-44YUo9eYisYlufBJvj9C3DqN5xJUqcb/1YUOrHhJWXo=" crossorigin=anonymous></script></body></html>